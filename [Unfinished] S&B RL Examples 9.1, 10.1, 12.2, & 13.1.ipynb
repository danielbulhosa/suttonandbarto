{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S&B RL Examples 9.1, 10.1, & 13.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chapters 9, 10, and 13 do not have any programming exercises. However, some of the examples in these chapters are interesting and we would like to implement them for the sake of getting more practice. We are interested in particular in:\n",
    "\n",
    "+ **Example 9.1**: This example describes a 1000-state random walk in which function approximation is used for estimating the value function. The approximation used is state aggregation on every 100 states. The solution implements gradient MC.\n",
    "\n",
    "+ **Example 10.1**: This example describes a control problem in which a car must learn how to fight gravity to get itself out of a valley. We want to implemtn n-step semi-gradient Sarsa to solve this problem using a linear model as is done in the example.\n",
    "\n",
    "+ **Example 12.2**: Same as Example 10.1 but implementing Sarsa($\\lambda$) and True Online Sarsa($\\lambda$).\n",
    "\n",
    "+ **Example 13.1**: This example optimizes a very simple random walk where the results of the actions \"left\" and \"right\" get flipped in one of the states. Given the limited representation of the actions the optimal policy is stochastic. We want to learn this stochastic policy using policy gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import copy\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "import math\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 9.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode(n, width):\n",
    "    \"\"\"\n",
    "    Generate a full episode of our MRP. Function \n",
    "    returns tuple containing sequence of states in\n",
    "    episode and the final (total) reward.\n",
    "    \"\"\"\n",
    "    \n",
    "    episode = []\n",
    "    s = int(n/2) # start state\n",
    "    episode_done = False\n",
    "    reward = None\n",
    "    \n",
    "    while not episode_done:\n",
    "        # Record current state\n",
    "        episode.append(s)\n",
    "        \n",
    "        # Determine the next state\n",
    "        left_edge = s - width\n",
    "        right_edge = s + width\n",
    "        move_right = np.random.binomial(1, 0.5)\n",
    "        \n",
    "        if move_right:\n",
    "            s_prime = np.random.randint(s + 1, right_edge + 1)\n",
    "        else:\n",
    "            s_prime = np.random.randint(left_edge, s)\n",
    "            \n",
    "        # Figure out if the transition was terminal and handle\n",
    "        if s_prime >= n:\n",
    "            episode_done = True\n",
    "            s = n\n",
    "            reward = 1\n",
    "        elif s_prime <= 1:\n",
    "            episode_done = True\n",
    "            s = 1\n",
    "            reward = -1\n",
    "        else:\n",
    "            s = s_prime\n",
    "            \n",
    "    return episode, reward\n",
    "\n",
    "def zero_vector(group_size, n):\n",
    "\n",
    "    assert n % group_size == 0, \"Group size must divide total number of states\"\n",
    "    \n",
    "    num_groups = int(n/group_size)\n",
    "    return np.zeros(num_groups)\n",
    "\n",
    "def feature_vector(s, group_size, n):\n",
    "    \"\"\"\n",
    "    Given a state s this returns the feature \n",
    "    vector X(s). Note that in this problem we\n",
    "    use state aggregation so each component of \n",
    "    the returned vector corresponds to a contiguous \n",
    "    group of states.\n",
    "    \"\"\"\n",
    "    X = zero_vector(group_size, n)\n",
    "    \n",
    "    group_num = int((s - 1)/group_size)\n",
    "    X[group_num] = 1\n",
    "    \n",
    "    return X\n",
    "    \n",
    "def value_function(X, w, group_size, n):\n",
    "    \"\"\"\n",
    "    Calculates the value function linearly\n",
    "    parametrized by w with feature vector \n",
    "    X(s).\n",
    "    \"\"\"\n",
    "\n",
    "    return np.dot(X, w)\n",
    "\n",
    "def gradient_monte_carlo(episodes, alpha, n, width, group_size):\n",
    "    \"\"\"\n",
    "    Takes in the problem parameters and the number\n",
    "    of episodes to be simulated and returns the vector\n",
    "    w describing our estimate of the value function for\n",
    "    the target policy (or process in our MRP case).\n",
    "    \"\"\"\n",
    "\n",
    "    w = zero_vector(group_size, n)\n",
    "    \n",
    "    t_i = time.time()\n",
    "    base_t_i = t_i\n",
    "\n",
    "    print(\"\\nBEGIN GRADIENT MC WITH PARAMS:\\n\" +\n",
    "          \"episodes: {}\\n\".format(episodes) +\n",
    "          \"alpha: {}\\n\".format(alpha) +\n",
    "          \"n: {}\\n\".format(n) +\n",
    "          \"width: {}\\n\".format(width) +\n",
    "          \"group_size: {}\\n\".format(group_size))\n",
    "    \n",
    "    for episode_num in range(episodes):\n",
    "        \n",
    "        episode, G = generate_episode(n, width)\n",
    "        \n",
    "        for s in episode:\n",
    "            # Note derivative of value function is just X(s) for linear model\n",
    "            # Also note the total reward (G_t) is the same at all t\n",
    "            X = feature_vector(s, group_size, n)\n",
    "            w += alpha * (G - value_function(X, w, group_size, n)) * X\n",
    "            \n",
    "        if (episode_num + 1) % int(episodes/10) == 0:\n",
    "            delta_t = round(time.time() - t_i, 2)\n",
    "            print(\"Episodes {}-{} complete: {} s\".format(episode_num + 1 - int(episodes/10),\n",
    "                                                         episode_num + 1,\n",
    "                                                         delta_t))\n",
    "            t_i = time.time() \n",
    "            \n",
    "    base_delta_t = round(time.time() - base_t_i, 2)\n",
    "    print(\"\\nSIMULATIONS COMPLETED, TOTAL TIME: {} s\\n\".format(base_delta_t))\n",
    "            \n",
    "    return w\n",
    "\n",
    "def value_function_arrays(w, n, group_size):\n",
    "    \"\"\"\n",
    "    Takes the vector w parametrizing our value \n",
    "    function and returns the arrays need to plot \n",
    "    the value as a function of the states.\n",
    "    \"\"\"\n",
    "    \n",
    "    states = np.arange(1, n + 1)\n",
    "    values = np.array([w[int((s - 1)/group_size)] for s in states])\n",
    "    \n",
    "    return states, values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_params = {\"episodes\": 10**5, \"alpha\": 2 * 10**(-5), \"n\": 10**3 , \"width\": 100, \"group_size\": 100}\n",
    "non_agg_params = {\"episodes\": 10**5, \"alpha\": 3 * 10**(-3), \"n\": 10**3, \"width\": 100, \"group_size\": 1}\n",
    "agg_w = gradient_monte_carlo(**agg_params)\n",
    "non_agg_w = gradient_monte_carlo(**non_agg_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_states, agg_values = value_function_arrays(agg_w, n, agg_params[\"group_size\"])\n",
    "nonagg_states, nonagg_values = value_function_arrays(non_agg_w, n, non_agg_params[\"group_size\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in the plot below comparing the estimates for group size of 1 vs. 100 the estimate of the value function for the smaller group size is much noiser. This is because, for the same number of episodes, we have many less samples for each group or bin. Thus aggregating states increases bias (since it forces contiguous states to share a value) but reduces variance (less noise for estimated state values)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize=(20, 10))\n",
    "\n",
    "plt.plot(nonagg_states[1:-1], nonagg_values[1:-1], label=\"Group Size = 1\")\n",
    "plt.plot(agg_states, agg_values, linewidth=5, label=\"Group Size = 100\")\n",
    "plt.xlabel(\"States $[s]$\", size=30)\n",
    "plt.xticks(size=20)\n",
    "plt.ylabel(\"State Value $[v_{\\pi}(s)]$\", size=30)\n",
    "plt.yticks(size=20)\n",
    "plt.legend(prop={\"size\": 20})\n",
    "plt.title(\"Gradient Monte Carlo Approximations To Value Function\", size=30)\n",
    "\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 10.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_bounds_valid(bounds):\n",
    "    assert len(bounds) == 2 \\\n",
    "           and bounds[0] < bounds[1], \"Passed bounds are invalid\"\n",
    "\n",
    "\n",
    "class MountainCarSimulator():\n",
    "    \n",
    "    def __init__(self, initial_position_bounds=[-0.6, -0.4], initial_velocity=0,\n",
    "                 x_bounds=[-1.2, 0.5], x_dot_bounds=[-0.07, 0.07], acceleration=0.001, \n",
    "                 gravity=0.0025, stepness_factor=3):\n",
    "    \n",
    "        check_bounds_valid(x_bounds)\n",
    "        check_bounds_valid(x_dot_bounds)\n",
    "    \n",
    "        self.x_bounds = x_bounds\n",
    "        self.x_dot_bounds = x_dot_bounds\n",
    "        self.alpha = alpha\n",
    "        self.acceleration = acceleration\n",
    "        self.gravity = gravity\n",
    "        self.stepness_factor = stepness_factor\n",
    "        self.initial_position_bounds = initial_position_bounds\n",
    "        self.initial_position = np.random.uniform(initial_position_bounds[0], initial_position_bounds[1])\n",
    "        self.initial_velocity = initial_velocity\n",
    "        \n",
    "        MountainCarSimulator.check_initial_state([self.initial_position, self.initial_velocity], \n",
    "                                                 x_bounds, x_dot_bounds)\n",
    "        \n",
    "    @staticmethod\n",
    "    def check_initial_state(initial_state, x_bounds, x_dot_bounds):\n",
    "        assert x_bounds[0] <= initial_state[0] <= x_bounds[1], \"Initial position is not valid\"\n",
    "        assert x_dot_bounds[0] <= initial_state[1] <= x_dot_bounds[1], \"Initial velocity is not valid\"\n",
    "        \n",
    "    @staticmethod\n",
    "    def bound_clip(x, bounds):\n",
    "        return max(min(x, bounds[1]), bounds[0])\n",
    "    \n",
    "    def get_initial_state(self):\n",
    "        return [self.initial_position, self.initial_velocity]\n",
    "        \n",
    "    def next_x(self, x, next_x_dot):\n",
    "        \"\"\"\n",
    "        Note that in this problem x, x' are NOT feature\n",
    "        vectors, together they specify the true state s.\n",
    "        \"\"\"\n",
    "        next_x = x + next_x_dot\n",
    "        next_x_bounded = MountainCarSimulator.bound_clip(next_x, self.x_bounds)\n",
    "        \n",
    "        return next_x_bounded\n",
    "\n",
    "    def next_x_dot(self, a, x, x_dot):\n",
    "        \"\"\"\n",
    "        Note that in this problem x, x' are NOT feature\n",
    "        vectors, together they specify the true state s.\n",
    "        \"\"\"\n",
    "        next_x_dot = x_dot + self.acceleration * a \\\n",
    "                     - self.gravity * math.cos(self.stepness_factor * x)\n",
    "        \n",
    "        next_x_dot_bounded = MountainCarSimulator.bound_clip(next_x_dot, self.x_dot_bounds)\n",
    "        \n",
    "        return next_x_dot\n",
    "    \n",
    "    def next_state(self, a, x, x_dot):\n",
    "        \"\"\"\n",
    "        Convenience method for getting the \n",
    "        next state, but handles the crucial \n",
    "        case of the car hitting the left boundary.\n",
    "        \"\"\"\n",
    "        \n",
    "        next_x_dot = self.next_x_dot(a, x, x_dot)\n",
    "        next_x = self.next_x(x, next_x_dot)\n",
    "        \n",
    "        if next_x == self.x_bounds[0]:\n",
    "            next_x_dot = 0\n",
    "            \n",
    "        return [next_x, next_x_dot]\n",
    "    \n",
    "    def check_terminal(self, x):\n",
    "        return x >= self.x_bounds[1]\n",
    "\n",
    "\n",
    "class TileOffsetGenerator2D():\n",
    "    \n",
    "    def __init__(self, dim, width, num_tilings):\n",
    "        \"\"\"\n",
    "        Based on recommendations of Miller and\n",
    "        Glanz explained on page 220 of S&B.\n",
    "        \"\"\"\n",
    "        assert 2**num_tilings >= 4*dim, \"Miller and Glanz recommend that 2**num_tilings >= 4*dim\"\n",
    "        \n",
    "        self.width = width\n",
    "        self.num_tilings = num_tilings\n",
    "        self.base_unit = self.width/num_tilings\n",
    "        \n",
    "        self.fundamental_vector = np.arange(1, 2*dim, 2)\n",
    "        \n",
    "        self.offsets = [step * self.base_unit * self.fundamental_vector for step in range(0, self.num_tilings)]\n",
    "        \n",
    "    def get_offsets(self):\n",
    "        return self.offsets\n",
    "\n",
    "    \n",
    "class RectangularTiling2D():\n",
    "    \n",
    "    def __init__(self, x_num, y_num, offset=[0, 0]):\n",
    "        assert len(offset) == 2, \"offset_vec must have length two\"\n",
    "        assert type(x_num) == int and type(y_num) == int, \"x_num and y_num must be ints\"\n",
    "        \n",
    "        # Since the tiles are rectangular\n",
    "        self.x_lines = np.append(np.sort((np.linspace(0, 1, x_num, endpoint=False) + offset[0]) % 1), [1.0])\n",
    "        self.y_lines = np.append(np.sort((np.linspace(0, 1, y_num, endpoint=False) + offset[1]) % 1), [1.0])\n",
    "        \n",
    "        # When there is no offset we have exactly x_num tiles along x and y_num along y\n",
    "        self.extra_x_tile = self.x_lines[0] != 0\n",
    "        self.extra_y_tile = self.y_lines[0] != 0\n",
    "        \n",
    "        self.x_tiles = x_num + int(self.extra_x_tile)\n",
    "        self.y_tiles = y_num + int(self.extra_y_tile)\n",
    "        \n",
    "        self.num_tiles = self.x_tiles * self.y_tiles\n",
    "        \n",
    "    @staticmethod\n",
    "    def check_normalized(coord):\n",
    "        assert 0 <= coord <= 1, \"Coordinates passed must be normalized.\"\n",
    "        \n",
    "    def find_tile_index(self, x_norm, y_norm):\n",
    "        \"\"\"\n",
    "        Passed coordinates are assumed to be normalized.\n",
    "        Returns 2D index of the tile within which coordinates\n",
    "        fall. 0-indexing is used.\n",
    "        \"\"\"\n",
    "        RectangularTiling2D.check_normalized(x_norm)\n",
    "        RectangularTiling2D.check_normalized(y_norm)\n",
    "        \n",
    "        x_index = np.argmin(x_norm >= self.x_lines) - int(not self.extra_x_tile)\n",
    "        y_index = np.argmin(y_norm >= self.y_lines) - int(not self.extra_y_tile)\n",
    "        \n",
    "        assert 0 <= x_index < self.x_tiles, \"x tile index {} out of bounds [0, {}).\".format(x_index, self.x_tiles)\n",
    "        assert 0 <= y_index < self.y_tiles, \"y tile index {} out of bounds [0, {}).\".format(y_index, self.y_tiles)\n",
    "        \n",
    "        return [x_index, y_index]\n",
    "    \n",
    "    def find_tile_scalar_index(self, x_norm, y_norm):\n",
    "        \"\"\"\n",
    "        Returns scalar index for tile. This will be the\n",
    "        index for this tile in the one dimensional vector\n",
    "        representing the encoding for this tile.\n",
    "        \"\"\"\n",
    "        \n",
    "        x_index, y_index = self.find_tile_index(x_norm, y_norm)\n",
    "        \n",
    "        return x_index + self.y_tiles * y_index\n",
    "\n",
    "    \n",
    "class TileEncoder2D():\n",
    "    \n",
    "    def __init__(self, offsets, x_bounds, x_dot_bounds, x_num=8, x_dot_num=8,\n",
    "                 tiler=RectangularTiling2D):\n",
    "        \n",
    "        check_bounds_valid(x_bounds)\n",
    "        check_bounds_valid(x_dot_bounds)\n",
    "        \n",
    "        self.num_tilings = len(offsets)\n",
    "        self.offsets = offsets\n",
    "        self.x_bounds = x_bounds\n",
    "        self.x_dot_bounds = x_dot_bounds\n",
    "        self.x_num = x_num\n",
    "        self.x_dot_num = x_dot_num\n",
    "        \n",
    "        # A list containing all of our tile objects\n",
    "        self.tilings = []\n",
    "        \n",
    "        for offset in offsets:\n",
    "            self.tilings.append(tiler(x_num, x_dot_num, offset))\n",
    "            \n",
    "        # What dimension vector X(s) is required to represent our encoding?\n",
    "        self.encoder_dim = 0 \n",
    "        \n",
    "        for tiling in self.tilings:\n",
    "            self.encoder_dim += tiling.num_tiles\n",
    "        \n",
    "    @staticmethod\n",
    "    def normalize(var, var_range):\n",
    "        return (var - var_range[0])/(var_range[1] - var_range[0])\n",
    "    \n",
    "    @staticmethod\n",
    "    def denormalize(norm_var, var_range):\n",
    "        return (var_range[1] - var_range[0]) * norm_var + var_range[0]\n",
    "    \n",
    "    def featurize(self, x, x_dot):\n",
    "        \"\"\"\n",
    "        Returns feature vector X(s) correspoding \n",
    "        to the state coordinate s=(x, x').\n",
    "        \"\"\"\n",
    "        x_norm, x_dot_norm = TileEncoder2D.normalize(x, self.x_bounds), \\\n",
    "                             TileEncoder2D.normalize(x_dot, self.x_dot_bounds)\n",
    "        tiling_vectors = []\n",
    "        \n",
    "        for tiling in self.tilings:\n",
    "            dim = tiling.num_tiles\n",
    "            index = tiling.find_tile_scalar_index(x_norm, x_dot_norm)\n",
    "            tiling_vector = np.zeros(dim, dtype=int)\n",
    "            tiling_vector[index] = 1\n",
    "            \n",
    "            tiling_vectors.append(tiling_vector)\n",
    "            \n",
    "        feature_vector = np.concatenate(tiling_vectors)\n",
    "        \n",
    "        assert len(feature_vector) == self.encoder_dim, \"Actual encoder dimension not what expected\"\n",
    "        assert len(feature_vector.shape) == 1, \"Feature vector is not one dimensional\"\n",
    "            \n",
    "        return feature_vector\n",
    "    \n",
    "class SemiGradientLinearNStepSarsa():\n",
    "    \n",
    "    \"\"\"\n",
    "    This class maintains a queue which contains the\n",
    "    last seen N + 1 states, actions, and rewards. When a\n",
    "    state, action, and reward are outside of the n-step \n",
    "    window they are removed from the queue. The queue will\n",
    "    be shorter than length n if:\n",
    "    \n",
    "    1) The episode has less than n-steps\n",
    "    2) The episode has just begun\n",
    "    3) The episode is ending\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n, alpha, gamma, epsilon):\n",
    "        self.n = n\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.features = [] # X(s) -- storing features rather than states to save compute\n",
    "        self.rewards = []\n",
    "        self.actions = []\n",
    "        \n",
    "        self.t = 0\n",
    "        self.tau = self.t - self.n + 1\n",
    "        self.T = np.inf\n",
    "        \n",
    "    @staticmethod\n",
    "    def Q(X, w_a):\n",
    "        \"\"\"\n",
    "        We assume each of the three actions -1, 0, 1\n",
    "        have their own weight vector of dimension \n",
    "        len(X(s)).\n",
    "        \n",
    "        This assumption is equivalent to assuming that\n",
    "        X(s, a) \\dot w = X(s, a) \\dot [w_{-1}, w_0, w_1]^T\n",
    "        = X(s, a) \\dot w_{a*=a} = X(s) \\cdot w_a. This follows\n",
    "        from the fact that X(s, a) \\dot  w_{a*!=a} = 0 because\n",
    "        the components of the vector X(s, a_0) corresponding \n",
    "        to a != a_0 are equal to zero. \n",
    "        \"\"\"\n",
    "        \n",
    "        assert len(X) == len(w_a), \"X and w_a must have the same length\"\n",
    "        return np.dot(X, w_a)\n",
    "    \n",
    "    @staticmethod\n",
    "    def Del_Q(X, w_a):\n",
    "        \"\"\"\n",
    "        In linear case the derivative is just equal\n",
    "        to the feature vector X(s). This follows from \n",
    "        the definition of our representation X(s) and\n",
    "        of X(s, a) as explained in the Q method.\n",
    "        \"\"\"\n",
    "        return X\n",
    "    \n",
    "    def pi(self, X, w):\n",
    "        Q_s = np.array([SemiGradientLinearNStepSarsa.Q(X, w[a + 1]) for a in [-1, 0, 1]])\n",
    "        optimal_actions = np.argwhere(Q_s == np.amax(Q_s)) - 1\n",
    "        not_greedy = np.random.binomial(1, self.epsilon)\n",
    "        \n",
    "        if not_greedy:\n",
    "            return np.random.choice([-1, 0, 1])\n",
    "        else:\n",
    "            return np.random.choice(optimal_actions.flatten())\n",
    "        \n",
    "    def add_observation(self, observation_list, observation):\n",
    "        # We pop elements if the list is sufficiently long or if the episode has terminated\n",
    "        if len(observation_list) > self.n or self.T != np.inf:\n",
    "            observation_list.pop(0)\n",
    "            \n",
    "        # When None is passed as an observation it is not appended (handles post episode termination case)\n",
    "        if observation is not None:\n",
    "            observation_list.append(observation)\n",
    "    \n",
    "    def increment_time(self):\n",
    "        self.t +=1\n",
    "        self.tau += 1\n",
    "    \n",
    "    def add_timestep(self, feature_t_1, action_t_1, reward_t_2):\n",
    "        feature_none = feature_t_1 is None\n",
    "        action_none = action_t_1 is None\n",
    "        reward_none = reward_t_2 is None\n",
    "        \n",
    "        if feature_none or action_none or reward_none:\n",
    "            assert  feature_none and action_none and reward_none, \\\n",
    "                \"If one state variable is `None` all should be.\"\n",
    "        \n",
    "        self.add_observation(self.features, feature_t_1)\n",
    "        self.add_observation(self.actions, action_t_1)\n",
    "        self.add_observation(self.rewards, reward_t_2)\n",
    "        \n",
    "    def G(self, w):\n",
    "        \"\"\"\n",
    "        We represent w as a 3 x num_features matrix where\n",
    "        each of the three rows corresponds to w_{-1}, w_{0},\n",
    "        and w_{1} respectively.\n",
    "        \"\"\"\n",
    "        \n",
    "        n = min(self.n, len(self.rewards)) # If less than n rewards left we truncate sum\n",
    "        gamma_factors = np.logspace(0, n - 1, n, base=self.gamma)\n",
    "        \n",
    "        G = np.dot(gamma_factors, self.rewards[:self.n])\n",
    "        \n",
    "        if self.tau + self.n < self.T:\n",
    "            X_tau_n = self.features[self.n]\n",
    "            w_a_tau_n = w[self.actions[self.n] + 1] # Actions are -1, 0, 1\n",
    "            G += self.gamma**n + SemiGradientLinearNStepSarsa.Q(X_tau_n, w_a_tau_n)\n",
    "            \n",
    "        return G\n",
    "    \n",
    "    def w_update(self, w):\n",
    "        G = self.G(w)\n",
    "        X_tau = self.features[0]\n",
    "        w_a_tau = w[self.actions[0] + 1] # Actions are -1, 0, 1\n",
    "        \n",
    "        # Note that we only get a gradient for the portion of w \n",
    "        # corresponding to the action value for which we update Q(S,A)\n",
    "        delta_w = np.zeros((3, len(w_a_tau)))\n",
    "        delta_w[self.actions[0] + 1] = alpha * (G - self.Q(X_tau, w_a_tau)) * self.Del_Q(X_tau, w_a_tau)\n",
    "        \n",
    "        return delta_w\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets the state of the Sarsa optimizer\n",
    "        \"\"\"\n",
    "        \n",
    "        self.features = []\n",
    "        self.rewards = []\n",
    "        self.actions = []\n",
    "        \n",
    "        self.t = 0\n",
    "        self.tau = self.t - self.n + 1\n",
    "        self.T = np.inf\n",
    "    \n",
    "    def check_done(self):\n",
    "        # Note that on the last iteration we pop first, then calculate the last G\n",
    "        # THEN measure the length. This means we need to check the length equals 1\n",
    "        # at the final timestep\n",
    "        features_empty = len(self.features) == 1\n",
    "        actions_empty = len(self.actions) == 1\n",
    "        rewards_empty = len(self.rewards) == 1\n",
    "        \n",
    "        lists_empty = features_empty and actions_empty and rewards_empty\n",
    "        time_limit = (self.tau == self.T - 1)\n",
    "        \n",
    "        # Just a consistency check that our implementation agrees with the pseudo-code\n",
    "        # Note we expect to be one step ahead of pseudo-code when lists are empty\n",
    "        assert (lists_empty == time_limit), \"Time condition and queue being empty should match.\"\n",
    "        \n",
    "        if lists_empty:\n",
    "            self.reset()\n",
    "        \n",
    "        return lists_empty\n",
    "    \n",
    "    def set_T_terminal(self):\n",
    "        self.T = self.t + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Main execution of algorithm here -- may want to wrap in method or class \n",
    "## if we want to be able to easily run multiple runs\n",
    "\n",
    "class SolveMountainCar():\n",
    "    \n",
    "    def __init__(self, episodes, simulator, sarsa, tiler):\n",
    "        self.episodes = episodes\n",
    "        self.simulator = simulator\n",
    "        self.sarsa = sarsa\n",
    "        self.tiler = tiler\n",
    "        self.X_dim = self.tiler.encoder_dim\n",
    "        self._w_array = np.empty((self.episodes, 3, self.X_dim))\n",
    "        self._steps_array = np.empty(self.episodes)\n",
    "        \n",
    "    def get_w_array(self):\n",
    "        return self._w_array\n",
    "    \n",
    "    def get_steps_array(self):\n",
    "        return self._steps_array\n",
    "    \n",
    "    def set_w_array(self, episode, w):\n",
    "        self._w_array[episode] = w\n",
    "        \n",
    "    def set_steps_array(self, episode, steps):\n",
    "        self._steps_array[episode] = steps\n",
    "        \n",
    "    def run(self):\n",
    "        w = np.zeros((3, self.X_dim))\n",
    "        \n",
    "        print(\"BEGIN MOUNTAIN CAR SARSA\\n\")\n",
    "        t_i = time.time()\n",
    "        t_i_abs = t_i\n",
    "        \n",
    "        for episode in range(self.episodes):\n",
    "            steps = 0\n",
    "            \n",
    "            x, v = self.simulator.get_initial_state()\n",
    "            X = self.tiler.featurize(x, v)\n",
    "            a = self.sarsa.pi(X, w)\n",
    "            r_prime = -1\n",
    "            \n",
    "            self.sarsa.add_timestep(X, a, r_prime)\n",
    "            sarsa_terminated = False\n",
    "            episode_terminated = False\n",
    "            \n",
    "            while not sarsa_terminated:\n",
    "                # Check at the beginning if each step if we've already terminated. If not, \n",
    "                # get the next state and check if that state if terminal\n",
    "                if not episode_terminated:\n",
    "                    x_prime, v_prime = self.simulator.next_state(a, x, v)\n",
    "                    episode_terminated = self.simulator.check_terminal(x_prime)\n",
    "                    \n",
    "                    if episode_terminated:\n",
    "                        self.sarsa.set_T_terminal()\n",
    "                \n",
    "                # If next state is terminal pass next features as None\n",
    "                if episode_terminated:\n",
    "                    X_prime = None\n",
    "                    a_prime = None\n",
    "                    r_dprime = None\n",
    "                    \n",
    "                # Else featurize next state, get next action and reward\n",
    "                else:\n",
    "                    X_prime = self.tiler.featurize(x_prime, v_prime)\n",
    "                    a_prime = self.sarsa.pi(X_prime, w)\n",
    "                    r_dprime = -1\n",
    "                    steps += 1\n",
    "                    \n",
    "                # Store next feature, action, and reward\n",
    "                self.sarsa.add_timestep(X_prime, a_prime, r_dprime)\n",
    "                \n",
    "                # If tau >= 0 apply the required update to w\n",
    "                if self.sarsa.tau >= 0:\n",
    "                    w += self.sarsa.w_update(w)\n",
    "                \n",
    "                # Finally, increment t and tau, and check whehter we have applied \n",
    "                # sarsa to all data in the episode before moving to next episode\n",
    "                x = x_prime\n",
    "                v = v_prime\n",
    "                a = a_prime\n",
    "                \n",
    "                sarsa_terminated = self.sarsa.check_done()\n",
    "                if not sarsa_terminated:\n",
    "                    self.sarsa.increment_time()\n",
    "                    \n",
    "            if self.episodes >= 100 and (episode + 1) % int(self.episodes/100) == 0:\n",
    "                t_f = time.time()\n",
    "                print(\"Iterations {}-{} Completed - Time: {}s\".format(episode + 1 - int(self.episodes/100),\n",
    "                                                                      episode + 1,\n",
    "                                                                      round(t_f-t_i, 2)))\n",
    "                t_i = t_f\n",
    "                \n",
    "            self.set_w_array(episode, w)\n",
    "            self.set_steps_array(episode, steps)\n",
    "\n",
    "        delta_t_abs = round((time.time() - t_i_abs)/(self.episodes), 2)\n",
    "        print(\"\\nEND MOUNTAIN CAR SARSA - Avg. Episode Time: {}s\".format(delta_t_abs))\n",
    "            \n",
    "        return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 100000 # start with 1, work up to 500\n",
    "simulator_params = {\"x_bounds\": [-1.2, 0.5], \"x_dot_bounds\": [-0.07, 0.07]}\n",
    "learning_params = {\"n\": 4, \"alpha\": 0.1, \"gamma\": 1, \"epsilon\": 0.1}\n",
    "offset_params = {\"dim\": 2, \"width\": 1/8, \"num_tilings\": 8}\n",
    "\n",
    "offsets = TileOffsetGenerator2D(**offset_params)\n",
    "tiling_params = {\"offsets\": offsets.get_offsets(), \n",
    "                 \"x_bounds\": simulator_params[\"x_bounds\"], \n",
    "                 \"x_dot_bounds\": simulator_params[\"x_dot_bounds\"]}\n",
    "\n",
    "simulator = MountainCarSimulator(**simulator_params)\n",
    "sarsa = SemiGradientLinearNStepSarsa(**learning_params)\n",
    "tiler = TileEncoder2D(**tiling_params)\n",
    "\n",
    "solver = SolveMountainCar(episodes, simulator, sarsa, tiler)\n",
    "w_final = solver.run()\n",
    "w_array = solver.get_w_array()\n",
    "steps_array = solver.get_steps_array()\n",
    "\n",
    "np.save(\"example_10_1_outputs/w_array_100000.npy\", w_array)\n",
    "np.save(\"example_10_1_outputs/steps_array_100000.npy\", steps_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_array = np.load(\"example_10_1_outputs/w_array_100000.npy\")\n",
    "w_final = w_array[-1]\n",
    "steps_array = np.load(\"example_10_1_outputs/steps_array_100000.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx, ny = 100, 100\n",
    "\n",
    "x = np.linspace(0, 1, nx, False)\n",
    "y = np.linspace(0, 1, ny, False)\n",
    "\n",
    "xv, yv = np.meshgrid(x, y)\n",
    "\n",
    "vdenorm_x = np.vectorize(lambda x: tiler.denormalize(x, tiler.x_bounds))\n",
    "vdenorm_x_dot = np.vectorize(lambda x_dot: tiler.denormalize(x_dot, tiler.x_dot_bounds))\n",
    "\n",
    "def vQ(x_v, x_dot_v, w, tiler, sarsa):\n",
    "    assert x_v.shape == x_dot_v.shape, \"Shapes must be equal\"\n",
    "    input_shape = x_v.shape\n",
    "    Q = np.empty((input_shape[0] * input_shape[1], 3))\n",
    "\n",
    "    x_flat = x_v.flatten()\n",
    "    x_dot_flat = x_dot_v.flatten()\n",
    "    \n",
    "    index = 0\n",
    "    for x, x_dot in zip(x_flat, x_dot_flat):\n",
    "        for a in np.arange(-1,2,1, dtype=int): \n",
    "            X = tiler.featurize(x, x_dot)\n",
    "            Q[index, a + 1] = sarsa.Q(X, w[a + 1])\n",
    "            \n",
    "        index += 1\n",
    "        \n",
    "    return Q.reshape(input_shape[0], input_shape[1], 3)\n",
    "\n",
    "x_v = vdenorm_x(xv)\n",
    "x_dot_v = vdenorm_x_dot(yv)\n",
    "Q_a = vQ(x_v, x_dot_v, w_final, tiler, sarsa)\n",
    "Q = np.max(Q_a, axis=2)\n",
    "pi = np.argmax(Q_a, axis=2) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "ax = fig.add_subplot(121)\n",
    "ax.contourf(x_v, x_dot_v, pi)\n",
    "ax = fig.add_subplot(122, projection='3d')\n",
    "ax.plot_surface(x_v, x_dot_v, -Q, cmap=\"coolwarm\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "ax = fig.add_subplot(121)\n",
    "ax.contourf(x_v, x_dot_v, -Q)\n",
    "ax = fig.add_subplot(122, projection='3d')\n",
    "ax.plot_surface(x_v, x_dot_v, -Q, cmap=\"coolwarm\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "pi0 = np.argmax(vQ(x_v, x_dot_v, w_array[0], tiler, sarsa), axis=2) - 1\n",
    "pi1 = np.argmax(vQ(x_v, x_dot_v, w_array[10], tiler, sarsa), axis=2) - 1\n",
    "pi2 = np.argmax(vQ(x_v, x_dot_v, w_array[100], tiler, sarsa), axis=2) - 1\n",
    "pi3 = np.argmax(vQ(x_v, x_dot_v, w_array[1000], tiler, sarsa), axis=2) - 1\n",
    "pi4 = np.argmax(vQ(x_v, x_dot_v, w_array[10000], tiler, sarsa), axis=2) - 1\n",
    "pi5 = np.argmax(vQ(x_v, x_dot_v, w_array[99999], tiler, sarsa), axis=2) - 1\n",
    "\n",
    "fig = plt.figure(figsize=(20,20))\n",
    "ax = fig.add_subplot(321)\n",
    "plt1 = ax.contourf(x_v, x_dot_v, pi0)\n",
    "fig.colorbar(plt1)\n",
    "ax = fig.add_subplot(322)\n",
    "plt2 = ax.contourf(x_v, x_dot_v, pi1)\n",
    "fig.colorbar(plt2)\n",
    "ax = fig.add_subplot(323)\n",
    "plt3 = ax.contourf(x_v, x_dot_v, pi2)\n",
    "fig.colorbar(plt3)\n",
    "ax = fig.add_subplot(324)\n",
    "plt4 = ax.contourf(x_v, x_dot_v, pi3)\n",
    "fig.colorbar(plt4)\n",
    "ax = fig.add_subplot(325)\n",
    "plt5 = ax.contourf(x_v, x_dot_v, pi4)\n",
    "fig.colorbar(plt5)\n",
    "ax = fig.add_subplot(326)\n",
    "plt6 = ax.contourf(x_v, x_dot_v, pi5)\n",
    "fig.colorbar(plt6)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q0 = np.max(vQ(x_v, x_dot_v, w_array[0], tiler, sarsa), axis=2)\n",
    "Q1 = np.max(vQ(x_v, x_dot_v, w_array[10], tiler, sarsa), axis=2)\n",
    "Q2 = np.max(vQ(x_v, x_dot_v, w_array[100], tiler, sarsa), axis=2)\n",
    "Q3 = np.max(vQ(x_v, x_dot_v, w_array[1000], tiler, sarsa), axis=2)\n",
    "Q4 = np.max(vQ(x_v, x_dot_v, w_array[10000], tiler, sarsa), axis=2)\n",
    "Q5 = np.max(vQ(x_v, x_dot_v, w_array[99999], tiler, sarsa), axis=2)\n",
    "\n",
    "%matplotlib inline\n",
    "fig = plt.figure(figsize=(20,20))\n",
    "ax = fig.add_subplot(321, projection='3d')\n",
    "ax.plot_surface(x_v, x_dot_v, -Q0, cmap=\"coolwarm\")\n",
    "ax = fig.add_subplot(322, projection='3d')\n",
    "ax.plot_surface(x_v, x_dot_v, -Q1, cmap=\"coolwarm\")\n",
    "ax = fig.add_subplot(323, projection='3d')\n",
    "ax.plot_surface(x_v, x_dot_v, -Q2, cmap=\"coolwarm\")\n",
    "ax = fig.add_subplot(324, projection='3d')\n",
    "ax.plot_surface(x_v, x_dot_v, -Q3, cmap=\"coolwarm\")\n",
    "ax = fig.add_subplot(325, projection='3d')\n",
    "ax.plot_surface(x_v, x_dot_v, -Q4, cmap=\"coolwarm\")\n",
    "ax = fig.add_subplot(326, projection='3d')\n",
    "ax.plot_surface(x_v, x_dot_v, -Q5, cmap=\"coolwarm\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,10))\n",
    "plt.plot(np.log10(steps_array), zorder=0)\n",
    "plt.hlines(2.18, 0, 100000, zorder=1, linewidth=3, linestyle=\"dashed\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 12.2 [FIXME- Complete]\n",
    "\n",
    "Use Sarsa($\\lambda$) and True Online Sarsa($\\lambda$) to solve the mountain car problem. Can reuse most of the clases from the solution to Example 10.1. Just need to chance the Sarsa and main classes. Want to compare the performance of these algorithms (with near optimal parameters) to $n$-step Sarsa with $n=4$ which is optimal according to S&B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 13.1\n",
    "\n",
    "We parametrize the policy by equations 13.2 and 13.3. In this case exercise 13.3 (which we solved in our notebook) implies that:\n",
    "\n",
    "$$ \\nabla \\ln{ \\pi(a \\ | \\ s, \\mathbf{\\theta} ) } = \\mathbf{x}(s,a) - \\sum_{b} \\pi(b  \\ | \\ s, \\mathbf{\\theta}) \\ \\mathbf{x}(s,b)$$ \n",
    "\n",
    "We use this formula to implement the REINFORCE Monte Carlo Policy-Gradient Control algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_h(a, theta):\n",
    "    \"\"\"\n",
    "    Note that the feature representation is\n",
    "    independent of the state so we can omit s\n",
    "    from the arguments of h. \n",
    "    \n",
    "    We represent right with 0 and left with 1\n",
    "    to align with the representation in the example\n",
    "    in the book.\n",
    "    \"\"\"\n",
    "    \n",
    "    assert a in [0, 1], \"a must be one of `[0, 1]`\"\n",
    "    \n",
    "    # Dot product is same as indexing since x(a) is basis vector\n",
    "    return theta[a]\n",
    "\n",
    "def calc_pi(a, theta):\n",
    "    \"\"\"\n",
    "    See documentation for `calc_h`\n",
    "    \"\"\"\n",
    "    \n",
    "    h_0 = calc_h(0, theta) \n",
    "    h_1 = calc_h(1, theta) \n",
    "    h_a = h_0 if a == 0 else h_1\n",
    "    \n",
    "    return np.exp(h_a)/(np.exp(h_0) + np.exp(h_1))\n",
    "\n",
    "def calc_del_log_pi(a, theta):\n",
    "    \"\"\"\n",
    "    Calculates the derivative of the natural\n",
    "    logarithm of the policy based on the formula\n",
    "    above.\n",
    "    \"\"\"\n",
    "    \n",
    "    x_0 = np.array([1, 0])\n",
    "    x_1 = np.array([0, 1])\n",
    "    x_a = np.array([1, 0]) if a == 0 else np.array([0, 1])\n",
    "    \n",
    "    return x_a - calc_pi(0, theta) * x_0 - calc_pi(1, theta) * x_1 \n",
    "\n",
    "def polgrad_episode(theta):\n",
    "    \n",
    "    s = 0 # Initial state representation\n",
    "    states = []\n",
    "    actions = []\n",
    "    prob_right = calc_pi(0, theta)\n",
    "    episode_over = False\n",
    "    \n",
    "    while not episode_over:\n",
    "        states.append(s)\n",
    "        try:\n",
    "            take_right = np.random.binomial(1, prob_right)\n",
    "        except ValueError as e:\n",
    "            h_0 = calc_h(0, theta) \n",
    "            h_1 = calc_h(1, theta) \n",
    "            print(h_0)\n",
    "            print(h_1)\n",
    "            print(theta)\n",
    "            print(prob_right)\n",
    "            raise e\n",
    "        actions.append(1 - take_right)    \n",
    "        \n",
    "        # This line of code captures the fact that at s=1 left and right are reversed\n",
    "        actually_right = take_right if s != 1 else 1 - take_right\n",
    "        \n",
    "        # Move the agent to the state that the action actually transitions to\n",
    "        if actually_right:\n",
    "            s = s + 1\n",
    "            episode_over = True if s >= 3 else False\n",
    "        else:\n",
    "            s = s - 1 if s > 0 else 0\n",
    "    \n",
    "    assert len(states) == len(actions), \"Length of actions and state list should be same\"\n",
    "    \n",
    "    return states, actions\n",
    "\n",
    "def reinforce(simulations, episodes, alpha, theta_i=np.zeros(2)):\n",
    "    rewards = np.empty((simulations, episodes))\n",
    "    pi = np.empty((simulations, episodes, 2))\n",
    "    \n",
    "    print(\"\\nBEGIN REINFORCE POLICY GRADIENT WITH PARAMS:\\n\" +\n",
    "          \"simulations: {}\\n\".format(simulations) +\n",
    "          \"episodes: {}\\n\".format(episodes) +\n",
    "          \"alpha: {}\\n\".format(alpha) +\n",
    "          \"theta_i: {}\\n\".format(theta_i) + \n",
    "          \"theta_i: {}\\n\".format(np.array([calc_pi(0, theta_i), calc_pi(1, theta_i)])))\n",
    "    \n",
    "    base_t_i = time.time()\n",
    "    t_i = time.time()\n",
    "    \n",
    "    for simulation in range(simulations): \n",
    "        theta = copy.deepcopy(theta_i) # Note setting theta = theta_i just points theta to theta_i\n",
    "                                       # which biases simulations beyond the first one\n",
    "        \n",
    "        for episode in range(episodes):\n",
    "            states, actions = polgrad_episode(theta)\n",
    "            theta_history = np.full((len(states), 2), np.nan) # For debugging\n",
    "            G = - len(states) # G_0 is total steps until episode end\n",
    "            rewards[simulation, episode] = G\n",
    "            pi_0 = calc_pi(0, theta)\n",
    "            pi[simulation, episode] = np.array([pi_0, 1 - pi_0]) \n",
    "\n",
    "            # Don't really need to loop through actions\n",
    "            i = 0\n",
    "            for s, a in zip(states, actions):\n",
    "                prev_theta = copy.deepcopy(theta)\n",
    "                theta_history[i] = prev_theta\n",
    "                theta += alpha * G * calc_del_log_pi(a, theta)\n",
    "                \n",
    "                if np.log10(calc_pi(0, theta)) < -4 or np.log10(calc_pi(1, theta)) < -4:\n",
    "                    # Checks for divergence scenario explained above\n",
    "                    # Logs useful details about it\n",
    "                    print(\"Previous theta\")\n",
    "                    print(prev_theta)\n",
    "                    print(\"Current theta\")\n",
    "                    print(theta)\n",
    "                    print(\"G\")\n",
    "                    print(G)\n",
    "                    print(\"Current pi\")\n",
    "                    print([calc_pi(0, theta), 1-calc_pi(0, theta)])\n",
    "                    print(\"Current policy gradient\")\n",
    "                    print(calc_del_log_pi(a, theta))\n",
    "                    print(\"Current policy update\")\n",
    "                    print(alpha * G * calc_del_log_pi(a, theta))\n",
    "                    print(\"Current action\")\n",
    "                    print(a)\n",
    "                    print(\"Current state\")\n",
    "                    print(s)\n",
    "                    print(\"Theta history\")\n",
    "                    print(np.concatenate((np.array(theta_history), \n",
    "                                          np.array(states).reshape(-1,1), \n",
    "                                          np.array(actions).reshape(-1,1)), axis=1))\n",
    "                    raise ValueError(\"Probabilities are becoming too low\")\n",
    "                \n",
    "                G += 1 # Remove single -1 from G since we move from G_t -> G_{t+1}\n",
    "                \n",
    "        if (simulation + 1) % 10 == 0:\n",
    "            t_f = time.time()\n",
    "            delta_t = round(t_f - t_i, 2)\n",
    "            print(\"10 SIMULATIONS COMPLETED, TOTAL TIME: {} s\".format(delta_t))\n",
    "            t_i = t_f\n",
    "            \n",
    "    base_delta_t = round(time.time() - base_t_i, 2)\n",
    "    print(\"\\nALL SIMULATIONS COMPLETED, TOTAL TIME: {} s\\n\".format(base_delta_t))\n",
    "            \n",
    "    return rewards, pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note a couple of observations about the simulations in S&B:\n",
    "\n",
    "+ The expected value of $G_0$ for the first episode seems to be around $-90$ which would imply a $\\mathbf{\\theta}$ such that the probability of taking a right is either $\\sim 4\\%$ or $\\sim 97\\%$. This follows from our solution to exercise 13.1 regarding the optimal probability. We achieve this approximately by setting $\\mathbf{\\theta} = [0, 4]^T, [4, 0]^T$.\n",
    "\n",
    "    + Interestingly, it seems that increasing the difference between the components of $\\mathbf{\\theta}$ can increase the likelihood of $\\pi$ diverging. A feedback cycle can start which drives the probability of taking a right to 1, which then makes episodes longer and longer, etc. \n",
    "    \n",
    "    + We think this happens because we get a lot of feedback that \"right is the wrong action\" since right has a high probability initially but tends to send the agent back to state 0 for a while until we happen to pick left by chance in state 1. This feedback makes the policy shift far towards taking left with high probability, which leads to the same problem, swinging the probability back towards right (likely with a higher probability than before). Eventually either we get a NaN error because of exponential overflow, or the episodes get so long the episodes get too slow to end in a reasonable time.\n",
    "    \n",
    "    + There are three possible solutions. We attempted both (2) and (3) with successful results below: \n",
    "    \n",
    "        1) Avoid overflow errors by improving the numerical simulation framework. In theory the learning should correct itself if we can avoid numerical errors. However a sufficiently smaller learning rate **may also** be needed.\n",
    "        \n",
    "        2) Reduce the learning rate so that we do not overshoot the minimum within an episode.\n",
    "        \n",
    "        3) Make the initial theta less extreme so this overly-strong feedback does not lead to the aforementioned instability.\n",
    "        \n",
    "    + We're not sure how the authors used the given learning rates and got $-90$ for the initial expected reward on episode one -- it could have been a perfectly balanced choice of $\\alpha$ and $\\mathbf{\\theta}_i$. We're even less sure about how their $\\alpha=2^{-12}$ performed the worst given their expected initial reward. Our results yield the opposite result.\n",
    "\n",
    "\n",
    "+ There is a lot of variance in the solutions we have obtained by setting $\\alpha = 2^{-12}, 2^{-13}, 2^{-14}$. We believe the learning rate in the book may be overstated so we're trying different learning rates with larger spacings between them.\n",
    "    + It turns out that going further down slows learning so much that it basically doesn't happen in the first 1000 episodes.\n",
    "    + We brought up the variance because we thought it meant the learning rate was too high, and that this was why our average rewards seeem to converge so quickly to the optimum. However, the actual reason it was learning so fast is because we were not reseting $\\mathbf{\\theta}_i$ correctly. $\\mathbf{\\theta}$ was being initialized to the previously learned value at each simulation start! This was biasing the results for simulations beyond the first one! Hence why our average reward curve was flat, but a single reward curve had the right (noisy) shape.\n",
    "\n",
    "There is also a possibility that there is an error with our code and that is causing issues with our reward predictions. However, when we eliminate the flip of the actions on $s=1$ the curves we get make sense. Thus it seems that most of the code here should be working correctly..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why are we getting overflow errors, binomial parameter errors, or the code freezing altogether???\n",
    "import sys\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "simulations = 100\n",
    "episodes = 1000\n",
    "alphas = [2**(-13), 2**(-14), 2**(-15)]\n",
    "\n",
    "# We set theta this way to attain an expected reward of -90 for the first step\n",
    "# To avoid divergence this required starting alpha at 2**(-13) or smaller\n",
    "rewards_12_full, pi_12_full = reinforce(simulations, episodes, alphas[0], np.array([3.737, 0], dtype=float))\n",
    "rewards_13_full, pi_13_full = reinforce(simulations, episodes, alphas[1], np.array([3.737, 0], dtype=float))\n",
    "rewards_14_full, pi_14_full = reinforce(simulations, episodes, alphas[2], np.array([3.737, 0], dtype=float))\n",
    "\n",
    "rewards_12, pi_12 = np.mean(rewards_12_full, axis=0), np.mean(pi_12_full[:,-1,:], axis=0)\n",
    "rewards_13, pi_13 = np.mean(rewards_13_full, axis=0), np.mean(pi_13_full[:,-1,:], axis=0)\n",
    "rewards_14, pi_14 = np.mean(rewards_14_full, axis=0), np.mean(pi_14_full[:,-1,:], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize=(20, 10))\n",
    "\n",
    "plt.plot(rewards_12, label=r\"$ \\alpha=2^{-13}$\", zorder=0)\n",
    "plt.plot(rewards_13, label=r\"$ \\alpha=2^{-14}$\", zorder=1)\n",
    "plt.plot(rewards_14, label=r\"$ \\alpha=2^{-15}$\", zorder=2)\n",
    "plt.hlines(-11.6, 0, 1000, color=\"black\", linestyle=\"dashed\", linewidth=5, label=\"$v_{*}(S_0)$\", zorder=3)\n",
    "\n",
    "plt.xlabel(\"Episode\", size=30)\n",
    "plt.xticks(size=20)\n",
    "plt.ylabel(\"$G_0$\", size=30)\n",
    "plt.yticks(size=20)\n",
    "plt.legend(prop={\"size\": 20})\n",
    "plt.title(\"Total Reward On Episodes - Average Over 100 Runs\", size=30)\n",
    "\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the probability attained doesn't quite reach the optimal value! More learning time would be required under the chosen parameters. However, it looks like to match the graph in the book (i.e. the expected value in the first episode) we could have reduced $\\mathbf{\\theta}_i$ a little bit.\n",
    "\n",
    "It's worth noting in other runs we got quite close to the optimal probability value under these parameters. This means that the average of 100 runs still has significant variance. More runs would be needed to get a better estimate of the performance of these parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize=(20, 10))\n",
    "\n",
    "plt.plot(np.mean(pi_12_full[:,:,0], axis=0), label=r\"$ \\alpha=2^{-13}$\", linewidth=4)\n",
    "plt.plot(np.mean(pi_13_full[:,:,0], axis=0), label=r\"$ \\alpha=2^{-14}$\", linewidth=4)\n",
    "plt.plot(np.mean(pi_14_full[:,:,0], axis=0), label=r\"$ \\alpha=2^{-15}$\", linewidth=4)\n",
    "plt.hlines(0.58579, 0, 1000, linestyle=\"dashed\", label=\"$P_{*}(Right)$\", linewidth=4)\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "plt.xlabel(\"Episode\", size=30)\n",
    "plt.xticks(size=20)\n",
    "plt.ylabel(\"$P(Right)$\", size=30)\n",
    "plt.yticks(size=20)\n",
    "plt.legend(prop={\"size\": 20})\n",
    "plt.title(\"Total Reward On Episodes - Average Over 100 Runs\", size=30)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Probability of taking right for -13: {}\".format(np.mean(pi_12_full[:,-1,0], axis=0)))\n",
    "print(\"Probability of taking right for -14: {}\".format(np.mean(pi_13_full[:,-1,0], axis=0)))\n",
    "print(\"Probability of taking right for -15: {}\".format(np.mean(pi_14_full[:,-1,0], axis=0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We re-run the experiment but with a slightly smaller $\\mathbf{\\theta}$ differential and with the learning rates specified in the book. Note the expected initial reward is higher but still pretty close to that pictured in the book! \n",
    "\n",
    "Although it's much less likely that divergence happens under these parameters, there is still some probability of it happening. We've had it occur in some instances with $\\alpha=2^{-12}$ in the runs below... The more correct approach would be to decrease $\\alpha$ and to have the learning occur over more episodes. But we want a direct comparison to the approach taken in the book.\n",
    "\n",
    "The fact that divergence is probabilistic makes me think it is due to $\\alpha$ not being small enough to guarantee the theoretical conditions necessary for convergence. The choice for $\\alpha$ is not as clear in policy gradients as in other approaches because in practice we only calculate a quantity proportional to the true gradient, not the exact gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why are we getting overflow errors, binomial parameter errors, or the code freezing altogether???\n",
    "import sys\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "simulations = 100\n",
    "episodes = 1000\n",
    "alphas = [2**(-12), 2**(-13), 2**(-14)]\n",
    "\n",
    "# With this choice for theta we can use the learning rates specified in the book without diverging\n",
    "rewards_12_full, pi_12_full = reinforce(simulations, episodes, alphas[0], np.array([3.5, 0], dtype=float))\n",
    "rewards_13_full, pi_13_full = reinforce(simulations, episodes, alphas[1], np.array([3.5, 0], dtype=float))\n",
    "rewards_14_full, pi_14_full = reinforce(simulations, episodes, alphas[2], np.array([3.5, 0], dtype=float))\n",
    "\n",
    "rewards_12, pi_12 = np.mean(rewards_12_full, axis=0), np.mean(pi_12_full[:,-1,:], axis=0)\n",
    "rewards_13, pi_13 = np.mean(rewards_13_full, axis=0), np.mean(pi_13_full[:,-1,:], axis=0)\n",
    "rewards_14, pi_14 = np.mean(rewards_14_full, axis=0), np.mean(pi_14_full[:,-1,:], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize=(20, 10))\n",
    "\n",
    "plt.plot(rewards_12, label=r\"$ \\alpha=2^{-12}$\", zorder=0)\n",
    "plt.plot(rewards_13, label=r\"$ \\alpha=2^{-13}$\", zorder=1)\n",
    "plt.plot(rewards_14, label=r\"$ \\alpha=2^{-14}$\", zorder=2)\n",
    "plt.hlines(-11.6, 0, 1000, color=\"black\", linestyle=\"dashed\", linewidth=5, label=\"$v_{*}(S_0)$\", zorder=3)\n",
    "\n",
    "plt.xlabel(\"Episode\", size=30)\n",
    "plt.xticks(size=20)\n",
    "plt.ylabel(\"$G_0$\", size=30)\n",
    "plt.yticks(size=20)\n",
    "plt.legend(prop={\"size\": 20})\n",
    "plt.title(\"Total Reward On Episodes - Average Over 100 Runs\", size=30)\n",
    "\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We nearly attain the optimal probability! We've seen this choice of parameters be more consistent (over admittedly only a couple of trials). This is a somewhat biased result since we've seen $2^{-12}$ diverge and only kept runs that succeeded, but we don't know what fraction of runs would be divergent and hence can't quantify the bias. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize=(20, 10))\n",
    "\n",
    "plt.plot(np.mean(pi_12_full[:,:,0], axis=0), label=r\"$ \\alpha=2^{-12}$\", linewidth=4)\n",
    "plt.plot(np.mean(pi_13_full[:,:,0], axis=0), label=r\"$ \\alpha=2^{-13}$\", linewidth=4)\n",
    "plt.plot(np.mean(pi_14_full[:,:,0], axis=0), label=r\"$ \\alpha=2^{-14}$\", linewidth=4)\n",
    "plt.hlines(0.58579, 0, 1000, linestyle=\"dashed\", label=r\"$P_{*}(Right)\", linewidth=4)\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "plt.xlabel(\"Episode\", size=30)\n",
    "plt.xticks(size=20)\n",
    "plt.ylabel(\"$P(Right)$\", size=30)\n",
    "plt.yticks(size=20)\n",
    "plt.legend(prop={\"size\": 20})\n",
    "plt.title(\"Total Reward On Episodes - Average Over 100 Runs\", size=30)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Probability of taking right for -12: {}\".format(np.mean(pi_12_full[:,-1,0], axis=0)))\n",
    "print(\"Probability of taking right for -13: {}\".format(np.mean(pi_13_full[:,-1,0], axis=0)))\n",
    "print(\"Probability of taking right for -14: {}\".format(np.mean(pi_14_full[:,-1,0], axis=0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We believe the case $2^{-12}$ converges to a suboptimal solution for the authors because each episode we learn we overshoot the optimum. How they achieved this and simultaneously attained an expected reward on the first episode of about $-90$ is unclear to me. It seems to me you would need a $\\mathbf{\\theta}_i$ close to the optimum to begin with along with a high $\\alpha$ to fall into that scenario. At that point however the intial episode should have a better expectation than what is shown in their graph. \n",
    "\n",
    "However, for our purposes the solution above is close enough and satisfactory!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
