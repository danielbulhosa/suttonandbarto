{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S&B 7.10\n",
    "\n",
    "We are going to use the very simple example of a random walk on which the reward is $-1$ at each time step. The environment is deterministic and the policy is simply the probability of taking a right. This is similar to the Markov Reward Process described in earlier chapters, but we treat the transition probability as part of the policy rather than as a result of the environment. This way we can calculate the value function under one probability choice by simulating episodes using a different probability choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomWalkEpisode():\n",
    "    \n",
    "    def __init__(self, p_right, width):\n",
    "        self.p_right = p_right\n",
    "        self.width = width\n",
    "        \n",
    "    def initial_state(self):\n",
    "        return 0\n",
    "    \n",
    "    def check_terminal(self, s):\n",
    "        return s > self.width\n",
    "    \n",
    "    def reward(self, s, a):\n",
    "        return 1 if a == 1 else -1\n",
    "        \n",
    "    \n",
    "    def next_action_state_reward(self, s):        \n",
    "        if self.check_terminal(s):\n",
    "            raise ValueError(\"Episode hasd already terminated.\")\n",
    "            \n",
    "        take_right = np.random.binomial(1, self.p_right)\n",
    "        if take_right:\n",
    "            a = 1\n",
    "        else: \n",
    "            a = -1\n",
    "        \n",
    "        r = self.reward(s, a)\n",
    "        s_prime = max(s + a, 0) # You can't move further left than 0\n",
    "        \n",
    "        return a, s_prime, r\n",
    "    \n",
    "    \n",
    "class NStepOffPolicyPrediction():\n",
    "    \n",
    "    \"\"\"\n",
    "    This class maintains a queue which contains the\n",
    "    last seen N + 1 states, actions, and rewards. When a\n",
    "    state, action, and reward are outside of the n-step \n",
    "    window they are removed from the queue. The queue will\n",
    "    be shorter than length n if:\n",
    "    \n",
    "    1) The episode has less than n-steps\n",
    "    2) The episode has just begun\n",
    "    3) The episode is ending\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n, alpha, gamma, width, b, pi):\n",
    "        self.n = n\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.states = [] # Store states since this is a tabular case\n",
    "        self.rewards = []\n",
    "        self.actions = []\n",
    "        self.V = np.zeros(width + 1)\n",
    "        \n",
    "        # Probability of taking a right under each policy\n",
    "        self.b = b\n",
    "        self.pi = pi\n",
    "        \n",
    "        self.t = 0\n",
    "        self.tau = self.t - self.n\n",
    "        self.T = np.inf\n",
    "        \n",
    "        # For performance\n",
    "        self.gamma_factors = np.logspace(0, self.n - 1, self.n, base=self.gamma)\n",
    "        \n",
    "    def add_observation(self, observation_list, observation):\n",
    "        # We pop elements if the list is sufficiently long or if the episode has terminated\n",
    "        if len(observation_list) > self.n or self.T != np.inf:\n",
    "            observation_list.pop(0)\n",
    "            \n",
    "        # When None is passed as an observation it is not appended (handles post episode termination case)\n",
    "        if observation is not None:\n",
    "            observation_list.append(observation)\n",
    "    \n",
    "    def increment_time(self):\n",
    "        self.t +=1\n",
    "        self.tau += 1\n",
    "    \n",
    "    def add_timestep(self, state_t_1, action_t_1, reward_t_2):\n",
    "        state_none = state_t_1 is None\n",
    "        action_none = action_t_1 is None\n",
    "        reward_none = reward_t_2 is None\n",
    "        \n",
    "        if state_none or action_none or reward_none:\n",
    "            assert state_none and action_none and reward_none, \\\n",
    "                \"If one state variable is `None` all should be.\"\n",
    "        \n",
    "        self.add_observation(self.states, state_t_1)\n",
    "        self.add_observation(self.actions, action_t_1)\n",
    "        self.add_observation(self.rewards, reward_t_2)\n",
    "        \n",
    "    def G(self):\n",
    "        n = min(self.n, len(self.rewards)) # If less than n rewards left we truncate sum\n",
    "        \n",
    "        G = np.dot(self.gamma_factors[:n], self.rewards[:self.n])\n",
    "        \n",
    "        if self.tau + self.n < self.T:\n",
    "            s_tau_n = self.states[self.n]\n",
    "            V = self.V[s_tau_n]\n",
    "            G += self.gamma**n * V\n",
    "            \n",
    "        return G\n",
    "    \n",
    "    def rho(self):\n",
    "        \n",
    "        n = min(self.n, len(self.rewards)) # If less than n rewards left we truncate sum\n",
    "        pi_values = np.where(np.array(self.actions[:n-1]) == 1, self.pi, 1-self.pi)\n",
    "        b_values = np.where(np.array(self.actions[:n-1]) == 1, self.b, 1-self.b)\n",
    "        \n",
    "        assert len(pi_values.shape) == 1 and len(b_values.shape) == 1, \"pi and b tensors have wrong rank, {}, {}\".format(pi_values, b_values)\n",
    "        \n",
    "        rho = np.prod(pi_values/b_values)\n",
    "        \n",
    "        return rho\n",
    "        \n",
    "    \n",
    "    def V_update(self):\n",
    "        G = self.G()\n",
    "        rho = self.rho()\n",
    "        s_tau = self.states[0]\n",
    "        \n",
    "        delta_V = self.alpha * rho * (G - self.V[s_tau])\n",
    "        \n",
    "        return delta_V\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets the state of the off policy predictor\n",
    "        \"\"\"\n",
    "        \n",
    "        self.features = []\n",
    "        self.rewards = []\n",
    "        self.actions = []\n",
    "        \n",
    "        self.t = 0\n",
    "        self.tau = self.t - self.n\n",
    "        self.T = np.inf\n",
    "    \n",
    "    def check_done(self):\n",
    "        # Note that on the last iteration we pop first, then calculate the last G\n",
    "        # THEN measure the length. This means we need to check the length equals 1\n",
    "        # at the final timestep\n",
    "        states_empty = len(self.states) == 1\n",
    "        actions_empty = len(self.actions) == 1\n",
    "        rewards_empty = len(self.rewards) == 1\n",
    "        \n",
    "        lists_empty = states_empty and actions_empty and rewards_empty\n",
    "        time_limit = (self.tau == self.T - 1)\n",
    "        \n",
    "        # Just a consistency check that our implementation agrees with the pseudo-code\n",
    "        # Note we expect to be one step ahead of pseudo-code when lists are empty\n",
    "        assert (lists_empty == time_limit), \"Time condition and queue being empty should match.\"\n",
    "        \n",
    "        if lists_empty:\n",
    "            self.reset()\n",
    "        \n",
    "        return lists_empty\n",
    "    \n",
    "    def set_T_terminal(self):\n",
    "        self.T = self.t + 1\n",
    "\n",
    "\n",
    "class RunSimulator():\n",
    "    \n",
    "    def __init__(self, episodes, episode_simulator, value_predictor):\n",
    "        self.episodes = episodes\n",
    "        self.episode_simulator = episode_simulator\n",
    "        self.value_predictor = value_predictor\n",
    "        \n",
    "    def simulate(self):\n",
    "        V_array = np.empty((self.episodes, self.episode_simulator.width + 1))\n",
    "        \n",
    "        for episode in range(self.episodes):\n",
    "            s = self.episode_simulator.initial_state()\n",
    "            a, s_prime, r_prime = self.episode_simulator.next_action_state_reward(s)\n",
    "            \n",
    "            prediction_terminated = False\n",
    "            episode_terminated = False\n",
    "            \n",
    "            self.value_predictor.add_timestep(s, a, r_prime)\n",
    "            self.value_predictor.increment_time()\n",
    "            \n",
    "            s = s_prime\n",
    "            \n",
    "            while not prediction_terminated:\n",
    "                # If next state is terminal pass next features as None\n",
    "                if episode_terminated:\n",
    "                    s = None\n",
    "                    a = None\n",
    "                    r_prime = None\n",
    "                \n",
    "                # Check at the beginning of each step if we've already terminated. If not, \n",
    "                # get the next state and check if that state if terminal\n",
    "                if not episode_terminated:\n",
    "                    a, s_prime, r_prime = self.episode_simulator.next_action_state_reward(s)\n",
    "                    episode_terminated = self.episode_simulator.check_terminal(s_prime)\n",
    "                    \n",
    "                    if episode_terminated:\n",
    "                        self.value_predictor.set_T_terminal()\n",
    "                                        \n",
    "                # Store next feature, action, and reward\n",
    "                self.value_predictor.add_timestep(s, a, r_prime)\n",
    "                \n",
    "                # If tau >= 0 apply the required update to w\n",
    "                if self.value_predictor.tau >= 0:\n",
    "                    s_tau = self.value_predictor.states[0]\n",
    "                    self.value_predictor.V[s_tau] += self.value_predictor.V_update()\n",
    "                    V_array[episode] = self.value_predictor.V\n",
    "                \n",
    "                # Finally, increment t and tau, and check whehter we have applied \n",
    "                # value prediction to all data in the episode before moving to next episode\n",
    "                s = s_prime\n",
    "                \n",
    "                prediction_terminated = self.value_predictor.check_done()\n",
    "                if not prediction_terminated:\n",
    "                    self.value_predictor.increment_time()\n",
    "                            \n",
    "        return V_array\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(simulations, episodes, n, gamma, pi, b, width, alpha, per_decision=False):\n",
    "    print(\"\\nRUN SIMULATION WITH PARAMS:\\n\" + \\\n",
    "          \"episodes: {}\\n\".format(episodes) + \\\n",
    "          \"n: {}\\n\".format(n) + \\\n",
    "          \"gamma: {}\\n\".format(gamma) + \\\n",
    "          \"pi: {}\\n\".format(pi) + \\\n",
    "          \"b: {}\\n\".format(b) + \\\n",
    "          \"width: {}\\n\".format(width) + \\\n",
    "          \"alpha: {}\\n\".format(alpha)\n",
    "         )\n",
    "    \n",
    "    V_array_list = []\n",
    "    \n",
    "    t_i = time.time()\n",
    "    for simulation in range(simulations):\n",
    "        simulator = RandomWalkEpisode(b, width)\n",
    "        predictor = NStepOffPolicyPrediction(n, alpha, gamma, width, b, pi) if not per_decision \\\n",
    "                    else NStepOffPolicyPerDecisionPrediction(n, alpha, gamma, width, b, pi)\n",
    "        runner = RunSimulator(episodes, simulator, predictor)\n",
    "        V_array = runner.simulate()\n",
    "        V_array_list.append(V_array)\n",
    "        \n",
    "        if (simulation + 1) % 10 == 0:\n",
    "            t_f = time.time()\n",
    "            delta_t = round(t_f - t_i, 2)\n",
    "            print(\"Simulations #{}-{} Completed: {}s\".format(simulation + 1 - 10,\n",
    "                                                             simulation + 1, \n",
    "                                                             delta_t))\n",
    "            t_i = time.time()\n",
    "    \n",
    "    # Axes are simulation, episode, state\n",
    "    return np.array(V_array_list)\n",
    "\n",
    "def avg_mean_squared_error(V_array, V_ref):\n",
    "    # Axes are simulations, episodes\n",
    "    mean_squared_errors = np.sum(np.power(V_ref - V_array, 2), axis=2)/len(V_ref)\n",
    "    avg_mse = np.mean(mean_squared_errors, axis=0)\n",
    "    return avg_mse\n",
    "\n",
    "def mean_squared_error(V_array, V_ref):\n",
    "    # Axes are simulations, episodes\n",
    "    mean_squared_errors = np.sum(np.power(V_ref - V_array, 2), axis=1)/len(V_ref)\n",
    "    return mean_squared_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I) Fixed Target Policy: Greedy Target Policy, Exploratory Data Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V_on_policy = run(100, 1000, 4, 1, 0.9, 0.9, 50, 0.05)\n",
    "V_b_80 = run(100, 1000, 4, 1, 0.9, 0.8, 50, 0.05)\n",
    "V_b_70 = run(100, 1000, 4, 1, 0.9, 0.7, 50, 0.05)\n",
    "V_b_60 = run(100, 1000, 4, 1, 0.9, 0.6, 50, 0.05)\n",
    "\n",
    "V_ref = np.mean(V_on_policy[:,-1,:], axis=0)\n",
    "\n",
    "avg_MSE_on_policy = avg_mean_squared_error(V_on_policy, V_ref)\n",
    "avg_MSE_b_80 = avg_mean_squared_error(V_b_80, V_ref)\n",
    "avg_MSE_b_70 = avg_mean_squared_error(V_b_70, V_ref)\n",
    "avg_MSE_b_60 = avg_mean_squared_error(V_b_60, V_ref)\n",
    "\n",
    "MSE_b_80 = mean_squared_error(V_b_80[0], V_ref)\n",
    "MSE_b_70 = mean_squared_error(V_b_70[0], V_ref)\n",
    "MSE_b_60 = mean_squared_error(V_b_60[0], V_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,10))\n",
    "\n",
    "fig.add_subplot(321)\n",
    "plt.title(\"Average: 100 Simulations\", size=20)\n",
    "plt.plot(avg_MSE_on_policy, label=\"On-Policy\")\n",
    "plt.plot(avg_MSE_b_80, label=\"Off-Policy\")\n",
    "plt.legend(prop={'size': 10})\n",
    "plt.ylabel(\"$b(Right)=80\\%$\", size=20)\n",
    "\n",
    "fig.add_subplot(322)\n",
    "plt.title(\"Single Simulation\", size=20)\n",
    "plt.plot(avg_MSE_on_policy, label=\"On-Policy\")\n",
    "plt.plot(MSE_b_80, label=\"Off-Policy\")\n",
    "plt.legend(prop={'size': 10})\n",
    "\n",
    "fig.add_subplot(323)\n",
    "plt.plot(avg_MSE_on_policy, label=\"On-Policy\")\n",
    "plt.plot(avg_MSE_b_70, label=\"Off-Policy\")\n",
    "plt.legend(prop={'size': 10})\n",
    "plt.ylabel(\"$b(Right)=50\\%$\", size=20)\n",
    "\n",
    "fig.add_subplot(324)\n",
    "plt.plot(avg_MSE_on_policy, label=\"On-Policy\")\n",
    "plt.plot(MSE_b_70, label=\"Off-Policy\")\n",
    "plt.legend(prop={'size': 10})\n",
    "\n",
    "fig.add_subplot(325)\n",
    "plt.plot(avg_MSE_on_policy, label=\"On-Policy\")\n",
    "plt.plot(avg_MSE_b_60, label=\"Off-Policy\")\n",
    "plt.legend(prop={'size': 10})\n",
    "plt.ylabel(\"$b(Right)=60\\%$\", size=20)\n",
    "\n",
    "fig.add_subplot(326)\n",
    "plt.plot(avg_MSE_on_policy, label=\"On-Policy\")\n",
    "plt.plot(MSE_b_60, label=\"Off-Policy\")\n",
    "plt.legend(prop={'size': 10})\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our solution was diverging for a while, until we realized we were generating episodes using the target policy instead of the sample policy. Once we addressed that we did not have exponential divergence (as anticipated, since the expectation of G was then correct) but now our solutions converge to a high error for smaller $b$. We're not sure why this new problem is occurring. \n",
    "\n",
    "It turns out that replacing $G$ with $V(S_t)$ when rho_t equals zero is NOT equivalent to thes formula control variate formula in the book.(even though it seems conceptually like this would be the case). Whe we tried the latter our solution had a lot more variance, even in the $b=80\\%$ case, which is the simplest and works with this formula...\n",
    "\n",
    "It turns out the convergent but high error solutions were due to bugs in the implementation of the policy prediction algorithm. In essense, we were not logging the time or states correctly:\n",
    "+ The time step $\\tau$ was $+1$ ahead of where it should have been. This makes sense for Sarsa because of the fact we have $A_{t+1}$ at time $t$. \n",
    "+ We did not increment the time step after logging the initial state and action, so we were one time step behind from the get go.\n",
    "+ Needed to fix how we handle termination since policy prediction does not follow the exact same termination sequence as Sarsa.\n",
    "\n",
    "Those fixes were sufficent to get this working!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NStepOffPolicyPerDecisionPrediction(NStepOffPolicyPrediction):    \n",
    "    def G(self):\n",
    "        n = min(self.n, len(self.rewards)) # If less than n rewards left we truncate sum        \n",
    "        \n",
    "        # Note that whether the horizon is beyond the end of the episode\n",
    "        # determines how we initialize G in the recursion\n",
    "        if self.tau + self.n < self.T:\n",
    "            s_tau_n = self.states[self.n]\n",
    "            V = self.V[s_tau_n]\n",
    "            G = V\n",
    "        else:\n",
    "            G = 0\n",
    "            \n",
    "        reverse_indices = list(range(n))\n",
    "        reverse_indices.reverse()\n",
    "            \n",
    "        # The actual recursion, which is computed backwards\n",
    "        for i in reverse_indices:\n",
    "            rho = self.pi/self.b if self.actions[i] == 1 else (1 - self.pi)/(1 - self.b)\n",
    "            # Note the control variate is calculated at the S_t corresponding to G_t:h\n",
    "            s_i = self.states[i]\n",
    "            r_i_1 = self.rewards[i]\n",
    "                \n",
    "            G = rho * (r_i_1 + self.gamma * G) + (1 - rho) * self.V[s_i]\n",
    "            \n",
    "        return G\n",
    "    \n",
    "    def V_update(self):\n",
    "        G = self.G()\n",
    "        s_tau = self.states[0]\n",
    "        \n",
    "        delta_V = self.alpha * (G - self.V[s_tau])\n",
    "        \n",
    "        return delta_V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V_on_policy_pd = run(100, 1000, 4, 1, 0.9, 0.9, 50, 0.05, per_decision=True)\n",
    "V_b_80_pd = run(100, 1000, 4, 1, 0.9, 0.8, 50, 0.05, per_decision=True)\n",
    "V_b_70_pd = run(100, 1000, 4, 1, 0.9, 0.7, 50, 0.05, per_decision=True)\n",
    "V_b_60_pd = run(100, 1000, 4, 1, 0.9, 0.6, 50, 0.05, per_decision=True)\n",
    "\n",
    "V_ref_pd = np.mean(V_on_policy[:,-1,:], axis=0)\n",
    "\n",
    "avg_MSE_on_policy_pd = avg_mean_squared_error(V_on_policy_pd, V_ref_pd)\n",
    "avg_MSE_b_80_pd = avg_mean_squared_error(V_b_80_pd, V_ref_pd)\n",
    "avg_MSE_b_70_pd = avg_mean_squared_error(V_b_70_pd, V_ref_pd)\n",
    "avg_MSE_b_60_pd = avg_mean_squared_error(V_b_60_pd, V_ref_pd)\n",
    "\n",
    "MSE_b_80_pd = mean_squared_error(V_b_80_pd[0], V_ref_pd)\n",
    "MSE_b_70_pd = mean_squared_error(V_b_70_pd[0], V_ref_pd)\n",
    "MSE_b_60_pd = mean_squared_error(V_b_60_pd[0], V_ref_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,10))\n",
    "\n",
    "fig.add_subplot(321)\n",
    "plt.title(\"Average: 100 Simulations\", size=20)\n",
    "plt.plot(avg_MSE_on_policy_pd, label=\"On-Policy\")\n",
    "plt.plot(avg_MSE_b_80_pd, label=\"Off-Policy\")\n",
    "plt.legend(prop={'size': 10})\n",
    "plt.ylabel(\"$b(Right)=80\\%$\", size=20)\n",
    "\n",
    "fig.add_subplot(322)\n",
    "plt.title(\"Single Simulation\", size=20)\n",
    "plt.plot(avg_MSE_on_policy_pd, label=\"On-Policy\")\n",
    "plt.plot(MSE_b_80_pd, label=\"Off-Policy\")\n",
    "plt.legend(prop={'size': 10})\n",
    "\n",
    "fig.add_subplot(323)\n",
    "plt.plot(avg_MSE_on_policy_pd, label=\"On-Policy\")\n",
    "plt.plot(avg_MSE_b_70_pd, label=\"Off-Policy\")\n",
    "plt.legend(prop={'size': 10})\n",
    "plt.ylabel(\"$b(Right)=70\\%$\", size=20)\n",
    "\n",
    "fig.add_subplot(324)\n",
    "plt.plot(avg_MSE_on_policy_pd, label=\"On-Policy\")\n",
    "plt.plot(MSE_b_70_pd, label=\"Off-Policy\")\n",
    "plt.legend(prop={'size': 10})\n",
    "\n",
    "fig.add_subplot(325)\n",
    "plt.plot(avg_MSE_on_policy_pd, label=\"On-Policy\")\n",
    "plt.plot(avg_MSE_b_60_pd, label=\"Off-Policy\")\n",
    "plt.legend(prop={'size': 10})\n",
    "plt.ylabel(\"$b(Right)=60\\%$\", size=20)\n",
    "\n",
    "fig.add_subplot(326)\n",
    "plt.plot(avg_MSE_on_policy_pd, label=\"On-Policy\")\n",
    "plt.plot(MSE_b_60_pd, label=\"Off-Policy\")\n",
    "plt.legend(prop={'size': 10})\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we compare the per-decison formula's performance to the standard formula's performance. As we can see below, the per-decision formula performs better as expected! The effect is extremely slight however. We attribute this to the fact that the data-generating policy is much more exploratory, so it tends to collect more data than the target policy would have **per episode**. This in turn makes the performance per episode from both approaches likely near optimal.\n",
    "\n",
    "It could be worth plotting performance of both methods against cumulative timestep rather than episode. The authors specified the per-decision methodology specifically to be more efficient per data sample. It may be that the gap is larger in per-decision space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,20))\n",
    "\n",
    "fig.add_subplot(4, 4, 1)\n",
    "plt.title(\"Average: 100 Simulations\", size=20)\n",
    "plt.plot(avg_MSE_on_policy, label=\"On-Policy - Standard\")\n",
    "plt.plot(avg_MSE_on_policy_pd, label=\"On-Policy - Per Decision\")\n",
    "plt.legend(prop={'size': 10})\n",
    "plt.ylabel(\"On-Policy: $b(Right)=90\\%$\", size=20)\n",
    "\n",
    "fig.add_subplot(4, 4, 2)\n",
    "plt.title(\"Single Simulation\", size=20)\n",
    "plt.plot(mean_squared_error(V_on_policy[0], V_ref_pd), label=\"On-Policy - Standard\")\n",
    "plt.plot(mean_squared_error(V_on_policy_pd[0], V_ref_pd), label=\"On-Policy - Per Decision\")\n",
    "plt.legend(prop={'size': 10})\n",
    "\n",
    "fig.add_subplot(4, 4, 3)\n",
    "plt.title(\"All Simulations: Standard\", size=20)\n",
    "for index in np.arange(0, 100, 1):\n",
    "    plt.plot(mean_squared_error(V_on_policy[index], V_ref_pd))\n",
    "plt.yscale('log')\n",
    "plt.ylim(10**(-9), 10**(4))\n",
    "\n",
    "fig.add_subplot(4, 4, 4)\n",
    "plt.title(\"All Simulations: Per Decision\", size=20)\n",
    "for index in np.arange(0, 100, 1):\n",
    "    plt.plot(mean_squared_error(V_on_policy_pd[index], V_ref_pd))\n",
    "plt.yscale('log')\n",
    "plt.ylim(10**(-9), 10**(4))\n",
    "\n",
    "fig.add_subplot(4, 4, 5)\n",
    "plt.plot(avg_MSE_on_policy_pd, label=\"On-Policy\")\n",
    "plt.plot(avg_MSE_b_80, label=\"Off-Policy - Standard\")\n",
    "plt.plot(avg_MSE_b_80_pd, label=\"Off-Policy - Per-Decision\")\n",
    "plt.legend(prop={'size': 10})\n",
    "plt.ylabel(\"$b(Right)=80\\%$\", size=20)\n",
    "\n",
    "fig.add_subplot(4, 4, 6)\n",
    "plt.plot(avg_MSE_on_policy_pd, label=\"On-Policy\")\n",
    "plt.plot(MSE_b_80, label=\"Off-Policy - Standard\")\n",
    "plt.plot(MSE_b_80_pd, label=\"Off-Policy - Per-Decision\")\n",
    "plt.legend(prop={'size': 10})\n",
    "\n",
    "fig.add_subplot(4, 4, 7)\n",
    "for index in np.arange(0, 100, 1):\n",
    "    plt.plot(mean_squared_error(V_b_80[index], V_ref_pd))\n",
    "plt.yscale('log')\n",
    "plt.ylim(10**(-9), 10**(4))\n",
    "\n",
    "fig.add_subplot(4, 4, 8)\n",
    "for index in np.arange(0, 100, 1):\n",
    "    plt.plot(mean_squared_error(V_b_80_pd[index], V_ref_pd))\n",
    "plt.yscale('log')\n",
    "plt.ylim(10**(-9), 10**(4))\n",
    "\n",
    "fig.add_subplot(4, 4, 9)\n",
    "plt.plot(avg_MSE_on_policy_pd, label=\"On-Policy\")\n",
    "plt.plot(avg_MSE_b_70, label=\"Off-Policy - Standard\")\n",
    "plt.plot(avg_MSE_b_70_pd, label=\"Off-Policy - Per-Decision\")\n",
    "plt.legend(prop={'size': 10})\n",
    "plt.ylabel(\"$b(Right)=70\\%$\", size=20)\n",
    "\n",
    "fig.add_subplot(4, 4, 10)\n",
    "plt.plot(avg_MSE_on_policy_pd, label=\"On-Policy\")\n",
    "plt.plot(MSE_b_70, label=\"Off-Policy - Standard\")\n",
    "plt.plot(MSE_b_70_pd, label=\"Off-Policy - Per-Decision\")\n",
    "plt.legend(prop={'size': 10})\n",
    "\n",
    "fig.add_subplot(4, 4, 11)\n",
    "for index in np.arange(0, 100, 1):\n",
    "    plt.plot(mean_squared_error(V_b_70[index], V_ref_pd))\n",
    "plt.yscale('log')\n",
    "plt.ylim(10**(-9), 10**(4))\n",
    "\n",
    "fig.add_subplot(4, 4, 12)\n",
    "for index in np.arange(0, 100, 1):\n",
    "    plt.plot(mean_squared_error(V_b_70_pd[index], V_ref_pd))\n",
    "plt.yscale('log')\n",
    "plt.ylim(10**(-9), 10**(4))\n",
    "\n",
    "fig.add_subplot(4, 4, 13)\n",
    "plt.plot(avg_MSE_on_policy_pd, label=\"On-Policy\")\n",
    "plt.plot(avg_MSE_b_60, label=\"Off-Policy - Standard\")\n",
    "plt.plot(avg_MSE_b_60_pd, label=\"Off-Policy - Per-Decision\")\n",
    "plt.legend(prop={'size': 10})\n",
    "plt.ylabel(\"$b(Right)=60\\%$\", size=20)\n",
    "\n",
    "fig.add_subplot(4, 4, 14)\n",
    "plt.plot(avg_MSE_on_policy_pd, label=\"On-Policy\")\n",
    "plt.plot(MSE_b_60, label=\"Off-Policy - Standard\")\n",
    "plt.plot(MSE_b_60_pd, label=\"Off-Policy - Per-Decision\")\n",
    "plt.legend(prop={'size': 10})\n",
    "\n",
    "fig.add_subplot(4, 4, 15)\n",
    "for index in np.arange(0, 100, 1):\n",
    "    plt.plot(mean_squared_error(V_b_60[index], V_ref_pd))\n",
    "plt.yscale('log')\n",
    "plt.ylim(10**(-9), 10**(4))\n",
    "\n",
    "fig.add_subplot(4, 4, 16)\n",
    "for index in np.arange(0, 100, 1):\n",
    "    plt.plot(mean_squared_error(V_b_60_pd[index], V_ref_pd))\n",
    "plt.yscale('log')\n",
    "plt.ylim(10**(-9), 10**(4))\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II) Fixed Target Policy: Exploratory Target Policy, Greedy Data Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ii_V_b_90 = run(100, 1000, 4, 1, 0.6, 0.9, 50, 0.05)\n",
    "ii_V_b_80 = run(100, 1000, 4, 1, 0.6, 0.8, 50, 0.05)\n",
    "ii_V_b_70 = run(100, 1000, 4, 1, 0.6, 0.7, 50, 0.05)\n",
    "ii_V_on_policy = run(100, 1000, 4, 1, 0.6, 0.6, 50, 0.05)\n",
    "\n",
    "ii_V_ref = np.mean(ii_V_on_policy[:,-1,:], axis=0)\n",
    "\n",
    "ii_avg_MSE_b_90 = avg_mean_squared_error(ii_V_b_90, ii_V_ref)\n",
    "ii_avg_MSE_b_80 = avg_mean_squared_error(ii_V_b_80, ii_V_ref)\n",
    "ii_avg_MSE_b_70 = avg_mean_squared_error(ii_V_b_70, ii_V_ref)\n",
    "ii_avg_MSE_on_policy = avg_mean_squared_error(ii_V_on_policy, ii_V_ref)\n",
    "\n",
    "ii_MSE_b_90 = mean_squared_error(ii_V_b_90[0], ii_V_ref)\n",
    "ii_MSE_b_80 = mean_squared_error(ii_V_b_80[0], ii_V_ref)\n",
    "ii_MSE_b_70 = mean_squared_error(ii_V_b_70[0], ii_V_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ii_V_b_90_pd = run(100, 1000, 4, 1, 0.6, 0.9, 50, 0.05, per_decision=True)\n",
    "ii_V_b_80_pd = run(100, 1000, 4, 1, 0.6, 0.8, 50, 0.05, per_decision=True)\n",
    "ii_V_b_70_pd = run(100, 1000, 4, 1, 0.6, 0.7, 50, 0.05, per_decision=True)\n",
    "ii_V_on_policy_pd = run(100, 1000, 4, 1, 0.6, 0.6, 50, 0.05, per_decision=True)\n",
    "\n",
    "ii_V_ref_pd = np.mean(ii_V_on_policy_pd[:,-1,:], axis=0)\n",
    "\n",
    "ii_avg_MSE_b_90_pd = avg_mean_squared_error(ii_V_b_90_pd, ii_V_ref_pd)\n",
    "ii_avg_MSE_b_80_pd = avg_mean_squared_error(ii_V_b_80_pd, ii_V_ref_pd)\n",
    "ii_avg_MSE_b_70_pd = avg_mean_squared_error(ii_V_b_70_pd, ii_V_ref_pd)\n",
    "ii_avg_MSE_on_policy_pd = avg_mean_squared_error(ii_V_on_policy_pd, ii_V_ref_pd)\n",
    "\n",
    "ii_MSE_b_90_pd = mean_squared_error(ii_V_b_90_pd[0], ii_V_ref_pd)\n",
    "ii_MSE_b_80_pd = mean_squared_error(ii_V_b_80_pd[0], ii_V_ref_pd)\n",
    "ii_MSE_b_70_pd = mean_squared_error(ii_V_b_70_pd[0], ii_V_ref_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we compare the per-decison formula's performance to the standard formula's performance. As we can see below, the per-decision formula performs worse... This is surprising given what the authors anticipated in the book. It is interesting that when we flip around the nature of the target and data-generating policies (exploratory vs. greedy) it changes the performance of the different approaches so much.\n",
    "\n",
    "However, given the fact the results in the previous section make so much sense we feel confident about the results here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,20))\n",
    "\n",
    "fig.add_subplot(441)\n",
    "plt.title(\"Average: 100 Simulations\", size=20)\n",
    "plt.plot(ii_avg_MSE_on_policy_pd, label=\"On-Policy\")\n",
    "plt.plot(ii_avg_MSE_b_90, label=\"Off-Policy - Standard\")\n",
    "plt.plot(ii_avg_MSE_b_90_pd, label=\"Off-Policy - Per-Decision\")\n",
    "plt.legend(prop={'size': 10})\n",
    "plt.ylabel(\"$b(Right)=90\\%$\", size=20)\n",
    "\n",
    "fig.add_subplot(442)\n",
    "plt.title(\"Single Simulation\", size=20)\n",
    "plt.plot(ii_avg_MSE_on_policy_pd, label=\"On-Policy\")\n",
    "plt.plot(ii_MSE_b_90, label=\"Off-Policy - Standard\")\n",
    "plt.plot(ii_MSE_b_90_pd, label=\"Off-Policy - Per-Decision\")\n",
    "plt.legend(prop={'size': 10})\n",
    "\n",
    "fig.add_subplot(443)\n",
    "plt.title(\"All Simulations: Standard\", size=20)\n",
    "for index in np.arange(0, 100, 1):\n",
    "    plt.plot(mean_squared_error(ii_V_b_90[index], ii_V_ref_pd))\n",
    "plt.yscale('log')\n",
    "plt.ylim(10**(-5), 10**(8))\n",
    "\n",
    "fig.add_subplot(444)\n",
    "plt.title(\"All Simulations: Per Decision\", size=20)\n",
    "for index in np.arange(0, 100, 1):\n",
    "    plt.plot(mean_squared_error(ii_V_b_90_pd[index], ii_V_ref_pd))\n",
    "plt.yscale('log')\n",
    "plt.ylim(10**(-5), 10**(8))\n",
    "\n",
    "fig.add_subplot(445)\n",
    "plt.plot(ii_avg_MSE_on_policy_pd, label=\"On-Policy\")\n",
    "plt.plot(ii_avg_MSE_b_80, label=\"Off-Policy - Standard\")\n",
    "plt.plot(ii_avg_MSE_b_80_pd, label=\"Off-Policy - Per-Decision\")\n",
    "plt.legend(prop={'size': 10})\n",
    "plt.ylabel(\"$b(Right)=80\\%$\", size=20)\n",
    "\n",
    "fig.add_subplot(446)\n",
    "plt.plot(ii_avg_MSE_on_policy_pd, label=\"On-Policy\")\n",
    "plt.plot(ii_MSE_b_80, label=\"Off-Policy - Standard\")\n",
    "plt.plot(ii_MSE_b_80_pd, label=\"Off-Policy - Per-Decision\")\n",
    "plt.legend(prop={'size': 10})\n",
    "\n",
    "fig.add_subplot(447)\n",
    "for index in np.arange(0, 100, 1):\n",
    "    plt.plot(mean_squared_error(ii_V_b_80[index], ii_V_ref_pd))\n",
    "plt.yscale('log')\n",
    "plt.ylim(10**(-5), 10**(8))\n",
    "\n",
    "fig.add_subplot(448)\n",
    "for index in np.arange(0, 100, 1):\n",
    "    plt.plot(mean_squared_error(ii_V_b_80_pd[index], ii_V_ref_pd))\n",
    "plt.yscale('log')\n",
    "plt.ylim(10**(-5), 10**(8))\n",
    "\n",
    "fig.add_subplot(449)\n",
    "plt.plot(ii_avg_MSE_on_policy_pd, label=\"On-Policy\")\n",
    "plt.plot(ii_avg_MSE_b_70, label=\"Off-Policy - Standard\")\n",
    "plt.plot(ii_avg_MSE_b_70_pd, label=\"Off-Policy - Per-Decision\")\n",
    "plt.legend(prop={'size': 10})\n",
    "plt.ylabel(\"$b(Right)=70\\%$\", size=20)\n",
    "\n",
    "fig.add_subplot(4, 4, 10)\n",
    "plt.plot(ii_avg_MSE_on_policy_pd, label=\"On-Policy\")\n",
    "plt.plot(ii_MSE_b_70, label=\"Off-Policy - Standard\")\n",
    "plt.plot(ii_MSE_b_70_pd, label=\"Off-Policy - Per-Decision\")\n",
    "plt.legend(prop={'size': 10})\n",
    "\n",
    "fig.add_subplot(4, 4, 11)\n",
    "for index in np.arange(0, 100, 1):\n",
    "    plt.plot(mean_squared_error(ii_V_b_70[index], ii_V_ref_pd))\n",
    "plt.yscale('log')\n",
    "plt.ylim(10**(-5), 10**(8))\n",
    "\n",
    "fig.add_subplot(4, 4, 12)\n",
    "for index in np.arange(0, 100, 1):\n",
    "    plt.plot(mean_squared_error(ii_V_b_70_pd[index], ii_V_ref_pd))\n",
    "plt.yscale('log')\n",
    "plt.ylim(10**(-5), 10**(8))\n",
    "\n",
    "fig.add_subplot(4, 4, 13)\n",
    "plt.plot(ii_avg_MSE_on_policy, label=\"On-Policy - Standard\")\n",
    "plt.plot(ii_avg_MSE_on_policy_pd, label=\"On-Policy - Per Decision\")\n",
    "plt.legend(prop={'size': 10})\n",
    "plt.ylabel(\"On-Policy: $b(Right)=60\\%$\", size=20)\n",
    "\n",
    "fig.add_subplot(4, 4, 14)\n",
    "plt.plot(mean_squared_error(ii_V_on_policy[0], ii_V_ref_pd), label=\"On-Policy - Standard\")\n",
    "plt.plot(mean_squared_error(ii_V_on_policy_pd[0], ii_V_ref_pd), label=\"On-Policy - Per Decision\")\n",
    "plt.legend(prop={'size': 10})\n",
    "\n",
    "fig.add_subplot(4, 4, 15)\n",
    "for index in np.arange(0, 100, 1):\n",
    "    plt.plot(mean_squared_error(ii_V_on_policy[index], ii_V_ref_pd))\n",
    "plt.yscale('log')\n",
    "plt.ylim(10**(-5), 10**(8))\n",
    "\n",
    "fig.add_subplot(4, 4, 16)\n",
    "for index in np.arange(0, 100, 1):\n",
    "    plt.plot(mean_squared_error(ii_V_on_policy_pd[index], ii_V_ref_pd))\n",
    "plt.yscale('log')\n",
    "plt.ylim(10**(-5), 10**(8))\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III) Fixed Data Generating Policy: Exponentially Small Target Policies "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We expect that the small sampling ratios occurring for most actions will bias the model towards previously seen values, since it will tend to bootstrap more. This should induce a lot of variance in the standard formula case, and should perform better in the per-decision case because of the control variate term. There will be a large amplifying effect for unlikely actions like taking a left, but we expect this to be overwhelmed by the sheer number of \"right\" actions taken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iii_V_pi_0 = run(100, 1000, 4, 1, 1, 0.9, 50, 0.05)\n",
    "iii_V_pi_1 = run(100, 1000, 4, 1, 0.1, 0.9, 50, 0.05)\n",
    "iii_V_pi_2 = run(100, 1000, 4, 1, 0.01, 0.9, 50, 0.05)\n",
    "iii_V_pi_3 = run(100, 1000, 4, 1, 0.001, 0.9, 50, 0.05)\n",
    "iii_V_on_policy = run(100, 1000, 4, 1, 0.9, 0.9, 50, 0.05)\n",
    "\n",
    "iii_V_ref = np.mean(iii_V_on_policy[:,-1,:], axis=0)\n",
    "\n",
    "iii_avg_MSE_pi_0 = avg_mean_squared_error(iii_V_pi_0, iii_V_ref)\n",
    "iii_avg_MSE_pi_1 = avg_mean_squared_error(iii_V_pi_1, iii_V_ref)\n",
    "iii_avg_MSE_pi_2 = avg_mean_squared_error(iii_V_pi_2, iii_V_ref)\n",
    "iii_avg_MSE_pi_3 = avg_mean_squared_error(iii_V_pi_3, iii_V_ref)\n",
    "iii_avg_MSE_on_policy = avg_mean_squared_error(iii_V_on_policy, iii_V_ref)\n",
    "\n",
    "iii_MSE_pi_0 = mean_squared_error(iii_V_pi_0[0], iii_V_ref)\n",
    "iii_MSE_pi_1 = mean_squared_error(iii_V_pi_1[0], iii_V_ref)\n",
    "iii_MSE_pi_2 = mean_squared_error(iii_V_pi_2[0], iii_V_ref)\n",
    "iii_MSE_pi_3 = mean_squared_error(iii_V_pi_3[0], iii_V_ref)\n",
    "iii_MSE_on_policy = mean_squared_error(iii_V_on_policy[0], iii_V_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iii_V_pi_0_pd = run(100, 1000, 4, 1, 1, 0.9, 50, 0.05, per_decision=True)\n",
    "iii_V_pi_1_pd = run(100, 1000, 4, 1, 0.1, 0.9, 50, 0.05, per_decision=True)\n",
    "iii_V_pi_2_pd = run(100, 1000, 4, 1, 0.01, 0.9, 50, 0.05, per_decision=True)\n",
    "iii_V_pi_3_pd = run(100, 1000, 4, 1, 0.001, 0.9, 50, 0.05, per_decision=True)\n",
    "iii_V_on_policy_pd = run(100, 1000, 4, 1, 0.9, 0.9, 50, 0.05, per_decision=True)\n",
    "\n",
    "iii_V_ref_pd = np.mean(iii_V_on_policy_pd[:,-1,:], axis=0)\n",
    "\n",
    "iii_avg_MSE_pi_0_pd = avg_mean_squared_error(iii_V_pi_0_pd, iii_V_ref_pd)\n",
    "iii_avg_MSE_pi_1_pd = avg_mean_squared_error(iii_V_pi_1_pd, iii_V_ref_pd)\n",
    "iii_avg_MSE_pi_2_pd = avg_mean_squared_error(iii_V_pi_2_pd, iii_V_ref_pd)\n",
    "iii_avg_MSE_pi_3_pd = avg_mean_squared_error(iii_V_pi_3_pd, iii_V_ref_pd)\n",
    "iii_avg_MSE_on_policy_pd = avg_mean_squared_error(iii_V_on_policy_pd, iii_V_ref_pd)\n",
    "\n",
    "iii_MSE_pi_0_pd = mean_squared_error(iii_V_pi_0_pd[0], iii_V_ref_pd)\n",
    "iii_MSE_pi_1_pd = mean_squared_error(iii_V_pi_1_pd[0], iii_V_ref_pd)\n",
    "iii_MSE_pi_2_pd = mean_squared_error(iii_V_pi_2_pd[0], iii_V_ref_pd)\n",
    "iii_MSE_pi_3_pd = mean_squared_error(iii_V_pi_3_pd[0], iii_V_ref_pd)\n",
    "iii_MSE_pi_on_policy_pd = mean_squared_error(iii_V_on_policy_pd[0], iii_V_ref_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,20))\n",
    "\n",
    "fig.add_subplot(5, 4, 1)\n",
    "plt.title(\"Average: 100 Simulations\", size=20)\n",
    "plt.plot(iii_avg_MSE_on_policy, label=\"On-Policy - Standard\")\n",
    "plt.plot(iii_avg_MSE_on_policy_pd, label=\"On-Policy - Per Decision\")\n",
    "plt.legend(prop={'size': 10})\n",
    "plt.ylabel(\"On-Policy: $b(Right)=90\\%$\", size=20)\n",
    "\n",
    "fig.add_subplot(5, 4, 2)\n",
    "plt.title(\"Single Simulation\", size=20)\n",
    "plt.plot(iii_MSE_pi_on_policy_pd, label=\"On-Policy - Standard\")\n",
    "plt.plot(iii_MSE_pi_on_policy_pd, label=\"On-Policy - Per Decision\")\n",
    "plt.legend(prop={'size': 10})\n",
    "\n",
    "fig.add_subplot(5, 4, 3)\n",
    "plt.title(\"All Simulations: Standard\", size=20)\n",
    "for index in np.arange(0, 100, 1):\n",
    "    plt.plot(mean_squared_error(iii_V_on_policy_pd[index], iii_V_ref_pd))\n",
    "plt.yscale('log')\n",
    "\n",
    "fig.add_subplot(5, 4, 4)\n",
    "plt.title(\"All Simulations: Per Decision\", size=20)\n",
    "for index in np.arange(0, 100, 1):\n",
    "    plt.plot(mean_squared_error(iii_V_on_policy_pd[index], iii_V_ref_pd))\n",
    "plt.yscale('log')\n",
    "\n",
    "fig.add_subplot(5, 4, 5)\n",
    "plt.plot(iii_avg_MSE_on_policy_pd, label=\"On-Policy\")\n",
    "plt.plot(iii_avg_MSE_pi_0, label=\"Off-Policy - Standard\")\n",
    "plt.plot(iii_avg_MSE_pi_0_pd, label=\"Off-Policy - Per-Decision\")\n",
    "plt.legend(prop={'size': 10})\n",
    "plt.ylabel(\"$b(Right)=100\\%$\", size=20)\n",
    "\n",
    "fig.add_subplot(5, 4, 6)\n",
    "plt.plot(iii_avg_MSE_on_policy_pd, label=\"On-Policy\")\n",
    "plt.plot(iii_MSE_pi_0, label=\"Off-Policy - Standard\")\n",
    "plt.plot(iii_MSE_pi_0_pd, label=\"Off-Policy - Per-Decision\")\n",
    "plt.legend(prop={'size': 10})\n",
    "\n",
    "fig.add_subplot(5, 4, 7)\n",
    "for index in np.arange(0, 100, 1):\n",
    "    plt.plot(mean_squared_error(iii_V_pi_0[index], iii_V_ref_pd))\n",
    "plt.yscale('log')\n",
    "\n",
    "fig.add_subplot(5, 4, 8)\n",
    "for index in np.arange(0, 100, 1):\n",
    "    plt.plot(mean_squared_error(iii_V_pi_0_pd[index], iii_V_ref_pd))\n",
    "plt.yscale('log')\n",
    "\n",
    "fig.add_subplot(5, 4, 9)\n",
    "plt.plot(iii_avg_MSE_on_policy_pd, label=\"On-Policy\")\n",
    "plt.plot(iii_avg_MSE_pi_1, label=\"Off-Policy - Standard\")\n",
    "plt.plot(iii_avg_MSE_pi_1_pd, label=\"Off-Policy - Per-Decision\")\n",
    "plt.legend(prop={'size': 10})\n",
    "plt.ylabel(\"$b(Right)=10\\%$\", size=20)\n",
    "plt.yscale('log')\n",
    "\n",
    "\n",
    "fig.add_subplot(5, 4, 10)\n",
    "plt.plot(iii_avg_MSE_on_policy_pd, label=\"On-Policy\")\n",
    "plt.plot(iii_MSE_pi_1, label=\"Off-Policy - Standard\")\n",
    "plt.plot(iii_MSE_pi_1_pd, label=\"Off-Policy - Per-Decision\")\n",
    "plt.legend(prop={'size': 10})\n",
    "plt.yscale('log')\n",
    "\n",
    "\n",
    "fig.add_subplot(5, 4, 11)\n",
    "for index in np.arange(0, 100, 1):\n",
    "    plt.plot(mean_squared_error(iii_V_pi_1[index], iii_V_ref_pd))\n",
    "plt.yscale('log')\n",
    "\n",
    "fig.add_subplot(5, 4, 12)\n",
    "for index in np.arange(0, 100, 1):\n",
    "    plt.plot(mean_squared_error(iii_V_pi_1_pd[index], iii_V_ref_pd))\n",
    "plt.yscale('log')\n",
    "\n",
    "fig.add_subplot(5, 4, 13)\n",
    "plt.plot(iii_avg_MSE_on_policy_pd, label=\"On-Policy\")\n",
    "plt.plot(iii_avg_MSE_pi_2, label=\"Off-Policy - Standard\")\n",
    "plt.plot(iii_avg_MSE_pi_2_pd, label=\"Off-Policy - Per-Decision\")\n",
    "plt.legend(prop={'size': 10})\n",
    "plt.ylabel(\"$b(Right)=1\\%$\", size=20)\n",
    "plt.yscale('log')\n",
    "\n",
    "\n",
    "fig.add_subplot(5, 4, 14)\n",
    "plt.plot(iii_avg_MSE_on_policy_pd, label=\"On-Policy\")\n",
    "plt.plot(iii_MSE_pi_2, label=\"Off-Policy - Standard\")\n",
    "plt.plot(iii_MSE_pi_2_pd, label=\"Off-Policy - Per-Decision\")\n",
    "plt.legend(prop={'size': 10})\n",
    "plt.yscale('log')\n",
    "\n",
    "\n",
    "fig.add_subplot(5, 4, 15)\n",
    "for index in np.arange(0, 100, 1):\n",
    "    plt.plot(mean_squared_error(iii_V_pi_2[index], iii_V_ref_pd))\n",
    "plt.yscale('log')\n",
    "\n",
    "fig.add_subplot(5, 4, 16)\n",
    "for index in np.arange(0, 100, 1):\n",
    "    plt.plot(mean_squared_error(iii_V_pi_2_pd[index], iii_V_ref_pd))\n",
    "plt.yscale('log')\n",
    "\n",
    "fig.add_subplot(5, 4, 17)\n",
    "plt.plot(iii_avg_MSE_on_policy_pd, label=\"On-Policy\")\n",
    "plt.plot(iii_avg_MSE_pi_3, label=\"Off-Policy - Standard\")\n",
    "plt.plot(iii_avg_MSE_pi_3_pd, label=\"Off-Policy - Per-Decision\")\n",
    "plt.legend(prop={'size': 10})\n",
    "plt.ylabel(\"$b(Right)=0.1\\%$\", size=20)\n",
    "plt.yscale('log')\n",
    "\n",
    "\n",
    "fig.add_subplot(5, 4, 18)\n",
    "plt.plot(iii_avg_MSE_on_policy_pd, label=\"On-Policy\")\n",
    "plt.plot(iii_MSE_pi_3, label=\"Off-Policy - Standard\")\n",
    "plt.plot(iii_MSE_pi_3_pd, label=\"Off-Policy - Per-Decision\")\n",
    "plt.legend(prop={'size': 10})\n",
    "plt.yscale('log')\n",
    "\n",
    "\n",
    "fig.add_subplot(5, 4, 19)\n",
    "for index in np.arange(0, 100, 1):\n",
    "    plt.plot(mean_squared_error(iii_V_pi_3[index], iii_V_ref_pd))\n",
    "plt.yscale('log')\n",
    "\n",
    "fig.add_subplot(5, 4, 20)\n",
    "for index in np.arange(0, 100, 1):\n",
    "    plt.plot(mean_squared_error(iii_V_pi_3_pd[index], iii_V_ref_pd))\n",
    "plt.yscale('log')\n",
    "\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV) Fixed Target Policies  Policy: Exponentially Small Data Generating Policies "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tackling this case would require changing the MDP under consideration (to have termination on the left), which in turn would require a significant investment of time. We believe we have explored the problem sufficiently for our purposes thus far.\n",
    "\n",
    "We would expect that in exploring this case we would end up with high-variance estimated value functions, given the amplifying effect of the importance sampling ratio for the most likely action \"right\". This should overwhelm the dampening effect of the importance sampling ratio on the \"left\" action, which is much less likely. The control variate should help stabilize the estimated results for left actions. Thus we expect then the per-decision formula would perform better in this case as well.\n",
    "\n",
    "We defer exploration of this to another time -- maybe after refactoring the code above to make it way less repetitive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
