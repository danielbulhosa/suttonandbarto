{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import scipy as sp\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S&B 2.5\n",
    "\n",
    "## I. Single Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization\n",
    "q_star_i = 0 # can be modified\n",
    "k = 10 # Number of levers for the bandit\n",
    "steps = 10000\n",
    "# The cumulative drift in each q* due to the random walk\n",
    "# Indices along axis 0 correspond to action, a = 0,...,9\n",
    "cum_walk = np.cumsum(0.01 * np.random.randn(k, steps), axis=1)\n",
    "q_star = q_star_i + cum_walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the drift of the q*'s to get a sense of expectations\n",
    "plt.figure(figsize=(20,15))\n",
    "plt.plot(q_star.T)\n",
    "plt.title(\"Cumulative drift of q*'s\", size=40)\n",
    "plt.xlabel(\"Step Number\", size=40)\n",
    "plt.xticks(size=30)\n",
    "plt.ylabel(\"q* Value\", size=40)\n",
    "plt.yticks(size=30)\n",
    "plt.legend(range(k), prop={'size':30})\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This represents the samples actually draw by the agent\n",
    "# Summing q* here sets the mean to q*(action, step)\n",
    "bandit_samples = q_star + np.random.randn(k, steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bandit():\n",
    "    \n",
    "    def __init__(self, k, epsilon):\n",
    "        self.estimates = np.zeros(k)\n",
    "        self.step_count = 0\n",
    "        self.k = k\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def sample_policy(self):\n",
    "        is_exploring = np.random.binomial(1, self.epsilon)\n",
    "        \n",
    "        if not is_exploring:\n",
    "            max_Q = np.amax(self.estimates)\n",
    "            greedy_actions = np.argwhere(max_Q == self.estimates).flatten()\n",
    "            next_action = np.random.choice(greedy_actions)\n",
    "            \n",
    "        else:\n",
    "            next_action = np.random.randint(0, self.k)\n",
    "        \n",
    "        return next_action\n",
    "        \n",
    "    def update_estimates(self, lever_num, sample):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "class AverageBandit(Bandit):\n",
    "    \n",
    "    def update_estimates(self, lever_num, sample):\n",
    "        self.step_count += 1\n",
    "        self.estimates[lever_num] = self.estimates[lever_num] + \\\n",
    "            + (1/self.step_count) * (sample - self.estimates[lever_num])\n",
    "        \n",
    "class ConstantStepBandit(Bandit):\n",
    "    \n",
    "    def __init__(self, k, epsilon, alpha):\n",
    "        self.alpha = alpha\n",
    "        Bandit.__init__(self, k, epsilon)\n",
    "    \n",
    "    def update_estimates(self, lever_num, sample):\n",
    "        self.estimates[lever_num] = self.estimates[lever_num] + \\\n",
    "            + (self.alpha) * (sample - self.estimates[lever_num])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.1\n",
    "alpha = 0.1\n",
    "average_bandit = AverageBandit(k, epsilon)\n",
    "step_bandit = ConstantStepBandit(k, epsilon, alpha)\n",
    "bandits = [average_bandit, step_bandit]\n",
    "reward_curves = np.empty((2, steps))\n",
    "action_taken = np.empty((2, steps))\n",
    "\n",
    "for step in range(steps):\n",
    "    if step % 500 == 0:\n",
    "        print(\"{} steps have been executed...\".format(step))\n",
    "    \n",
    "    for bandit_index, bandit in enumerate(bandits):\n",
    "        next_action = bandit.sample_policy()\n",
    "        reward = bandit_samples[next_action, step]\n",
    "        bandit.update_estimates(next_action, reward)\n",
    "        reward_curves[bandit_index, step] = reward\n",
    "        action_taken[bandit_index, step] = next_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_actions = np.argmax(q_star, axis=0)\n",
    "optimal_action_taken = (optimal_actions == action_taken)\n",
    "cumulative_rewards = np.cumsum(reward_curves, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(cumulative_rewards.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Multiple Simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bandit_samples(q_star_i, k ,steps):\n",
    "    cum_walk = np.cumsum(0.01 * np.random.randn(k, steps), axis=1)\n",
    "    q_star = q_star_i + cum_walk\n",
    "    bandit_samples = q_star + np.random.randn(k, steps)\n",
    "    \n",
    "    return q_star, bandit_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_simulations(epsilon, alpha, simulations, q_star_i, k ,steps):\n",
    "    \n",
    "    reward_curves = np.empty((2, simulations, steps))\n",
    "    action_taken = np.empty((2, simulations, steps))\n",
    "    q_star_sims = np.empty((simulations, k, steps))\n",
    "    t_avg = 0\n",
    "    \n",
    "    for sim in range(simulations):\n",
    "        t_i = time.time()\n",
    "        \n",
    "        average_bandit = AverageBandit(k, epsilon)\n",
    "        step_bandit = ConstantStepBandit(k, epsilon, alpha)\n",
    "        bandits = [average_bandit, step_bandit]\n",
    "        q_star, bandit_samples = generate_bandit_samples(q_star_i, k, \n",
    "                                                         steps)\n",
    "        q_star_sims[sim] = q_star\n",
    "\n",
    "        for step in range(steps):\n",
    "\n",
    "            for bandit_index, bandit in enumerate(bandits):\n",
    "                next_action = bandit.sample_policy()\n",
    "                reward = bandit_samples[next_action, step]\n",
    "                bandit.update_estimates(next_action, reward)\n",
    "                reward_curves[bandit_index, sim, step] = reward\n",
    "                action_taken[bandit_index, sim, step] = next_action\n",
    "                \n",
    "        t_avg = (time.time() - t_i)/(sim + 1) + sim/(sim+1) * t_avg \n",
    "        t_left = (simulations - sim - 1) * t_avg\n",
    "        \n",
    "        if (sim + 1) % 100 == 0:\n",
    "            print(\"\\nSimulation #{}\".format(sim + 1))\n",
    "            print(\"Avg. Sim. Time: {} s\".format(round(t_avg, 2)))\n",
    "            print(\"Expected Time Remaining: {} mins\".format(round(t_left/60, 2)))\n",
    "        \n",
    "            \n",
    "    optimal_actions = np.argmax(q_star_sims, axis=1)\n",
    "    optimal_action_perc = np.average(optimal_actions == action_taken,\n",
    "                                     axis=1)\n",
    "    avg_rewards = np.average(reward_curves, axis=1)\n",
    "    \n",
    "    return optimal_action_perc, avg_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Could be implemented more efficiently if simulations vectorized\n",
    "# This implementation is enough for our purposes as it is though\n",
    "optimal_action_perc, avg_rewards = \\\n",
    "    run_simulations(epsilon, alpha, 2000, q_star_i, k, steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,15))\n",
    "plt.plot(optimal_action_perc.T)\n",
    "plt.title(\"% Optimal Action Selected, Average Over 2,000 Sims\", size=40)\n",
    "plt.xlabel(\"Step Number\", size=40)\n",
    "plt.xticks(size=30)\n",
    "plt.ylabel(\"% Optimal Action\", size=40)\n",
    "plt.yticks(size=30)\n",
    "plt.legend(range(k), prop={'size':30})\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,15))\n",
    "plt.plot(avg_rewards.T)\n",
    "plt.title(\"Average Rewards, Average Over 2,000 Sims\", size=40)\n",
    "plt.xlabel(\"Step Number\", size=40)\n",
    "plt.xticks(size=30)\n",
    "plt.ylabel(\"Average Reward\", size=40)\n",
    "plt.yticks(size=30)\n",
    "plt.legend(range(k), prop={'size':30})\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see that the constant step averaging performs significantly better since it can capture the non-stationarity of the problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S&B  4.7 Jack's Car Rental Modified\n",
    "\n",
    "Let's let $V(c_1, c_2)$ be the value function, which depends on the number of cars at the first and second location respectively. Thus $s = (c_1, c_2)$, but it is more convenient to work with the individual car counts.\n",
    "\n",
    "There are a few constraints we need to enforce:\n",
    "\n",
    "+ We can never have more than 20 cars at any location, so we need to sum the tails of the Poisson distributions for returns.\n",
    "+ We can never have less than 0 cars at any location, so we need to sum the tails for the Poisson distributions for the rentals.\n",
    "+ The policy for any pair state $s=(c_1, c_2)$ can only take on certain values. These are determined by the fact that we can move a maximum of $b(=5)$ cars overnight and the fact that we can be no more than $t_1, t_2(=20)$ cars at each location:\n",
    "\n",
    "$$\\mathscr{A}(c_1, c_2) = [-min(t_1 - c_1, min(c_2, b)), min(t_2 - c_2, min(c_1, b))] \\cap \\mathbb{Z}$$\n",
    "\n",
    "Let $req_1$, $req_2$ and $ret_1$, $ret_2$ be the number of cars requested and returned respectively for each of the locations. Let $t_1$, $t_2$ denote the top number of cars we can hold at each location$. Then note that the maximum number of cars that can be rented each day is equal to the number of cars after transfers are accounted for:\n",
    "\n",
    "$$ \n",
    "\\begin{align*}\n",
    "maxrent_1 &= C_1 - \\pi(C_1, C_2) \\\\\n",
    "maxrent_2 &= C_2 + \\pi(C_1, C_2)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Here $C_i$ is the state today. And the state the next day is:\n",
    "\n",
    "$$ \n",
    "\\begin{align*}\n",
    "C_1' &= min[max[C_1 - Req_1 - \\pi(C_1, C_2), \\ 0] + Ret_1, \\ t_1] \\\\\n",
    "C_2' &= min[max[C_2 - Req_2 + \\pi(C_1, C_2), \\ 0] + Ret_2, \\ t_2]\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Here $Ret_i$, $Req_i$ are the cars requested and returned tomorrow day, and $C_i'$ is the state tomorrow. We use the capital letters to denote that these are random variables rather than specific values. The max and min functions guarantee that more than the number of cars available are not rented, and that we do not hold more than $t_1, t_2$ cars per location, respectively.\n",
    "\n",
    "Finally, if we let $R_i$ denote the reward we made at each location, $P$ denotes the cost of moving the cars, $p$ the price of moving a car between locations, and $m$ denotes the amount of money we make per rental then we have that:\n",
    "\n",
    "$$ \n",
    "\\begin{align*}\n",
    "R_1 &= m \\cdot min[Req_1, \\ maxrent_1(C_1, C_2)] \\\\\n",
    "R_2 &= m \\cdot min[Req_2, \\ maxrent_2(C_1, C_2)] \\\\ \n",
    "P &= p \\cdot \\left| \\pi(C_1, C_2) \\right|\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "And the total reward is just $R = R_1 + R_2 - P$. Since our problem is defined in terms of the distributions for $Req_i$ and $Ret_i$ it's easier to write the sum required for the update rule in terms of these variables. This is why we have derived these equations. We can use them now to rewrite the update rule -- note that every term in the expectation is a function of $C_1, C_2, ret_1, ret_2, req_1, ret_2$, so since the former 2 variables are fixed it suffices to take the expectation over the latter four:\n",
    "\n",
    "$$ \n",
    "\\begin{align*}\n",
    "V(s) &\\leftarrow q_{\\pi}(s, \\pi(s)) = E_{Ret_1, Ret_2, Req_1, Req_2}[R + \\gamma V(C_1', C_2') \\ | \\ S=(C_1,C_2), A=\\pi(C_1, C_2)]\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Here $V$ is our current estimate of $v_{\\pi}$, which is a lookup table of values. The expectation is taken over the distribution:\n",
    "\n",
    "$$ p(req_1, req_2, ret_1, ret_2) = p(req_1) \\cdot p(req_2) \\cdot p(ret_1) \\cdot p(ret_2)$$\n",
    "\n",
    "Where each of the indepedent distributions is a Poisson distribution with means $\\lambda_{req_1}$, $\\lambda_{req_2}$, $\\lambda_{ret_1}$, and $\\lambda_{ret_2}$ respectively.\n",
    "\n",
    "Given this formula for the expectation we can calculate it in many ways:\n",
    "\n",
    "\n",
    "+ Analytically computing the expectation for each term and subterm within the expectation, to derive a closed formula for the expectation. If we did this we may be able to sum the tail of the probability distributions as aforementioned. However this analytical computation is likely complex (in particular for $C_1', C_2'$) and not worth the increased precision.\n",
    "\n",
    "\n",
    "+ Calculating the value of the probability and of the variables, and calculating the infinite sum in the expectation until the tail of the sum gets sufficiently small. This is not a bad approach but it raises the question of how to order the terms in the sum so that the terms are monotonically decreasing in size.\n",
    "\n",
    "\n",
    "+ Sampling the probability distribution in order to produce a Monte Carlo estimate of $V(s)$. Note that even though we are estimating the expecation with MC in this case, this is NOT the same as the Reinforcement Learning MC approach of sampling trajectories. This would still be a Dynamic Programming approach, but with MC estimation of the environment distribution as a subroutine.\n",
    "\n",
    "The simplest implementation seemed to be the Monte Carlo based one so we attempted this alternative. Unfortunately we did not achieve sufficient accuracy with this approach for the policy to converge. It appears that even with $10^6$ samples we get $O(1)$ error in the policy matrix which means the policy does not converge. Fortunately we overestimated the complexity of motivating the sum truncation approach analytically. We note that each of the Poisson distributions is independent and that if we sum all of the terms for each random variable up to the $N$th term, the the largest the error can be is bounded by $O(1/N!)$. The factorial grows superexponentially, so even at $N=30$ we get an error less than $(10^{-33})$ and we only have to sum less than $10^6$ sum tertms. Thus we attempt this approach next.\n",
    "\n",
    "\n",
    "Before moving on to the implementation, we develop the theory behind the modifications required for Exercise 4.7. There are two:\n",
    "\n",
    "+ An employee is willing to move one car from location 1 to location 2 for free.\n",
    "+ If more than 10 cars are kept at either location an extra $\\$4$ dollars must be paid to use an extra parking lot, at  each (for a poissible total of $\\$8$).\n",
    "\n",
    "Note that both of these are simply modifications to the cost term $P$ of the reward function. Thus to apply the modifications we can simply define a new cost term:\n",
    "\n",
    "$$ P_{mod} = p \\cdot \\left| \\pi(C_1, C_2) \\right| - p \\cdot H(\\pi(C_1, C_2) - 1) + $_{park, 1} \\cdot H(maxrent_1 - 10) + $_{park,2} \\cdot H(maxrent_2 - 10) $$\n",
    "\n",
    "Here $H$ is the Heaviside step function with the convention that $H(0) = 1$ and $\\$_{park, i}(=4)$ is the parking lot cost. Thus we can simply switch out the code for calculating the cost in the two trials to modify the problem. Monte Carlo estimation of $V(s)$ will work equally well in both cases -- in fact since P_{mod} depends on variables that we condition on the new cost terms doesn't add any additional MC computational cost. Now we are finally ready to implement Policy Iteration on this problem.\n",
    "\n",
    "Let $\\theta$ be the policy evaluation error and $N$ the number of samples drawn for estimating each value of $V(s)$ by MC approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "t = [20, 20]\n",
    "b = 5\n",
    "lambda_req = [3, 4]\n",
    "lambda_ret = [3, 2]\n",
    "m, p, cost_park = 10, 2, [4, 4]\n",
    "theta, N = 1, 10**6\n",
    "gamma = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define methods\n",
    "def action_set(c_1, c_2, t=t, b=b):\n",
    "    lower_bound = - min(t[0] - c_1, min(c_2, b))\n",
    "    upper_bound = min(t[1] - c_2, min(c_1, b))\n",
    "    return np.arange(lower_bound, upper_bound + 1)\n",
    "\n",
    "def initialize(t=t, b=b):\n",
    "    V = np.zeros((t[0] + 1, t[1] + 1))\n",
    "    pi = np.zeros((t[0] + 1, t[1] + 1), dtype=int)\n",
    "\n",
    "    return V, pi\n",
    "\n",
    "def H(x):\n",
    "    return int(x >= 0)\n",
    "\n",
    "def state_policy_evaluation(c_1, c_2, a, V, gamma=gamma, P_mod=False, t=t, b=b, \n",
    "                            labmda_req=lambda_req, lambda_ret=lambda_ret, \n",
    "                            m=m , p=p, cost_park=cost_park, N=N):\n",
    "    \n",
    "    req_1 = np.random.poisson(lambda_req[0], N)\n",
    "    req_2 = np.random.poisson(lambda_req[1], N)\n",
    "    ret_1 = np.random.poisson(lambda_ret[0], N)\n",
    "    ret_2 = np.random.poisson(lambda_ret[1], N)\n",
    "    \n",
    "    maxrent_1 = c_1 - a\n",
    "    maxrent_2 = c_2 + a\n",
    "    \n",
    "    r_1 = m * np.minimum(req_1, maxrent_1)\n",
    "    r_2 = m * np.minimum(req_2, maxrent_2)\n",
    "    price = p * abs(a)\n",
    "    \n",
    "    if P_mod:\n",
    "        price += - p * H(a - 1) + cost_park[0] * H(maxrent_1 - 10) \\\n",
    "                 + cost_park[1] * H(maxrent_2 - 10)\n",
    "        \n",
    "    r = r_1 + r_2 - price\n",
    "    \n",
    "    c_prime_1 = np.minimum(np.maximum(maxrent_1 - req_1, 0) + ret_1, t[0])\n",
    "    c_prime_2 = np.minimum(np.maximum(maxrent_2 - req_2, 0) + ret_2, t[1])\n",
    "    \n",
    "    V_s = np.average(r + gamma * V[(c_prime_1, c_prime_2)])\n",
    "    \n",
    "    return V_s\n",
    "\n",
    "def state_policy_evaluation_sum(c_1, c_2, a, V, gamma=gamma, P_mod=False, t=t, b=b, \n",
    "                                labmda_req=lambda_req, lambda_ret=lambda_ret, \n",
    "                                m=m , p=p, cost_park=cost_park, N=N):\n",
    "    \n",
    "    V_s = 0\n",
    "    \n",
    "    poisson_vals = np.arange(N + 1)\n",
    "    \n",
    "    req_probs_1 = stats.poisson(lambda_req[0]).pmf(poisson_vals)\n",
    "    req_probs_2 = stats.poisson(lambda_req[1]).pmf(poisson_vals)\n",
    "    ret_probs_1 = stats.poisson(lambda_ret[0]).pmf(poisson_vals)\n",
    "    ret_probs_2 = stats.poisson(lambda_ret[1]).pmf(poisson_vals)\n",
    "    \n",
    "    maxrent_1 = c_1 - a\n",
    "    maxrent_2 = c_2 + a\n",
    "    \n",
    "    price = p * abs(a)\n",
    "    \n",
    "    if P_mod:\n",
    "        price += - p * H(a - 1) + cost_park[0] * H(maxrent_1 - 10) \\\n",
    "                 + cost_park[1] * H(maxrent_2 - 10)\n",
    "    \n",
    "    # Considered doing this as a tensor operation but it gets tricky.\n",
    "    # May come back to that approach if this is too inefficient.\n",
    "    for req_1, req_prob_1 in enumerate(req_probs_1):\n",
    "        for req_2, req_prob_2 in enumerate(req_probs_2):\n",
    "            for ret_1, ret_prob_1 in enumerate(ret_probs_1):\n",
    "                for ret_2, ret_prob_2 in enumerate(ret_probs_2):\n",
    "                    r_1 = m * min(req_1, maxrent_1)\n",
    "                    r_2 = m * min(req_2, maxrent_2)\n",
    "                    r = r_1 + r_2 - price\n",
    "                    c_prime_1 = min(max(maxrent_1 - req_1, 0) + ret_1, t[0])\n",
    "                    c_prime_2 = min(max(maxrent_2 - req_2, 0) + ret_2, t[1])\n",
    "                    \n",
    "                    V_s += req_prob_1*req_prob_2*ret_prob_1*ret_prob_2* \\\n",
    "                            (r + gamma * V[(c_prime_1, c_prime_2)])\n",
    "    \n",
    "    return V_s\n",
    "\n",
    "def state_policy_evaluation_vec(c_1, c_2, a, V, gamma=gamma, P_mod=False, t=t, b=b, \n",
    "                                labmda_req=lambda_req, lambda_ret=lambda_ret, \n",
    "                                m=m , p=p, cost_park=cost_park, N=N):\n",
    "    \n",
    "    poisson_vals = np.arange(N + 1, dtype=int)\n",
    "    \n",
    "    # Probability tensor, shape (N + 1, N + 1, N + 1, N + 1)\n",
    "    req_probs_1 = stats.poisson(lambda_req[0]).pmf(poisson_vals) # req_1 axis\n",
    "    req_probs_2 = stats.poisson(lambda_req[1]).pmf(poisson_vals) # req_2 axis\n",
    "    ret_probs_1 = stats.poisson(lambda_ret[0]).pmf(poisson_vals) # ret_1 axis\n",
    "    ret_probs_2 = stats.poisson(lambda_ret[1]).pmf(poisson_vals) # ret_2 axis\n",
    "    # axes ret_1, req_1, ret_2, req_2\n",
    "    p_tensor = np.multiply.outer(\n",
    "                   np.multiply.outer(\n",
    "                        np.multiply.outer(ret_probs_1, req_probs_1), \n",
    "                           ret_probs_2), \n",
    "                               req_probs_2)\n",
    "    \n",
    "    assert p_tensor.shape == (N + 1, N + 1, N + 1, N + 1), \"prob tensor is not the correct shape. \" + \\\n",
    "                                                           \"Current shape is {}\".format(p_tensor.shape)\n",
    "    \n",
    "    # Price and bound scalars\n",
    "    maxrent_1 = c_1 - a\n",
    "    maxrent_2 = c_2 + a\n",
    "    \n",
    "    price = p * abs(a)\n",
    "    \n",
    "    if P_mod:\n",
    "        price += - p * H(a - 1) + cost_park[0] * H(maxrent_1 - 10) \\\n",
    "                 + cost_park[1] * H(maxrent_2 - 10)\n",
    "        \n",
    "    # Reward matrix, shape (N + 1, N + 1, N + 1, N + 1)\n",
    "    r_1 = m * np.minimum(poisson_vals, maxrent_1)\n",
    "    r_2 = m * np.minimum(poisson_vals, maxrent_2)\n",
    "    # axes ret_1, req_1, ret_2, req_2\n",
    "    r = np.multiply.outer(\n",
    "            np.multiply.outer(\n",
    "                np.multiply.outer(np.ones(N + 1), r_1\n",
    "                ), np.ones(N + 1) \n",
    "        ), r_2\n",
    "    ) -  price\n",
    "    \n",
    "    assert r.shape == (N + 1, N + 1, N + 1, N + 1), \"r tensor is not the correct shape. \" + \\\n",
    "                                                    \"Current shape is {}\".format(r.shape)\n",
    "        \n",
    "    # c prime matrices, shape (N + 1, N + 1, N + 1, N + 1) each\n",
    "    # axes ret_1, req_1\n",
    "    c_prime_1 = np.minimum(np.add.outer(np.maximum(maxrent_1 - poisson_vals, 0), poisson_vals), t[0])\n",
    "    # axes ret_1, req_1, ret_2, req_2\n",
    "    c_prime_full_1 = np.multiply.outer(c_prime_1, np.ones((N + 1, N + 1), dtype=int))\n",
    "    assert c_prime_full_1.shape == (N + 1, N + 1, N + 1, N + 1), \"c prime 1 tensor is not the correct shape. \" + \\\n",
    "                                                                 \"Current shape is {}\".format(c_prime_full_1.shape)\n",
    "    # axes ret_2, req_2\n",
    "    c_prime_2 = np.minimum(np.add.outer(np.maximum(maxrent_2 - poisson_vals, 0), poisson_vals), t[1])\n",
    "    # axes ret_1, req_1, ret_2, req_2\n",
    "    c_prime_full_2 = np.multiply.outer(np.ones((N + 1, N + 1), dtype=int), c_prime_2)\n",
    "    assert c_prime_full_2.shape == (N + 1, N + 1, N + 1, N + 1), \"c prime 2 tensor is not the correct shape. \"  + \\\n",
    "                                                                 \"Current shape is {}\".format(c_prime_full_2.shape)\n",
    "    \n",
    "    # All the tensors being combined should agree on their dimensions\n",
    "    V_s = np.sum(p_tensor * (r + gamma * V[c_prime_full_1, c_prime_full_2]))\n",
    "    \n",
    "    return V_s\n",
    "    \n",
    "def policy_evaluation(pi, V, gamma=gamma, P_mod=False, mode=\"sum\", t=t, b=b, \n",
    "                      labmda_req=lambda_req, lambda_ret=lambda_ret, \n",
    "                      m=m , p=p, cost_park=cost_park, N=N, theta=theta):\n",
    "    \n",
    "    if mode == \"sum\":\n",
    "        v_estimator = state_policy_evaluation_sum\n",
    "    elif mode == \"sum_vec\":\n",
    "        v_estimator = state_policy_evaluation_vec\n",
    "    else:\n",
    "        v_estimator = state_policy_evaluation\n",
    "    \n",
    "    while True:\n",
    "        Delta = 0\n",
    "        \n",
    "        times = []\n",
    "        \n",
    "        for c_1 in range(t[0] + 1):\n",
    "            for c_2 in range(t[1] + 1):\n",
    "                t_i = time.time()\n",
    "                v = V[c_1, c_2]\n",
    "                V[c_1, c_2] = v_estimator(c_1, c_2, pi[c_1, c_2], V, gamma, P_mod, t, b, \n",
    "                                          labmda_req, lambda_ret, m , p, cost_park, N)\n",
    "                Delta = max(Delta, abs(v - V[c_1, c_2]))\n",
    "                times.append(time.time() - t_i)\n",
    "                \n",
    "        avg_t = round(sum(times)/len(times), 2)\n",
    "        total_t = round(sum(times), 2)\n",
    "                \n",
    "        print(\"Current Delta value: {}. Avg. State Est. Time: {}s. Avg. Sweep Time: {}s\".format(Delta,\n",
    "                                                                                                avg_t,\n",
    "                                                                                                total_t))\n",
    "        \n",
    "        if Delta < theta:\n",
    "            break\n",
    "                \n",
    "                \n",
    "def policy_improvement(pi, V, gamma=gamma, P_mod=False, mode=\"sum\", t=t, b=b, \n",
    "                       labmda_req=lambda_req, lambda_ret=lambda_ret, \n",
    "                       m=m , p=p, cost_park=cost_park, N=N):\n",
    "    \n",
    "    if mode == \"sum\":\n",
    "        v_estimator = state_policy_evaluation_sum\n",
    "    elif mode == \"sum_vec\":\n",
    "        v_estimator = state_policy_evaluation_vec\n",
    "    else:\n",
    "        v_estimator = state_policy_evaluation\n",
    "    \n",
    "    policy_stable = True\n",
    "    \n",
    "    for c_1 in range(t[0] + 1):\n",
    "        for c_2 in range(t[1] + 1):\n",
    "            \n",
    "                old_action = pi[c_1, c_2]\n",
    "                new_action = None\n",
    "                new_value = - np.inf\n",
    "                \n",
    "                for a in action_set(c_1, c_2, t=t, b=b):\n",
    "                    temp_value = v_estimator(c_1, c_2, a, V, gamma, P_mod, t, b, \n",
    "                                             labmda_req, lambda_ret, m, p, cost_park, N)\n",
    "            \n",
    "                    if temp_value > new_value:\n",
    "                        new_value = temp_value\n",
    "                        new_action = a\n",
    "                \n",
    "                pi[c_1, c_2] = new_action\n",
    "                if new_action != old_action:\n",
    "                    policy_stable = False\n",
    "    \n",
    "    print(pi)\n",
    "                    \n",
    "    return policy_stable\n",
    "\n",
    "def policy_iteration(gamma=gamma, P_mod=False, mode=\"sum_vec\", t=t, b=b, \n",
    "                     labmda_req=lambda_req, lambda_ret=lambda_ret, \n",
    "                     m=m , p=p, cost_park=cost_park, N=N, theta=theta):\n",
    "    \n",
    "    if mode not in [\"sum\", \"mc\", \"sum_vec\"]:\n",
    "        raise ValueError(\"Parameter `mode` must be one of [`sum`, `mc`, `sum_vec`]\")\n",
    "    \n",
    "    policy_stable = None\n",
    "    V, pi = initialize(t, b)\n",
    "    V_list = []\n",
    "    pi_list = []\n",
    "    \n",
    "    print(\"Starting policy iteration with parameters \\n\" + \\\n",
    "          \"\\tN: {N}\\n\".format(N=N) + \\\n",
    "          \"\\tgamma: {gamma}\\n\".format(gamma=gamma) + \\\n",
    "          \"\\tP_mod: {P_mod}\\n\".format(P_mod=P_mod) + \\\n",
    "          \"\\tt: {t}\\n\".format(t=t) + \\\n",
    "          \"\\tb: {b}\\n\".format(b=b) + \\\n",
    "          \"\\tlambda_req: {lambda_req}\\n\".format(lambda_req=lambda_req) + \\\n",
    "          \"\\tlambda_ret: {lambda_ret}\\n\".format(lambda_ret=lambda_ret) + \\\n",
    "          \"\\tm: {m}\\n\".format(m=m) + \\\n",
    "          \"\\tp: {p}\\n\".format(p=p) + \\\n",
    "          \"\\tcost_park: {cost_park}\\n\".format(cost_park=cost_park) + \\\n",
    "          \"\\tt: {t}\\n\".format(t=t) + \\\n",
    "          \"\\ttheta: {theta}\\n\\n\".format(theta=theta)\n",
    "          )\n",
    "    \n",
    "    iteration = 0\n",
    "    avg_t = 0\n",
    "    \n",
    "    while True:\n",
    "        print(\"Iteration number {} start\".format(iteration + 1))\n",
    "        t_i = time.time()\n",
    "        \n",
    "        policy_evaluation(pi, V, gamma, P_mod, mode, t, b, labmda_req, \n",
    "                          lambda_ret, m, p, cost_park, N, theta)\n",
    "        print(\"Policy Evaluation Complete\")\n",
    "        \n",
    "        print(pi)\n",
    "        policy_stable = policy_improvement(pi, V, gamma, P_mod, mode, t, b, labmda_req, \n",
    "                                           lambda_ret, m, p, cost_park, N)\n",
    "        print(\"Policy Improvement Complete - Policy Stable: {}\".format(policy_stable))\n",
    "        \n",
    "        V_list.append(V)\n",
    "        pi_list.append(pi)\n",
    "        \n",
    "        if policy_stable:\n",
    "            break\n",
    "            \n",
    "        delta_t = round(time.time() - t_i, 2)\n",
    "        iteration += 1\n",
    "\n",
    "        print(\"Iteration number {} complete: Latest iteration time {}s\\n\\n\".format(iteration, delta_t))\n",
    "            \n",
    "    return V_list, pi_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replication of Results: Summary of Attempts\n",
    "\n",
    "### Fourth attempt (?): Cleaned vectorized sum trunctation approximation\n",
    "+ We used a trick to generate correct rank-4 tensors. Namely multiplying by all-1s tensors\n",
    "+ Could we perform better by directly broadcasting instead? Should be at least more memory efficient. It should also save on some opeerations. Could get a slight performance improvement\n",
    "+ Won't implement because current performance is satisfactory but wanted to summarize future attempt here\n",
    "\n",
    "### Third attempt: Vectorized sum trunctation approximation\n",
    "+ Can we go even faster by vectorizing sum approach? YES!!!\n",
    "+ 3 seconds, N=10, sum approx. with vectorization for single sweep. ~0.01s for one state on average\n",
    "+ A greater than 10x improvement on performance for the same estimated O(0.01) error\n",
    "+ We finally achieved convergence!!! We believe the second approach would too but it just takes too long\n",
    "\n",
    "### Second attempt: Unvectorized sum trunctation approximation\n",
    "+ 45 seconds, N = 10, sum approx. without vectorizing for single sweep. ~0.1s for one state. \n",
    "+ 3x improvement for what we believe is much better error bounds O(0.01)\n",
    "\n",
    "### First attempt: Vectorized MC approximation\n",
    "+ 127 seconds, N = 10^6, MC approximation fully vectorized. ~0.3s for one state\n",
    "+ We believe is O(1) error because of variance, based on a rough estimate and the fact we don't converge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V_list, pi_list = policy_iteration(N=10, mode='sum_vec', theta=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,10))         # Sample figsize in inches\n",
    "sns.heatmap(pi_list[len(pi_list)-1].T, annot=True,  ax=ax)\n",
    "ax.invert_yaxis()\n",
    "plt.title(\"Policy Function - $\\pi(C_1, C_2)$\", size=20)\n",
    "plt.xlabel(\"$C_1$\", size=20)\n",
    "plt.ylabel(\"$C_2$\", size=20)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V_list, pi_list = policy_iteration(N=10, P_mod=True, mode='sum_vec', theta=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see in the plot below that: \n",
    "+ Even at high $C_1$ and $C_2$ we still end up moving the one free car since it incurs no cost to us.\n",
    "+ We see lines at $C_1=10$ and $C_2=10$ where we'd rather move cars away from a lot that has fewer just to avoid the parking lot fee.\n",
    "+ Besides this we see essentialy the same features we saw in the original case, which makes sense!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,10))         # Sample figsize in inches\n",
    "sns.heatmap(pi_list[len(pi_list)-1].T, annot=True,  ax=ax)\n",
    "ax.invert_yaxis()\n",
    "plt.title(\"Policy Function - $\\pi(C_1, C_2)$\", size=20)\n",
    "plt.xlabel(\"$C_1$\", size=20)\n",
    "plt.ylabel(\"$C_2$\", size=20)\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
