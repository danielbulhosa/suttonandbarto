{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.ndimage\n",
    "import copy\n",
    "import seaborn as sns\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from mpmath import mpf, mpc, mp, log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S&B 4.9 - Gambler's Problem\n",
    "\n",
    "For reference the update rule for value iteration is:\n",
    "\n",
    "$$ V_{k+1}(s) = \\max_{a} \\sum_{s', r} p(s', r| s, a) \\left[ r + \\gamma \\cdot V_k(s') \\right]$$\n",
    "\n",
    "We ran the simulation and found that:\n",
    "\n",
    "+ For $p_h = 0.45$ we replicate the results in the book. If we wanted to replicate the policy graph on the curve we would need to account for numerical error and simply take the **minimum** argmax of the Q function. The argmax should in theory return many different possibilities, and the minimum of these would produce the policy in the book. However, as mentioned in the book there are multiple \"optimality\" contours per se in the value function.\n",
    "\n",
    "    + Some of the contours focus on gambling the minimal amount that will get us to the next quartile (similar to what we see in the book).\n",
    "    \n",
    "    + Other contours focus on gambling **everything**. This strategy makes sense because it minimizes the number of coin flips we make. \n",
    "    \n",
    "    + From the graphs of $Q(s,a)$ and $Q(a|s)$ we can see that these optimal strategies form a set of diamonds, with up and to the right edges of the diamonds representing \"betting it all trajectories\" and up and to the left edges representing \"betting just enough to get to the next quartile\" strategies.\n",
    "    \n",
    "    \n",
    "+ For $p_h = 0.5$ the value function is basically $V(s) = s/100$ and when $p_h > 0.5$ as $p_h$ increases the value function gets closer and closer to a square (meaning the likelihood of winning gets closer and closer to 1 for low $s$ states).\n",
    "\n",
    "\n",
    "+ As $p_h$ gets smaller and smaller the kinks in the value function get more and more extreme and defined. In the limit of extremely small $p_h$ it becomes essentially a step function. The step jumps occur when we have enough \"coins\" so that we reduce the integral number of flips needed to win. The heights of the steps correlate with $p_h^n$ where $n$ is the number of consecutive flips needed to win.\n",
    "\n",
    "We'll leave our analysis at that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp.dps = 200\n",
    "\n",
    "win_threshold = 100\n",
    "p_heads = mpf(1)*4/10\n",
    "log_theta = -200\n",
    "V = np.array([mpf(0) for i in range(win_threshold + 1)])\n",
    "pi = np.zeros(win_threshold + 1)\n",
    "Q = np.array([[mpf(0) for i in range(win_threshold + 1)] for j in range(win_threshold + 1)])\n",
    "\n",
    "def get_states_and_rewards(s):\n",
    "    actions = np.arange(1, s + 1, dtype=int)\n",
    "    win_cond = (s + actions) >= win_threshold\n",
    "    lose_cond = (s - actions) <= 0\n",
    "    reward = np.array([mpf(int(cond)) for cond in win_cond])\n",
    "    winning_states = np.where(win_cond, win_threshold, s + actions)\n",
    "    losing_states = np.where(lose_cond, 0, s - actions)\n",
    "    \n",
    "    return actions, reward, winning_states, losing_states\n",
    "\n",
    "counter = 0\n",
    "t_i = time.time()\n",
    "\n",
    "while True:\n",
    "    log_delta = -mp.inf\n",
    "    for s in range(1, win_threshold):\n",
    "        actions, reward, winning_states, losing_states = get_states_and_rewards(s)\n",
    "        v = V[s]\n",
    "        V[s] = np.max(p_heads * (reward + V[winning_states]) + (1 - p_heads) * (V[losing_states]))\n",
    "        \n",
    "        log_delta = max(log_delta, log(abs(V[s] - v)))\n",
    "        \n",
    "    counter += 1\n",
    "    \n",
    "    if counter % 10 == 0:\n",
    "        t_f = time.time()\n",
    "        delta_t = round(t_f - t_i, 2)\n",
    "        print(\"Iteration #{} complete in {} s. Current Log Delta: {} \".format(counter, delta_t, log_delta))\n",
    "        t_i = t_f\n",
    "    if log_delta < log_theta:\n",
    "        for s in range(1, win_threshold):\n",
    "            actions, reward, winning_states, losing_states = get_states_and_rewards(s)\n",
    "            Q_sa = p_heads * (reward + V[winning_states]) + (1 - p_heads) * (V[losing_states])\n",
    "            \n",
    "            for action, q_value in zip(actions, Q_sa):\n",
    "                Q[s, action] = q_value\n",
    "            \n",
    "            pi[s] = np.argmax(Q_sa) \n",
    "        \n",
    "        print(\"Optimization completed after {} iterations\".format(counter))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,10))\n",
    "\n",
    "ax0 = fig.add_subplot(121)\n",
    "plt.plot(V[1:100])\n",
    "plt.xlabel(\"Current Stash Amount\", size=20)\n",
    "plt.xticks(size=15)\n",
    "plt.ylabel(\"State Value\", size=20)\n",
    "plt.yticks(size=15)\n",
    "ax1 = fig.add_subplot(122)\n",
    "plt.bar(x=np.arange(1,100),height=pi[1:100])\n",
    "plt.xlabel(\"Current Stash Amount\", size=20)\n",
    "plt.xticks(size=15)\n",
    "plt.ylabel(\"Wager Amount\", size=20)\n",
    "plt.yticks(size=15)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,10)) \n",
    "mp_to_float = np.vectorize(float)\n",
    "sns.heatmap(mp_to_float(Q.T))\n",
    "plt.title(\"Action Value Function - Q(s,a)\", size=30)\n",
    "plt.xlabel(\"s\", size=20)\n",
    "plt.ylabel(\"a\", size=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,20))\n",
    "\n",
    "q_max = np.max(Q[s][0:25])\n",
    "max_list = []\n",
    "\n",
    "for s in np.arange(0, 25):\n",
    "    plt.plot(Q[s][0:25], linewidth=2)\n",
    "    q_smax = np.max((mp_to_float(Q[s])))\n",
    "    plt.hlines(q_smax, 0, 25, linestyles='dashed')\n",
    "    max_list.append(q_smax)\n",
    "    plt.vlines(s, 0, q_max, linestyles='dashed')\n",
    "    \n",
    "plt.plot(max_list, color='black', linewidth=5)\n",
    "plt.plot(np.arange(25, 0, -1), max_list, color='black', linewidth=5)\n",
    "\n",
    "plt.legend(np.arange(0, 25))\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,20))\n",
    "\n",
    "q_max = np.max(Q[s][1:51])\n",
    "max_list = []\n",
    "\n",
    "for s in np.arange(0, 51):\n",
    "    plt.plot(range(0,51), Q[s][0:51], linewidth=2)\n",
    "    q_smax = np.max((mp_to_float(Q[s])))\n",
    "    plt.hlines(q_smax, 0, 51, linestyles='dashed')\n",
    "    max_list.append(q_smax)\n",
    "    plt.vlines(s, 0, q_max, linestyles='dashed')\n",
    "    \n",
    "#plt.plot(max_list, color='black', linewidth=5)\n",
    "#plt.plot(np.arange(50, 25, -1), max_list, color='black', linewidth=5)\n",
    "\n",
    "plt.legend(np.arange(0, 51))\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S&B 5.12 - Racetrack\n",
    "\n",
    "## I. Track Generation\n",
    "\n",
    "The first thing is that we need to create a racetrack. The simplest representation for the racetrack is simply an $N \\times N$ matrix in which an entry is 1 if it's outside the track and zero otherwise. We generate tracks as follows:\n",
    "\n",
    "+ We initialize the track to an $N \\times N$ grid where every square in the grid is on the track.\n",
    "+ We take out rectangles from the upper left and lower right of the track. The sizes of the rectangles are randomly sampled, with the length of each dimension $L$ being distributed as $L \\sim RandInt[Floor(N/4), \\ Floor(N/2)]$. Taking out these rectangles creates the \"right\" turn at the end requested by the authors, it also creates left turn for fun.\n",
    "+ We then iterate through each cell on the grid, check if it's outside the grid, and if it is we flip it to being on the track with probability equal to the percentage of its neighbors on the track. This (approximately) guarantees that only cells at the edge of the track get moved into the track.\n",
    "+ This iteration is repeated some number of times so we get some interesting shapes due to the randomness of the sampling. We do smoothing after the final iteration to attempt to eliminate blocks that are technically \"on the track\" but are inaccessible due to being surrounded by blocks \"off the track\".\n",
    "\n",
    "Here is the code -- if you run it you can see we get some nice tracks with semi-smooth curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 25\n",
    "repeats = 4\n",
    "filter_threshold = 0.2\n",
    "filter_size = 3\n",
    "\n",
    "base_track = np.zeros((N, N), dtype=float)\n",
    "\n",
    "upper_left_rect = np.random.randint(int(N/4), int(3*N/4)), np.random.randint(N - int(3*N/4), N - int(N/4))\n",
    "lower_right_rect = np.random.randint(int(N/4), int(3*N/4)), np.random.randint(int(N/4), int(3*N/4))\n",
    "\n",
    "for i in range(N):\n",
    "    for j in range(N):\n",
    "        if i < upper_left_rect[0] and j > upper_left_rect[1]:\n",
    "            base_track[i, j] = 1\n",
    "            \n",
    "        if i > N - lower_right_rect[0] and j < lower_right_rect[1]:\n",
    "            base_track[i, j] = 1\n",
    "\n",
    "for repeat in range(repeats):\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            if base_track[i, j] == 1:\n",
    "                neighs = np.array([base_track[i + n, j + m] for n in np.arange(-1, 2) for m in np.arange(-1, 2)\n",
    "                                   if 0 <= i + n < N and 0 <= j + m < N])\n",
    "\n",
    "                neighs_sum = np.sum(neighs)\n",
    "                is_flipped = np.random.binomial(1, 1 - np.mean(neighs))\n",
    "\n",
    "                if is_flipped:\n",
    "                    base_track[i, j] = 0\n",
    "                    \n",
    "track = (scipy.ndimage.filters.uniform_filter(base_track, filter_size) > filter_threshold)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,10)) \n",
    "\n",
    "sns.heatmap(track.T, ax=ax)\n",
    "plt.xlabel(\"X coordinate\", size=20)\n",
    "plt.ylabel(\"Y coordinate\", size=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the indices of the track array correspond to the x and y coordinates respectively. We only transpose when plotting so that the plot aligns with our intuition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Trajectory Sampling\n",
    "\n",
    "First let's formulate the problem concretely. The state space is the set of possible x and y coordinates and x and y velocities. Let $X_N$ be the set of all coordinates in the grid, $X_{off}$ be the set of all off-track coordinates, $X_S$ the set of start coordinates, and $V$ be the set of all possible velocities. Then $X_N = [0, N] \\times [0, N]$ where these are integer intervals, and the set of possible spatial states is the quotient $X_N - X_{off}$ since all off-track coordinates transition you to a start state. Also we have that $V = [0, v_{x, max}] \\times [0, v_{y, max}]$ where $v_{i, max} = 5$ in our instance of the problem. Finally the actual set of possible states is:\n",
    "\n",
    "$$ \\mathscr{S} = \\left( X_S \\times \\{ \\vec{0} \\} \\right) \\ \\bigcup \\ \\left( X_N - X_{off} - X_S \\right) \\times \\left( V -  \\{ \\vec{0} \\} \\right) $$\n",
    "\n",
    "Let us denote a particular state in terms of the spatial and velocity coordinates, $\\left( \\vec{x}, \\vec{v} \\right) \\in \\mathscr{S}$. Then if $\\vec{v} = (v_x, v_y)$ and $H$ is the Heaviside step function with the convention that $H(0) = 1$:\n",
    "\n",
    "$$ \\mathscr{A}(\\vec{x}, \\vec{v}) = \\left\\{ -H(v_y - 1), 0, 1 -  H(v_x - v_{x,max}) \\right\\} \\times \\left\\{-H(v_y - 1),0, 1 - H(v_y - v_{y,max}) \\right\\},  \\forall \\left( \\vec{x}, \\vec{v} \\right) \\in \\mathscr{S} $$\n",
    "\n",
    "Note that here we rely on the convention that duplicate values in a set are not distinct. Finally, if $\\vec{a} \\in \\mathscr{A}(\\vec{x}, \\vec{v})$ is the action vector then the environment acts as:\n",
    "\n",
    "$$ \n",
    "\\begin{equation}\n",
    "  (\\vec{X}', \\vec{V}') =\\begin{cases}\n",
    "    (\\vec{X} + \\vec{V} + \\vec{A}, \\vec{V} + \\vec{A}), & \\text{with probability } 1 - \\delta\\\\\n",
    "    (\\vec{X} + \\vec{V}, \\vec{V}), & \\text{with probability } \\delta\n",
    "  \\end{cases}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "In our particular case $\\delta=0.1$. Note that we are choosing to have the action $\\vec{A}$ apply immediately on the current state, so that it determines the next state if successful. Note that this equation only applies exactly if the newly calculated state is on the track -- otherwise the new state is the starting state.\n",
    "\n",
    "The task is to determine the optimal policy from each starting state. This means we start each episode always from the starting states and simulate our trajectories in this manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vx_max = 5\n",
    "vy_max = 5\n",
    "v_max = [vx_max, vy_max]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_start_states(track):\n",
    "    N = len(track)\n",
    "    x_start = []\n",
    "    for x in range(N):\n",
    "        if track[x, 0] == 0:\n",
    "            x_start.append(np.array([x, 0]))\n",
    "            \n",
    "    return np.array(x_start)\n",
    "    \n",
    "def get_end_states(track):\n",
    "    N = len(track)\n",
    "    y_end = []\n",
    "    for y in range(N):\n",
    "        if track[N - 1, y] == 0:\n",
    "            y_end.append(np.array([N - 1, y]))\n",
    "            \n",
    "    return np.array(y_end)\n",
    "\n",
    "def valid_actions(v_x, v_y, v_max):\n",
    "    all_actions = [(i, j) for i in range(-1, 2) for j in range(-1, 2)]\n",
    "    actions = [(i, j) for i, j in all_actions \n",
    "               if (0 <= v_x + i <= v_max[0]) and (0 <= v_y + j <= v_max[1])\n",
    "                   and  (v_x + i, v_y + j) != (0, 0)]\n",
    "    return actions\n",
    "    \n",
    "\n",
    "class PolicyClass():\n",
    "    \n",
    "    def __init__(self, track, epsilon, v_max=v_max):\n",
    "        \"\"\"\n",
    "        We represent the policy as a dictionary indexed by\n",
    "        (valid) coordinate tuples. Each value of the dictionary\n",
    "        is a dictionary whose keys correspond to an allowed velociy \n",
    "        state. \n",
    "        \n",
    "        Thus self.policy[(x, y)][(v_x, v_y)] returns the policy [a_x, a_y]\n",
    "        for some state with a caveat.\n",
    "        \"\"\"\n",
    "        self.start_states = get_start_states(track).tolist()\n",
    "        self.epsilon = epsilon\n",
    "        N = len(track)\n",
    "        self.policy = {}\n",
    "        self.v_max = v_max\n",
    "\n",
    "        for i in range(N):\n",
    "            for j in range(N):\n",
    "                if track[i, j] == 0 and [i, j] not in self.start_states:\n",
    "                    self.policy[(i, j)] = {}\n",
    "                    \n",
    "                    for v_x in range(0, v_max[0] + 1):\n",
    "                        for v_y in range(0, v_max[1] + 1):\n",
    "                            if v_x == 0 and v_y == 0:\n",
    "                                continue \n",
    "                                \n",
    "                            if v_x == v_max[0] and v_y != v_max[1]:\n",
    "                                self.policy[(i, j)][(v_x, v_y)] = np.array([0 , 1])\n",
    "                            elif v_x != v_max[0] and v_y == v_max[1]:\n",
    "                                self.policy[(i, j)][(v_x, v_y)] = np.array([1 , 0])\n",
    "                            elif v_x == v_max[0] and v_y == v_max[1]:\n",
    "                                self.policy[(i, j)][(v_x, v_y)] = np.array([0 , 0])\n",
    "                            else:\n",
    "                                self.policy[(i, j)][(v_x, v_y)] = np.array([1 , 1])\n",
    "\n",
    "                elif track[i, j] == 0 and [i, j] in self.start_states:\n",
    "                    self.policy[(i, j)] = {(0,0): np.array([1, 1])}\n",
    "                else:\n",
    "                    continue\n",
    "                    \n",
    "    def update_action(self, new_a_x, new_a_y, x, y, v_x, v_y):\n",
    "        if new_a_x not in [-1, 0, 1] or new_a_y not in [-1, 0, 1]:\n",
    "            raise ValueError(\"a_x and a_y must be one of `[-1, 0, 1]`\")\n",
    "        \n",
    "        if v_x == v_max[0] and new_a_x == 1:\n",
    "            raise ValueError(\"a_x cannot be 1 when v_x is already at its maximum value\")\n",
    "            \n",
    "        if v_y == v_max[1] and new_a_y == 1:\n",
    "            raise ValueError(\"a_y cannot be 1 when v_y is already at its maximum value\")\n",
    "            \n",
    "        if v_x == 0 and new_a_x == -1:\n",
    "            raise ValueError(\"a_x cannot be -1 when v_x is already 0\")\n",
    "            \n",
    "        if v_y == 0 and new_a_y == -1:\n",
    "            raise ValueError(\"a_y cannot be -1 when v_y is already 0\")\n",
    "            \n",
    "        if [x,y] in self.start_states and [v_x, v_y] != [0, 0]:\n",
    "            raise ValueError(\"The velocity at a start location must be zero\")\n",
    "            \n",
    "        self.policy[(x, y)][(v_x, v_y)] = np.array([new_a_x, new_a_y], dtype=int)\n",
    "            \n",
    "    def get_action(self, x, y, v_x, v_y):\n",
    "        return self.policy[(x, y)][(v_x, v_y)]\n",
    "        \n",
    "    def sample(self, x, y, v_x, v_y):\n",
    "        \"\"\"\n",
    "        Note the policy is actually epsilon greedy, but we \n",
    "        keep track of the deterministic part of it\n",
    "        \"\"\"\n",
    "        if [x, y] not in self.start_states:\n",
    "            actions = valid_actions(v_x, v_y, v_max)\n",
    "        else:\n",
    "            # Any starting velocity MUST leave the start line, just makes things simpler\n",
    "            actions = [(i,j) for i in [0, 1] for j in [1]]\n",
    "        \n",
    "        num_actions = len(actions)\n",
    "        \n",
    "        is_greedy = np.random.binomial(1, 1 - self.epsilon + self.epsilon/num_actions)\n",
    "\n",
    "        if is_greedy:\n",
    "            return self.get_action(x, y, v_x, v_y)\n",
    "        else:\n",
    "            action_index = np.random.randint(num_actions)\n",
    "            \n",
    "            a_x = actions[action_index][0]\n",
    "            a_y = actions[action_index][1]\n",
    "            return np.array([a_x, a_y])\n",
    "        \n",
    "class QClass():\n",
    "    \n",
    "    def __init__(self, track, v_max=v_max, init=0.0):\n",
    "        \"\"\"\n",
    "        We represent the action-value function as a dictionary indexed by\n",
    "        (valid) coordinate tuples. Each value of the dictionary\n",
    "        is a dictionary with keys corresponding to a combination\n",
    "        of allowed velociy states. The values of the velocity dictionary \n",
    "        are in turn dictionaries whose keys are valid action tuples\n",
    "        \n",
    "        Thus self.Q[(x, y)][(v_x, v_y)][(a_x, a_y)] returns the value\n",
    "        Q(s=(x,y,v_x,v_y), a=(a_x, a_y)) for some state and action combination.\n",
    "        \n",
    "        We also keep track of counts in a similar tensor so we can calcualte the\n",
    "        MC average incrementally.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.start_states = get_start_states(track).tolist()\n",
    "        N = len(track)\n",
    "        self.Q = {}\n",
    "        self.counts = {}\n",
    "        self.v_max = v_max\n",
    "\n",
    "        for i in range(N):\n",
    "            for j in range(N):\n",
    "                if track[i, j] == 0 and [i, j] not in self.start_states:\n",
    "                    self.Q[(i, j)] = {}\n",
    "                    self.counts[(i, j)] = {}\n",
    "                    \n",
    "                    for v_x in range(0, v_max[0] + 1):\n",
    "                        for v_y in range(0, v_max[1] + 1):\n",
    "                            \n",
    "                            if v_x == 0 and v_y == 0:\n",
    "                                continue\n",
    "                            \n",
    "                            actions = valid_actions(v_x, v_y, v_max)\n",
    "                                \n",
    "                            self.Q[(i, j)][(v_x, v_y)] = {a: init for a in actions}\n",
    "                            self.counts[(i, j)][(v_x, v_y)] = {a: 0 for a in actions}\n",
    "                    \n",
    "                elif track[i, j] == 0 and [i, j] in self.start_states:\n",
    "                    # Any starting velocity MUST leave the start line, just makes things simpler\n",
    "                    start_actions = [(0, 1), (1, 1)]\n",
    "                    self.Q[(i, j)] = {(0,0): {a: init for a in start_actions}}\n",
    "                    self.counts[(i, j)] = {(0,0): {a: 0 for a in start_actions}}\n",
    "                else:\n",
    "                    continue\n",
    "                    \n",
    "    def update(self, G, x, y, v_x, v_y, a_x, a_y):\n",
    "        n = self.get_n(x, y, v_x, v_y, a_x, a_y)\n",
    "        q = self.get_q(x, y, v_x, v_y, a_x, a_y)\n",
    "        \n",
    "        self.Q[(x, y)][(v_x, v_y)][(a_x, a_y)] = q + (1/(n + 1))*(G - q)    \n",
    "        self.counts[(x, y)][(v_x, v_y)][(a_x, a_y)] = n + 1\n",
    "        \n",
    "    def get_q_s(self, x, y, v_x, v_y):\n",
    "        return self.Q[(x, y)][(v_x, v_y)]\n",
    "        \n",
    "    def get_q(self, x, y, v_x, v_y, a_x, a_y):\n",
    "        return self.get_q_s(x, y, v_x, v_y)[(a_x, a_y)]\n",
    "            \n",
    "    def get_n(self, x, y, v_x, v_y, a_x, a_y):\n",
    "        return self.counts[(x, y)][(v_x, v_y)][(a_x, a_y)]\n",
    "        \n",
    "    def get_max_action(self, x, y, v_x, v_y):\n",
    "        q_s = self.get_q_s(x, y, v_x, v_y)\n",
    "        max_a = None\n",
    "        max_q_s = -np.inf\n",
    "        \n",
    "        for a, q in q_s.items():\n",
    "            if q > max_q_s:\n",
    "                max_q_s = q\n",
    "                max_a = a\n",
    "            \n",
    "        return np.array(max_a)    \n",
    "\n",
    "def generate_trajectory(pi, track, delta):\n",
    "    \"\"\"\n",
    "    Samples from pi to generate a simulated trajectory\n",
    "    Handles the logic for determining state transitions\n",
    "    including going off track, enviroment noise and episode \n",
    "    termination. \n",
    "    \n",
    "    The finish state is defined as crossing the finishing line\n",
    "    WITHOUT going above or below it.\n",
    "    \"\"\"\n",
    "    start_index = np.random.randint(len(pi.start_states))\n",
    "    X, V = np.array(copy.deepcopy(pi.start_states[start_index])), np.array([0, 0])\n",
    "    end_states = get_end_states(track)\n",
    "    end_x = end_states[0][0]\n",
    "    end_y_range = (end_states[0][1], end_states[len(end_states) - 1][1])\n",
    "    \n",
    "    \n",
    "    trajectory = []\n",
    "    \n",
    "    while True:\n",
    "        try: \n",
    "            A = pi.sample(X[0], X[1], V[0], V[1])\n",
    "        except:\n",
    "            print(\"SAMPLING ERROR\")\n",
    "            print(trajectory)\n",
    "            print(X, V)\n",
    "            raise\n",
    "        \n",
    "        trajectory.append((copy.deepcopy(X), copy.deepcopy(V), copy.deepcopy(A)))\n",
    "        \n",
    "        is_noise = np.random.binomial(1, delta)\n",
    "        if is_noise:\n",
    "            pass\n",
    "        else:\n",
    "            V += A\n",
    "        \n",
    "        X += V\n",
    "        \n",
    "        try:\n",
    "            is_on_track = not track[X[0], X[1]]\n",
    "            finish_state = False\n",
    "        except IndexError:\n",
    "            is_on_track = False\n",
    "            finish_state = X[0] > end_x and (end_y_range[0] <= X[1] <= end_y_range[1])\n",
    "            \n",
    "        if not is_on_track and not finish_state:\n",
    "            start_index = np.random.randint(len(pi.start_states))\n",
    "            X, V = np.array(copy.deepcopy(pi.start_states[start_index])), np.array([0, 0])\n",
    "        elif not is_on_track and finish_state:\n",
    "            return trajectory\n",
    "        elif is_on_track and not finish_state:\n",
    "            continue\n",
    "        elif is_on_track and finish_state:\n",
    "            raise ValueError(\"Agent should never be on track and in a finish state\")\n",
    "            \n",
    "def update_q_and_pi(trajectory, Q, pi):\n",
    "    \"\"\"                          \n",
    "    Recursively calculates G based on the trajectory\n",
    "    Uses G to update Q, and then Q to update pi.\n",
    "    \n",
    "    Note that our task is undiscounted so we don't\n",
    "    reference gamma. Also the reward is always -1 \n",
    "    for every step \n",
    "    \"\"\"             \n",
    "    trajectory.reverse()\n",
    "    G = 0\n",
    "                              \n",
    "    for X, V, A in trajectory:\n",
    "        G = G - 1\n",
    "        try: \n",
    "            Q.update(G, X[0], X[1], V[0], V[1], A[0], A[1])\n",
    "        except KeyError:\n",
    "            print(\"Q UPDATE ERROR\")\n",
    "            print(trajectory)\n",
    "            print(X, V, A)\n",
    "            raise\n",
    "        new_A = Q.get_max_action(X[0], X[1], V[0], V[1])\n",
    "        pi.update_action(new_A[0], new_A[1], X[0], X[1], V[0], V[1])\n",
    "\n",
    "def on_policy_mc_control(track, N, epsilon, delta, v_max=v_max, init=0.0, update_iter=1000):\n",
    "    \"\"\"Initialization is handled internally by each class\"\"\"\n",
    "    pi = PolicyClass(track, epsilon, v_max=v_max)\n",
    "    Q = QClass(track, v_max=v_max, init=0.0)\n",
    "    \n",
    "    t_i = time.time()\n",
    "    total_time = 0\n",
    "    \n",
    "    for iteration in range(N):\n",
    "        if (iteration + 1) % update_iter == 0:\n",
    "            t_f = time.time()\n",
    "            print(\"Iterations {} through {} complete: {} secs\".format(iteration + 1 - update_iter, \n",
    "                                                                      iteration + 1, \n",
    "                                                                      round((t_f - t_i), 2)))\n",
    "            total_time += (t_f - t_i)\n",
    "            t_i = time.time()\n",
    "            \n",
    "        trajectory = generate_trajectory(pi, track, delta)\n",
    "        update_q_and_pi(trajectory, Q, pi)\n",
    "    \n",
    "    print(\"Total Running Time: {} mins\".format(round(total_time/60, 2)))\n",
    "        \n",
    "    return Q, pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q, pi = on_policy_mc_control(track, 10**6, 0.1, 0.1, update_iter=5 * 10**4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME -- Need better collision detection. Currently just checking\n",
    "# if car lands on (not crosses) an off-track area. Also may be worth\n",
    "# trying training with less simulations and having a termination \n",
    "# condition on the simulator just in case.\n",
    "trajectory = generate_trajectory(pi, track, 0)\n",
    "\n",
    "spatial_trajectory = [state[0].tolist() for state in trajectory]\n",
    "velocities = [state[1].tolist() for state in trajectory]\n",
    "accel = [state[2].tolist() for state in trajectory]\n",
    "\n",
    "traj_steps = len(spatial_trajectory)\n",
    "track_and_path = np.array(copy.deepcopy(track), dtype=float)\n",
    "counter = 0\n",
    "\n",
    "for i in range(len(track)):\n",
    "    for j in range(len(track)):\n",
    "        if [i, j] in spatial_trajectory:\n",
    "            index = spatial_trajectory.index([i, j])\n",
    "            track_and_path[i, j] = 0.25 + (0.75-0.25)/traj_steps * counter\n",
    "            counter += 1\n",
    "            \n",
    "fig, ax = plt.subplots(figsize=(15,10)) \n",
    "sns.heatmap(track_and_path.T, ax=ax)\n",
    "\n",
    "for i in range(len(track)):\n",
    "    for j in range(len(track)):\n",
    "        if [i, j] in spatial_trajectory:\n",
    "            index = spatial_trajectory.index([i, j])\n",
    "            try: \n",
    "                plt.arrow(i , j, \n",
    "                          velocities[index + 1][0], \n",
    "                          velocities[index + 1][1], \n",
    "                          width=0.1, head_width=0.5,\n",
    "                          color=\"blue\",alpha=0.9)\n",
    "                plt.arrow(i , j, \n",
    "                          accel[index][0], \n",
    "                          accel[index][1], \n",
    "                          width=0.02, head_width=0.2,\n",
    "                          color=\"mediumseagreen\",alpha=0.9)\n",
    "            except IndexError:\n",
    "                continue\n",
    "                \n",
    "for i in range(len(track)):\n",
    "    for j in range(len(track)):\n",
    "        if [i, j] in get_start_states(track).tolist():\n",
    "            try: \n",
    "                plt.arrow(i , j, 1, 0, \n",
    "                          width=0.5, head_width=0,\n",
    "                          color=\"green\",alpha=0.9)\n",
    "            except IndexError:\n",
    "                continue\n",
    "                \n",
    "for i in range(len(track)):\n",
    "    for j in range(len(track)):\n",
    "        if [i, j] in get_end_states(track).tolist():\n",
    "            try: \n",
    "                plt.arrow(i + 1, j, 0, 1, \n",
    "                          width=0.5, head_width=0,\n",
    "                          color=\"red\",alpha=0.9)\n",
    "            except IndexError:\n",
    "                continue\n",
    "\n",
    "\n",
    "plt.xlabel(\"X coordinate\", size=20)\n",
    "plt.ylabel(\"Y coordinate\", size=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
