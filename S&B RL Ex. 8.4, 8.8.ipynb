{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import copy\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S&B 8.4\n",
    "\n",
    "In running Dyna-Q, Dyna-Q+, and Dyna-Q+ Modified on the maze example we've noticed a couple of issues: \n",
    "+ When the episode is terminated at an arbitrary timestep the policy implied by the Q function for the Dyna-Q+ and Dyna-Q+ Modified cases often is not the optimal policy. It is a noisy, exploratory policy. The question is how can we retrieve the optimal policy at an arbitrary termination point consistently? **Maybe we can just save the most recent policy at the episode with the highest reward!**\n",
    "+ There is a lot of variance in the number of timesteps required to complete the first episode. This makes comparison between methodologies difficult. **We're trying taking the average of many runs to address this, but the variance may be too large still.**\n",
    "+ In the case of Dyna-Q+ the addition of $\\tau$ is cumulative across simulated timesteps which seems like it could quickly drown out any actual simulated signal. **We need to add and remove $\\tau$ to Q somehow. Maybe we subtract the previous $\\tau$? I THINK THIS IS BREAKING Dyna-Q+**\n",
    "\n",
    "I think I have gotten enough out of this to pause for now. If I feel the need to come back to this exercise later I will. The main point is that Dyna-Q+ can handle non-static situations. I don't think we necessarily got the exercise clean enough to understand the difference between Dyna-Q+ and Dyna-Q+ Modified but we can come back to that if it makes sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridworld = np.ones((9, 6))\n",
    "gridworld[1:, 2] = 0\n",
    "start = ([3], [0])\n",
    "end = ([8], [5])\n",
    "gridworld[start] = 0.25\n",
    "gridworld[end] = 0.75\n",
    "ax = sns.heatmap(gridworld.T)\n",
    "ax.invert_yaxis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridworld_after = copy.deepcopy(gridworld)\n",
    "gridworld_after[8, 2] = 1\n",
    "ax = sns.heatmap(gridworld_after.T)\n",
    "ax.invert_yaxis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This would be better as a class, but I'm just saving time\n",
    "def dyna_q(gridworld, dyna_mode = \"dyna\", epsilon = 0.1, gamma = 0.95, \n",
    "           alpha = 0.1, n = 50, timesteps = 6000, kappa = 0.1, verbose=True):\n",
    "    \n",
    "    map_instance = copy.deepcopy(gridworld)\n",
    "    \n",
    "    assert dyna_mode in [\"dyna\", \"dyna_plus\", \"dyna_plus_mod\"], \"dyna_mode must take allowed value\"\n",
    "    \n",
    "    rewards = []\n",
    "    action_map = {0: \"left\", 1: \"right\", 2: \"up\", 3: \"down\"}\n",
    "    action_array = np.array([[-1, 0], [1, 0], [0, 1], [0, -1]])\n",
    "    # x, y, action\n",
    "    Q = np.zeros((9, 6, 4), dtype=float)\n",
    "    tau = np.zeros((9, 6, 4), dtype=int)\n",
    "    model_s_prime = np.full((9, 6, 4, 2), -1, dtype=int) # Note the problem doesn't allow negative coordinates\n",
    "    model_reward = np.full((9, 6, 4), -1, dtype=int) # It also does not allow negative rewards\n",
    "\n",
    "    # Training code\n",
    "    episode_start = True\n",
    "\n",
    "    t_i = time.time()\n",
    "    for step in range(timesteps): \n",
    "        if episode_start:\n",
    "            s = np.array([3, 0])\n",
    "            episode_start = False\n",
    "        else:\n",
    "            s = s_prime\n",
    "            \n",
    "        if step == int(timesteps/2) - 1:\n",
    "            # Open the right side of the map\n",
    "            map_instance[8, 2] = 1\n",
    "            \n",
    "        if (step + 1) % int(timesteps/10) == 0 and verbose: \n",
    "            t_f = time.time()\n",
    "            print(\"Steps #{}-{} Completed: {}s\".format(step + 1 - int(timesteps/10), \n",
    "                                                       step + 1, round(t_f - t_i, 2)))\n",
    "            t_i = t_f\n",
    "\n",
    "        is_not_greedy = np.random.binomial(1, epsilon)\n",
    "\n",
    "        if is_not_greedy: \n",
    "            action = np.random.randint(1, 4, 1)[0]\n",
    "        else:\n",
    "            action_vals = Q[s[0], s[1]]\n",
    "            if dyna_mode == \"dyna_plus_mod\":\n",
    "                action_vals = (Q + kappa * np.sqrt(tau))[s[0], s[1]]\n",
    "            else:\n",
    "                action_vals = Q[s[0], s[1]]\n",
    "            action = np.random.choice(np.where(action_vals == action_vals.max())[0])\n",
    "\n",
    "        s_prime = s + action_array[action]\n",
    "\n",
    "        if not (0 <= s_prime[0] <= 8) or not (0 <= s_prime[1] <= 5):\n",
    "            # No movement if we leave the grid\n",
    "            s_prime = s\n",
    "            reward = 0\n",
    "        elif map_instance[s_prime[0], s_prime[1]] == 0:\n",
    "            # No movement if we hit a wall\n",
    "            s_prime = s\n",
    "            reward = 0\n",
    "        elif np.all(s == np.array([8, 5])):\n",
    "            # Episode terminates\n",
    "            episode_start = True\n",
    "            reward = 1\n",
    "        else:\n",
    "            # Next timestep in episode\n",
    "            reward = 0\n",
    "\n",
    "        Q_max = np.max(Q[s_prime[0], s_prime[1]])\n",
    "        Q[s[0], s[1], action] += alpha * (reward + gamma * Q_max - Q[s[0], s[1], action])\n",
    "\n",
    "        # Keep track of rewards\n",
    "        rewards.append(reward)\n",
    "\n",
    "        # Increment tau everywhere, reset to zero at latest state-action and end state\n",
    "        tau += 1\n",
    "        tau[8, 5, :] = 0\n",
    "        tau[s[0], s[1], action] = 0\n",
    "\n",
    "        # Update model -- seems to improve performance to model other actions\n",
    "        # Suggestion from here: https://towardsdatascience.com/reinforcement-learning-model-based-planning-methods-extension-572dfee4cceb\n",
    "        for a in range(3):\n",
    "            if a == action:\n",
    "                model_s_prime[s[0], s[1], a] = s_prime\n",
    "                model_reward[s[0], s[1], a] = reward\n",
    "            else:\n",
    "                model_s_prime[s[0], s[1], a] = s\n",
    "                model_reward[s[0], s[1], a] = 0\n",
    "\n",
    "        # Get previously observed state-actions, pick them randomly\n",
    "        observed_state_actions = np.argwhere(model_s_prime >= 0)\n",
    "        num_state_actions = len(observed_state_actions)\n",
    "        rad_indices = np.random.randint(0, num_state_actions, n)\n",
    "\n",
    "        # Simulate randomly selected previously seen experiences\n",
    "        for index in rad_indices:\n",
    "            state_action = observed_state_actions[index]\n",
    "            sim_s_prime = model_s_prime[state_action[0], state_action[1], state_action[2]]\n",
    "            sim_reward = model_reward[state_action[0], state_action[1], state_action[2]]\n",
    "            \n",
    "            if dyna_mode == \"dyna_plus\":\n",
    "                sim_reward += kappa * np.sqrt(tau[state_action[0], state_action[1], state_action[2]])\n",
    "            \n",
    "            assert np.all(sim_s_prime >= np.array([0, 0])), \"Negative coordinate simulated\"\n",
    "            assert sim_reward >= 0, \"Negative reward simulated\"\n",
    "                \n",
    "            Q_max_sim = np.max(Q[sim_s_prime[0], sim_s_prime[1]])\n",
    "            Q[state_action[0], state_action[1], state_action[2]] += \\\n",
    "                alpha * (sim_reward + gamma * Q_max_sim - Q[state_action[0], state_action[1], state_action[2]])\n",
    "                \n",
    "    return Q, np.array(rewards), model_s_prime, model_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_policy(Q, ax=None):\n",
    "    if ax==None:\n",
    "        ax = sns.heatmap(gridworld_after.T)\n",
    "    else:\n",
    "        sns.heatmap(gridworld_after.T, ax=ax)\n",
    "    pi = np.argmax(Q, axis=2)\n",
    "    grid_shape = gridworld_after.shape\n",
    "    action_array = np.array([[-1, 0], [1, 0], [0, 1], [0, -1]])\n",
    "\n",
    "    for i in range(grid_shape[0]):\n",
    "        for j in range(grid_shape[1]):\n",
    "            if gridworld_after[i, j] == 0:\n",
    "                continue\n",
    "            plt.arrow(i + 0.5 , j + 0.5, \n",
    "                      0.25*action_array[pi[i, j]][0], \n",
    "                      0.25*action_array[pi[i, j]][1], \n",
    "                      width=0.05, head_width=0.2,\n",
    "                      color=\"blue\", alpha=0.9)\n",
    "\n",
    "    ax.invert_yaxis()\n",
    "    \n",
    "def simulate_policy(Q, ax=None):\n",
    "    if ax==None:\n",
    "        ax = sns.heatmap(gridworld_after.T)\n",
    "    else:\n",
    "        sns.heatmap(gridworld_after.T, ax=ax)\n",
    "    pi = np.argmax(Q, axis=2)\n",
    "    grid_shape = gridworld_after.shape\n",
    "    action_array = np.array([[-1, 0], [1, 0], [0, 1], [0, -1]])\n",
    "    \n",
    "    s = np.array([3, 0])\n",
    "    end = np.array([8, 5])\n",
    "    \n",
    "    counter = 0\n",
    "    while np.any(s != end):\n",
    "        action = action_array[pi[s[0], s[1]]]\n",
    "        \n",
    "        plt.arrow(s[0] + 0.5 , s[1] + 0.5, \n",
    "                  0.25*action[0], \n",
    "                  0.25*action[1], \n",
    "                  width=0.05, head_width=0.2,\n",
    "                  color=\"blue\", alpha=0.9)\n",
    "        \n",
    "        s += action\n",
    "        try:\n",
    "            if gridworld_after[s[0], s[1]] == 0:\n",
    "                break\n",
    "            if np.any(s < 0):\n",
    "                break\n",
    "        except IndexError:\n",
    "            break\n",
    "            \n",
    "        counter += 1\n",
    "        if counter > 20:\n",
    "            break\n",
    "\n",
    "    ax.invert_yaxis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_dyna, R_dyna, model_s_prime_dyna, model_reward_dyna = dyna_q(gridworld, \n",
    "                                                               dyna_mode = \"dyna\",\n",
    "                                                               kappa=0,\n",
    "                                                               n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_plus, R_plus, model_s_prime_plus, model_reward_plus = dyna_q(gridworld, \n",
    "                                                               dyna_mode = \"dyna_plus\",\n",
    "                                                               kappa=10**(-2),\n",
    "                                                               n=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_plus_mod, R_plus_mod, model_s_prime_plus_mod, model_reward_plus_mod = dyna_q(gridworld, \n",
    "                                                                               dyna_mode = \"dyna_plus_mod\",\n",
    "                                                                               kappa=10**(-1),\n",
    "                                                                               n=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 20))\n",
    "\n",
    "ax = fig.add_subplot(321)\n",
    "plot_policy(Q_dyna, ax=ax)\n",
    "plt.ylabel('Dyna-Q', size=30)\n",
    "plt.title('Policy Map', size=30)\n",
    "ax = fig.add_subplot(322)\n",
    "simulate_policy(Q_dyna, ax=ax)\n",
    "plt.title('Policy Trajectory', size=30)\n",
    "\n",
    "ax = fig.add_subplot(323)\n",
    "plot_policy(Q_plus, ax=ax)\n",
    "plt.ylabel('Dyna-Q+', size=30)\n",
    "ax = fig.add_subplot(324)\n",
    "simulate_policy(Q_plus, ax=ax)\n",
    "\n",
    "ax = fig.add_subplot(325)\n",
    "plot_policy(Q_plus_mod, ax=ax)\n",
    "plt.ylabel('Dyna-Q+ Modified', size=30)\n",
    "ax = fig.add_subplot(326)\n",
    "simulate_policy(Q_plus_mod, ax=ax)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,8))\n",
    "plt.plot(np.cumsum(R_dyna), label=\"Dyna-Q\")\n",
    "plt.plot(np.cumsum(R_plus), label=\"Dyna-Q+\")\n",
    "plt.plot(np.cumsum(R_plus_mod), label=\"Dyna-Q+ Modified\")\n",
    "plt.legend(prop={'size': 20})\n",
    "plt.ylabel(\"Cumulative reward\", size=20)\n",
    "plt.yticks(size=20)\n",
    "plt.xlabel(\"Time steps\", size=20)\n",
    "plt.xticks(size=20)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there's too much variance in the length of the first episode -- need to take an average\n",
    "num_sim = 20\n",
    "timesteps = 6000\n",
    "R_dyna = np.empty((num_sim, timesteps))\n",
    "R_plus = np.empty((3 * num_sim, timesteps))\n",
    "R_plus_mod = np.empty((3 * num_sim, timesteps))\n",
    "R_map = {\"dyna\": R_dyna, \"dyna_plus\": R_plus, \"dyna_plus_mod\": R_plus_mod}\n",
    "\n",
    "for mode in [\"dyna\", \"dyna_plus\", \"dyna_plus_mod\"]:\n",
    "    for k_index, k in enumerate([10**(-1), 10**(-2), 10**(-3)]):\n",
    "        for sim in range(num_sim):\n",
    "            if mode == \"dyna\" and k_index != 0:\n",
    "                continue\n",
    "            \n",
    "            t_i = time.time()\n",
    "            Q, R, model_s_prime, model_reward = dyna_q(gridworld, \n",
    "                                                       dyna_mode=mode, \n",
    "                                                       verbose=False, \n",
    "                                                       timesteps=6000,\n",
    "                                                       n=50,\n",
    "                                                       kappa=k)\n",
    "            \n",
    "            if mode != \"dyna\":\n",
    "                R_map[mode][num_sim * k_index + sim] = R\n",
    "            else:\n",
    "                R_map[mode][sim] = R\n",
    "                \n",
    "            t_f = time.time()\n",
    "            print(\"Mode: {}, Simulation: {}, k-index: {}, Array Index {}, Completed. Time: {}s\".format(mode,\n",
    "                                                                                       sim,\n",
    "                                                                                       k_index,\n",
    "                                                                                       num_sim * k_index + sim,\n",
    "                                                                                       round(t_f - t_i, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_dyna_avg = np.mean(np.cumsum(R_dyna, axis=1), axis=0)\n",
    "R_plus_avg_0 = np.mean(np.cumsum(R_plus[0:num_sim], axis=1), axis=0)\n",
    "R_plus_avg_1 = np.mean(np.cumsum(R_plus[num_sim:2*num_sim], axis=1), axis=0)\n",
    "R_plus_avg_2 = np.mean(np.cumsum(R_plus[2*num_sim:3*num_sim], axis=1), axis=0)\n",
    "R_plus_mod_avg_0 = np.mean(np.cumsum(R_plus_mod[0:num_sim], axis=1), axis=0)\n",
    "R_plus_mod_avg_1 = np.mean(np.cumsum(R_plus_mod[num_sim:2*num_sim], axis=1), axis=0)\n",
    "R_plus_mod_avg_2 = np.mean(np.cumsum(R_plus_mod[2*num_sim:3*num_sim], axis=1), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,8))\n",
    "plt.plot(R_dyna_avg, label=\"Dyna-Q\", color=\"grey\")\n",
    "plt.plot(R_plus_avg_0, label=\"Dyna-Q+, $\\kappa=10^{-1}$\", color=\"blue\", linestyle=\"dashed\")\n",
    "plt.plot(R_plus_avg_1, label=\"Dyna-Q+, $\\kappa=10^{-2}$\", color=\"blue\", linestyle=\"dotted\")\n",
    "plt.plot(R_plus_avg_2, label=\"Dyna-Q+, $\\kappa=10^{-3}$\", color=\"blue\", linestyle=\"dashdot\")\n",
    "plt.plot(R_plus_mod_avg_0, label=\"Dyna-Q+ Modified, $\\kappa=10^{-1}$\", color=\"orange\", linestyle=\"dashed\")\n",
    "plt.plot(R_plus_mod_avg_1, label=\"Dyna-Q+ Modified, $\\kappa=10^{-2}$\", color=\"orange\", linestyle=\"dotted\")\n",
    "plt.plot(R_plus_mod_avg_2, label=\"Dyna-Q+ Modified, $\\kappa=10^{-3}$\", color=\"orange\", linestyle=\"dashdot\")\n",
    "plt.legend(prop={'size': 20})\n",
    "plt.ylabel(\"Cumulative reward\", size=20)\n",
    "plt.yticks(size=20)\n",
    "plt.xlabel(\"Time steps\", size=20)\n",
    "plt.xticks(size=20)\n",
    "plt.vlines(timesteps/2, 0, \n",
    "           max(np.max(R_dyna_avg), np.max(R_plus_avg), np.max(R_plus_mod_avg)),\n",
    "           linestyle='dashed')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S&B 8.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrajectorySamplingDemo():\n",
    "    # Note: if we can draw the state samples ahead of time as an array\n",
    "    # and then vectorize the updates that would probably make this a lot faster.\n",
    "    \n",
    "    def __init__(self, sample_traj=False, epsilon = 0.1, num_states = 1000, \n",
    "                 term_prob=0.1, num_actions=2, b=1, verbose=True):\n",
    "    \n",
    "        ## PARAMETERS\n",
    "        self.sample_traj = sample_traj\n",
    "        self.epsilon = epsilon\n",
    "        self.num_states = num_states \n",
    "        self.term_prob = term_prob\n",
    "        self.num_actions = num_actions\n",
    "        self.b = b \n",
    "        self.term_prob = term_prob\n",
    "        self.verbose = verbose\n",
    "    \n",
    "        ## TENSORS DEFINING RL PROBLEM\n",
    "        # Randomly select the start state\n",
    "        self.s_0 = np.random.randint(0, high=num_states, size=1)[0]\n",
    "        self.s_final = self.num_states\n",
    "        # Axes: S, A, b new States. Value: S'\n",
    "        self.state_action_transitions = self.init_state_actions(num_states, num_actions, b)\n",
    "        # Axes: S, A. Value: R for (S, A) -> Terminal state transition\n",
    "        self.terminal_rewards = np.random.normal(size=(num_states, num_actions))\n",
    "        # Axes: S, A, b new States. Value: R for (S,A) -> S' transition\n",
    "        self.state_action_rewards = np.random.normal(size=(num_states, num_actions, b))\n",
    "        \n",
    "        ## Q TENSOR\n",
    "        # S, A\n",
    "        self.Q = np.zeros((num_states, num_actions), dtype=float)\n",
    "        \n",
    "        ## STATE VARIABLES FOR EPISODES\n",
    "        self.episode_start = True\n",
    "        \n",
    "        ## SET STATE AND UPDATE FUNCTIONS\n",
    "        if sample_traj:\n",
    "            self.next_action = self.trajectory_action\n",
    "            self.next_state = self.trajectory_state\n",
    "        else: \n",
    "            self.next_action = self.random_action\n",
    "            self.next_state = self.random_state\n",
    "            \n",
    "        ## RETURN ARRAY\n",
    "        self.start_state_values = []\n",
    "        \n",
    "    def init_state_actions(self, num_states, num_actions, b):\n",
    "        state_action_transitions = np.empty((num_states, num_actions, b), dtype=int)\n",
    "        \n",
    "        for s in range(num_states):\n",
    "            for a in range(num_actions):\n",
    "                state_action_transitions[s, a] = np.random.choice(range(num_states), \n",
    "                                                                  size=b, \n",
    "                                                                  replace=False)\n",
    "            \n",
    "        return state_action_transitions\n",
    "        \n",
    "    def Q_updates(self, updates, Q_update_freq, v_est_update, v_est_episodes, v_est_step_bound=10**6):\n",
    "        s = self.s_0\n",
    "        value_estimates = []\n",
    "        value_devs = []\n",
    "        t_updates = []\n",
    "        \n",
    "        for update in range(updates):\n",
    "            t_i_update = time.time()\n",
    "            \n",
    "            if (update + 1) % Q_update_freq == 0 and self.verbose:\n",
    "                delta_t_updates = round(sum(t_updates)/len(t_updates), 2)\n",
    "                t_updates = []\n",
    "                print(\"Updates #{}-{} Completed, Avg. Time: {}s\".format(update + 1 - Q_update_freq, \n",
    "                                                                        update + 1,\n",
    "                                                                        delta_t_updates))\n",
    "            \n",
    "            if (update + 1) % v_est_update == 0:\n",
    "                t_i_estimate = time.time()\n",
    "                v_est, v_std = self.calc_start_value(v_est_episodes, step_bound=v_est_step_bound)\n",
    "                value_estimates.append(v_est)\n",
    "                value_devs.append(v_std)\n",
    "                t_f_estimate = time.time()\n",
    "                \n",
    "                if self.verbose:\n",
    "                    delta_t_estimate = round(t_f_estimate - t_i_estimate, 2)\n",
    "                    print(\"V(S_0) Estimate At Update #{}, Time: {}s\".format(update + 1,\n",
    "                                                                            delta_t_estimate))\n",
    "            \n",
    "            if s == self.s_final:\n",
    "                s = self.s_0\n",
    "                continue\n",
    "            \n",
    "            a = self.next_action(s)\n",
    "            \n",
    "            s_primes = self.state_action_transitions[s, a] # Axes: b next states\n",
    "            rewards = self.state_action_rewards[s, a] # Axes: b next states\n",
    "            t_reward = self.terminal_rewards[s, a] # Scalar\n",
    "            \n",
    "            self.Q[s, a] = (0.9) * (1/self.b) * np.sum(rewards + np.max(self.Q[s_primes, :])) + 0.1 * t_reward\n",
    "            \n",
    "            s = self.next_state(s, a)\n",
    "            \n",
    "            t_f_update = time.time()\n",
    "            t_updates.append(t_f_update-t_i_update)\n",
    "            \n",
    "        return np.array(value_estimates), np.array(value_devs)\n",
    "            \n",
    "    def simulate_episode(self):\n",
    "        s = self.s_0\n",
    "        cum_reward = 0\n",
    "        steps = 0\n",
    "        \n",
    "        while s != self.s_final:\n",
    "            # Next action and next state\n",
    "            a = self.trajectory_action(s)\n",
    "            s_prime = self.trajectory_state(s, a)\n",
    "            \n",
    "            # Transition reward\n",
    "            if s_prime != self.s_final:\n",
    "                s_prime_index = np.where(self.state_action_transitions[s, a] == s_prime)[0][0]\n",
    "                reward = self.state_action_rewards[s, a, s_prime_index]\n",
    "            else:\n",
    "                reward = self.terminal_rewards[s, a]\n",
    "            \n",
    "            # Set next state to current state and add reward\n",
    "            cum_reward += reward\n",
    "            steps += 1\n",
    "            s = s_prime\n",
    "            \n",
    "        return cum_reward, steps\n",
    "            \n",
    "    def calc_start_value(self, num_episodes, step_bound=10**6):\n",
    "        # Monte Carlo is more efficient for calculating the value of \n",
    "        # a single state\n",
    "        assert num_episodes > 0, \"The number of episodes must be positive\"\n",
    "        \n",
    "        cum_steps = 0\n",
    "        episodes = 0\n",
    "        v_list = []\n",
    "        \n",
    "        while episodes < num_episodes and cum_steps < step_bound:\n",
    "            v_est, steps = self.simulate_episode()\n",
    "            v_list.append(v_est)\n",
    "            cum_steps += steps\n",
    "            episodes += 1\n",
    "            \n",
    "        v = np.array(v_list)\n",
    "        v_avg = np.mean(v)\n",
    "        v_std = np.sqrt(np.sum(np.power(v - v_avg, 2))/len(v_list))\n",
    "            \n",
    "        return v_avg, v_std\n",
    "        \n",
    "    def trajectory_state(self, s, a):\n",
    "        is_terminal = np.random.binomial(1, self.term_prob) \n",
    "        \n",
    "        if is_terminal:\n",
    "            s_prime = self.s_final\n",
    "        else:\n",
    "            s_prime = np.random.choice(self.state_action_transitions[s, a])\n",
    "            \n",
    "        return s_prime\n",
    "            \n",
    "    def trajectory_action(self, s, force_greedy=False):\n",
    "        is_not_greedy = np.random.binomial(1, self.epsilon)\n",
    "\n",
    "        if is_not_greedy and not force_greedy: \n",
    "            a = np.random.randint(0, 2, 1)[0]\n",
    "        else:\n",
    "            action_vals = self.Q[s]\n",
    "            a = np.random.choice(np.where(action_vals == action_vals.max())[0])\n",
    "            \n",
    "        return a\n",
    "        \n",
    "    def random_state(self, s, a):\n",
    "        return np.random.randint(0, high=self.num_states, size=1)[0]\n",
    "        \n",
    "    def random_action(self, s): \n",
    "        return np.random.randint(0, 2, 1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiple_runs(sample_traj, num_sim, updates, Q_update_freq, \n",
    "                  num_updates, v_est_update, sim_update=10, **params):\n",
    "\n",
    "    mean_curves = np.empty((num_sim, num_updates))\n",
    "    error_bars = np.empty((num_sim, num_updates))\n",
    "\n",
    "    t_i = time.time()\n",
    "    for sim in range(num_sim):\n",
    "        demo = TrajectorySamplingDemo(sample_traj=sample_traj, verbose=False, **params)\n",
    "        val_ests, val_devs = demo.Q_updates(updates, \n",
    "                                            Q_update_freq, \n",
    "                                            v_est_update, \n",
    "                                            np.inf, \n",
    "                                            v_est_step_bound=10**4)\n",
    "\n",
    "        mean_curves[sim] = val_ests\n",
    "        error_bars[sim] = val_devs\n",
    "\n",
    "        if (sim + 1) % sim_update == 0:\n",
    "            t_f = time.time()\n",
    "            print(\"Simulations #{}-{} Completed, Time: {}s\".format(sim+1-sim_update,\n",
    "                                                                   sim+1,\n",
    "                                                                   round(t_f-t_i, 2)))\n",
    "            t_i = time.time()\n",
    "            \n",
    "    return mean_curves, error_bars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replicate $\\|\\mathscr{S}\\|=1,000$ Case, with $b \\in \\{1, 3, 10\\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sim = 200\n",
    "updates = 20000\n",
    "Q_update_freq = 1000\n",
    "num_updates = int(updates/Q_update_freq)\n",
    "v_est_update = 1000 # Doesn't make a difference when verbose is set to false\n",
    "\n",
    "# Seems like scaling is independent of b, but 10x number of states increases run time by 3x\n",
    "# I find the latter surprising since there isn't any looping that depends on the number of states\n",
    "base_params = {\"epsilon\": 0.1, \n",
    "               \"num_states\": 1000, \n",
    "               \"term_prob\": 0.1,\n",
    "               \"num_actions\": 2}\n",
    "\n",
    "params_b1 = {**base_params, \"b\": 1}\n",
    "params_b3 = {**base_params, \"b\": 3}\n",
    "params_b5 = {**base_params, \"b\": 10}\n",
    "\n",
    "params_list_1000 = [params_b1, params_b3, params_b5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample_traj in [False, True]:\n",
    "    for params in params_list_1000:\n",
    "        \n",
    "        traj_string = \"trajectory\" if sample_traj else \"random\"\n",
    "        b_string = str(params[\"b\"])\n",
    "\n",
    "        print(\"### SAMPLING TYPE: {}. B={} ###\".format(traj_string, b_string))\n",
    "        mean_curves, error_bars = multiple_runs(sample_traj, \n",
    "                                                num_sim, \n",
    "                                                updates, \n",
    "                                                Q_update_freq, \n",
    "                                                num_updates, \n",
    "                                                v_est_update,\n",
    "                                                sim_update=20,\n",
    "                                                **params)\n",
    "\n",
    "        means = np.mean(mean_curves, axis=0)\n",
    "        stds = np.sqrt(np.sum(np.power(mean_curves  - means.reshape(1,20), 2), axis=0)/num_sim)\n",
    "\n",
    "        means.dump(\"ex_88_s1000_outputs/\" + traj_string + \"_b\" + b_string + \"_\" + \"avg\" + \".pkl\")\n",
    "        stds.dump(\"ex_88_s1000_outputs/\" + traj_string + \"_b\" + b_string + \"_\" + \"std\" + \".pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 10))\n",
    "color_map = {\"1\": \"blue\", \"3\": \"green\", \"10\": \"red\"}\n",
    "linestyle_map = {\"trajectory\": \"dashed\", \"random\": \"solid\"}\n",
    "\n",
    "for params in params_list_1000:\n",
    "    for sample_traj in [False, True]:\n",
    "        \n",
    "        traj_string = \"trajectory\" if sample_traj else \"random\"\n",
    "        b_string = str(params[\"b\"])\n",
    "        \n",
    "        means = np.load(\"ex_88_s1000_outputs/\" + traj_string + \"_b\" + b_string + \"_\" + \"avg\" + \".pkl\", \n",
    "                        allow_pickle=True)\n",
    "        stds = np.load(\"ex_88_s1000_outputs/\" + traj_string + \"_b\" + b_string + \"_\" + \"std\" + \".pkl\", \n",
    "                       allow_pickle=True)\n",
    "        \n",
    "        color = color_map[b_string]\n",
    "        linestyle = linestyle_map[traj_string]\n",
    "        plt.errorbar(np.arange(1000, 21000, 1000), means, color=color, \n",
    "                     linestyle=linestyle, label=\"sampling: {}, b={}\".format(traj_string, b_string))\n",
    "        plt.fill_between(np.arange(1000, 21000, 1000), means - stds, means + stds,\n",
    "                 color=color, alpha=0.1)\n",
    "        \n",
    "plt.legend(prop={'size': 15})\n",
    "plt.xlabel(\"Computation Steps\", size=30)\n",
    "plt.xticks(size=20)\n",
    "plt.ylabel(\"$v(s_0)$\", size=30)\n",
    "plt.yticks(size=20)\n",
    "plt.title(\"Initial State Value vs. Number of $Q$-Function Updates\", size=30)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate $\\|\\mathscr{S}\\|=10,000$ Case, with $b=3$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sim = 100\n",
    "updates = 200000\n",
    "Q_update_freq = 10000\n",
    "num_updates = int(updates/Q_update_freq)\n",
    "v_est_update = 10000 # Doesn't make a difference when verbose is set to false\n",
    "\n",
    "# Seems like scaling is independent of b, but 10x number of states increases run time by 3x\n",
    "# I find the latter surprising since there isn't any looping that depends on the number of states\n",
    "base_params = {\"epsilon\": 0.1, \n",
    "               \"num_states\": 10000, \n",
    "               \"term_prob\": 0.1,\n",
    "               \"num_actions\": 2}\n",
    "\n",
    "params_b1 = {**base_params, \"b\": 1}\n",
    "params_b3 = {**base_params, \"b\": 3}\n",
    "\n",
    "params_list_1000 = [params_b1, params_b3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample_traj in [False, True]:\n",
    "    for params in params_list_1000:\n",
    "        \n",
    "        traj_string = \"trajectory\" if sample_traj else \"random\"\n",
    "        b_string = str(params[\"b\"])\n",
    "\n",
    "        print(\"### SAMPLING TYPE: {}. B={} ###\".format(traj_string, b_string))\n",
    "        mean_curves, error_bars = multiple_runs(sample_traj, \n",
    "                                                num_sim, \n",
    "                                                updates, \n",
    "                                                Q_update_freq, \n",
    "                                                num_updates, \n",
    "                                                v_est_update,\n",
    "                                                sim_update=10,\n",
    "                                                **params)\n",
    "\n",
    "        means = np.mean(mean_curves, axis=0)\n",
    "        stds = np.sqrt(np.sum(np.power(mean_curves  - means.reshape(1,20), 2), axis=0)/num_sim)\n",
    "\n",
    "        means.dump(\"ex_88_s10000_outputs/\" + traj_string + \"_b\" + b_string + \"_\" + \"avg\" + \".pkl\")\n",
    "        stds.dump(\"ex_88_s10000_outputs/\" + traj_string + \"_b\" + b_string + \"_\" + \"std\" + \".pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 10))\n",
    "color_map = {\"1\": \"blue\", \"3\": \"green\"}\n",
    "linestyle_map = {\"trajectory\": \"dashed\", \"random\": \"solid\"}\n",
    "\n",
    "for params in params_list_1000:\n",
    "    for sample_traj in [False, True]:\n",
    "        \n",
    "        traj_string = \"trajectory\" if sample_traj else \"random\"\n",
    "        b_string = str(params[\"b\"])\n",
    "        \n",
    "        means = np.load(\"ex_88_s10000_outputs/\" + traj_string + \"_b\" + b_string + \"_\" + \"avg\" + \".pkl\", \n",
    "                        allow_pickle=True)\n",
    "        stds = np.load(\"ex_88_s10000_outputs/\" + traj_string + \"_b\" + b_string + \"_\" + \"std\" + \".pkl\", \n",
    "                       allow_pickle=True)\n",
    "        \n",
    "        color = color_map[b_string]\n",
    "        linestyle = linestyle_map[traj_string]\n",
    "        plt.errorbar(np.arange(10000, 210000, 10000), means, color=color, \n",
    "                     linestyle=linestyle, label=\"sampling: {}, b={}\".format(traj_string, b_string))\n",
    "        plt.fill_between(np.arange(10000, 210000, 10000), means - stds, means + stds,\n",
    "                 color=color, alpha=0.1)\n",
    "        \n",
    "plt.legend(prop={'size': 15})\n",
    "plt.xlabel(\"Computation Steps\", size=30)\n",
    "plt.xticks(size=20)\n",
    "plt.ylabel(\"$v(s_0)$\", size=30)\n",
    "plt.yticks(size=20)\n",
    "plt.title(\"Initial State Value vs. Number of $Q$-Function Updates\", size=30)\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
